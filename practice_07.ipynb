{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSWEcS2XKgzi"
   },
   "source": [
    "### Practice: Parameter Efficient Fine-Tuning\n",
    "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "7xeRF_hSKgzs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --quiet torch==2.1.0 transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.46.3', '2.1.2')"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "a7adb06901b34e03893f30ecc23b97ee",
      "d94f975c69b7421f9851edeff1acbc1d",
      "7cc587f710c94a72976f67013c0d18f1",
      "7db40f2dcb2e46309b29e137eac7bba2",
      "b6013ba4a99743b3a00f7a51a366a507",
      "7220ba464c234cbba33068f18f68e7c8",
      "94aa4f70041942639c8759a9e371b80e",
      "a83adb34773a459a89ae687f328b1aa2",
      "3ee1280bc2b6439e8298f0ea8c74d30e",
      "93ed39f5849c493eaa8035bd3d11047b",
      "845a855f36124f03a30829e21df98702"
     ]
    },
    "id": "VMzFwx29Kgzu",
    "outputId": "7077979d-a419-4b4b-af7f-ff34d54697ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.89it/s]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "# loading Llama tokenizer ...\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ... and the model itself\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
    "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
    "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
    "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgspB2JwSIS2"
   },
   "source": [
    "### Prompt tuning: the story of a fox (2 pts)\n",
    "\n",
    "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H13pYFRxQi4U",
    "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH {'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Output: <s>A quick brown fox jumps over the lazy dog.\n",
      "A quick\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "print('BATCH', batch)\n",
    "\n",
    "for i in range(10):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhZACT6SgLq"
   },
   "source": [
    "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_r6UVDl4NEua",
    "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916,  1258,   451, 12500,   975,\n",
      "           278, 17366, 11203, 29889, 19065, 29892,   393, 11203,   553,  9841,\n",
      "           372,  8763, 29991]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')} torch.Size([1, 23])\n",
      "next_word_logits torch.Size([1, 22, 32000])\n",
      "Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "print(batch, batch.input_ids.shape)\n",
    "outputs = model(**batch)\n",
    "\n",
    "next_word_logits = outputs.logits[:, :-1]\n",
    "#print('outputs', outputs)\n",
    "print('next_word_logits', next_word_logits.shape)\n",
    "true_next_tokens = batch['input_ids'][:, 1:]\n",
    "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amvNufS8WXa0"
   },
   "source": [
    "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
    "\n",
    "![img](https://i.imgur.com/VwNNKnb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "73ZOCFRZWR98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
    "    \"\"\"\n",
    "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n",
    "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n",
    "\n",
    "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
    "        super().__init__()\n",
    "        self.original_word_embeddings = word_embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        self.learnable_prompts = nn.Parameter(\n",
    "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # input_ids shape: [batch_size, seq length]\n",
    "        assert input_ids.dtype == torch.int64\n",
    "        assert input_ids.shape[1] > self.num_prompts\n",
    "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n",
    "\n",
    "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n",
    "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n",
    "\n",
    "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n",
    "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n",
    "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n",
    "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n",
    "\n",
    "        #<YOUR CODE HERE>\n",
    "        #print(input_ids, input_ids.shape)\n",
    "        embedded_prompt = self.original_word_embeddings(input_ids)\n",
    "        #print(embedded_prompt, embedded_prompt.shape, embedded_prompt[:, self.num_prompts:].shape)\n",
    "        replaced_prompt = torch.cat([self.learnable_prompts, embedded_prompt[:, self.num_prompts:]], dim = 1)\n",
    "        #print(replaced_prompt.shape)\n",
    "        return replaced_prompt #your_outputs_with_prompts_as_per_instructions_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxUyUU2uT2f1",
    "outputId": "9d0de5c1-162a-4a6f-92c1-1a7f41069464",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    }
   ],
   "source": [
    "num_prompts = 16\n",
    "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "                               dtype=torch.int64, device=device)\n",
    "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
    "\n",
    "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
    "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
    "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
    "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
    "print(\"Looks legit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbKPgfT-crqW"
   },
   "source": [
    "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "QRe0lpREV49G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
    "save_emb_tokens_matr = model.model.embed_tokens\n",
    "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "3gVQzgdka-Bm",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.3577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.5334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.8889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.3342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.8850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.2036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.3458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.0899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.8492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.8761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "                               dtype=torch.int64, device=device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "while loss.item() > 0.1:\n",
    "    opt.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
    "    true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "#raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
    "\n",
    "\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "F7DkWHD-r1Xo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
    "\n",
    "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32000, 4096]), torch.Size([16, 4096]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompts = torch.clone(model.model.embed_tokens.learnable_prompts[0]).cpu()\n",
    "emb_lookup = torch.clone(model.model.embed_tokens.original_word_embeddings.weight).cpu()\n",
    "emb_lookup.shape, learned_prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 4096])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompts[i].unsqueeze(0).repeat(emb_lookup.shape[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0147, -0.0122,  0.0007,  ...,  0.0304, -0.0115,  0.0060],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(learned_prompts[0].unsqueeze(0).repeat(emb_lookup.shape[0], 1), emb_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20416,\n",
       " 20571,\n",
       " 4450,\n",
       " 8083,\n",
       " 15283,\n",
       " 7053,\n",
       " 9412,\n",
       " 25510,\n",
       " 5593,\n",
       " 15517,\n",
       " 18577,\n",
       " 15376,\n",
       " 17708,\n",
       " 26476,\n",
       " 25865,\n",
       " 23766]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "similar_vecs, cos_similarity_save = [], []\n",
    "for i in range(learned_prompts.shape[0]):\n",
    "    cos_sim = cos(learned_prompts[i].unsqueeze(0).repeat(emb_lookup.shape[0], 1), emb_lookup)\n",
    "    max_idx = cos_sim.argmax()\n",
    "    cos_similarity_save.append(cos_sim[max_idx])\n",
    "    similar_vecs.append(max_idx.item())\n",
    "\n",
    "similar_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0704, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0687, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0617, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0618, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0595, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0569, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0654, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0673, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0714, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0610, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0592, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0642, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0671, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0614, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0665, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0681, grad_fn=<SelectBackward0>)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'governopk predlah exceptions Lat gemeMessages Air учи plants loadsSampleLayoutParamsExternalchy'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompt = tokenizer.decode(similar_vecs)\n",
    "learned_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:  governopk predlah exceptions Lat gemeMessages Air учи plants loadsSampleLayoutParamsExternalchy<s>A quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(learned_prompt + '<s>' + prompt, return_tensors='pt', return_token_type_ids=False, add_special_tokens=False).to(device)\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:  governopk predlah exceptions Lat gemeMessages Air учи plants loadsSampleLayoutParamsExternalchy<s>A quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'] = torch.cat([torch.tensor(similar_vecs, device=batch['input_ids'].device).unsqueeze(0), batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(torch.tensor(similar_vecs, device=batch['input_ids'].device).unsqueeze(0)), batch['attention_mask']], dim=1)\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEkoFNdlshv_"
   },
   "source": [
    "### Using HuggingFace PEFT (2 points)\n",
    "\n",
    "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqEEpZm2Q4UC",
    "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 65536\n",
      "Total parameters (excluding quantization): 3500478464\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
    "\n",
    "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
    "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW54GnzCwVpp"
   },
   "outputs": [],
   "source": [
    "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
    "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
    "# Finally, generate the sentence to make sure that the model learned the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71vJ9Mq7w67f"
   },
   "outputs": [],
   "source": [
    "# Feel free to structure your code as you see fit - as long as it's legible :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.9844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.4433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.4269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.0675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.4031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.2203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.9693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.8791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.1522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.4616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.4132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.01)\n",
    "#space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "#                               dtype=torch.int64, device=device)\n",
    "#batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "#batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "num_virtual_tokens = 16\n",
    "while loss.item() > 0.1:\n",
    "    opt.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    next_word_logits = outputs.logits[:, num_virtual_tokens:-1, :]\n",
    "    true_next_tokens = batch['input_ids'][:, 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "#raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
    "\n",
    "\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCkpKYjWxfhk"
   },
   "source": [
    "### Parameter-efficient finetuning with LoRA (2 points)\n",
    "\n",
    "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
    "\n",
    "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
    "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
    "\n",
    "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b87c54c3bed847ab933eae8175359169",
      "8aaa22971b3e416eae8394ef3b3b3f0f",
      "e9c4ba0d262c4b76baa00532e209b92f",
      "e6f9064e6ec545debe2e90163f4c712c",
      "6aad5b046def4a7db1048434e874b5d5",
      "dba48e929a2e43ec8e4bb3d4b32475ca",
      "530d66e4732b4d5486165654415bd2dc",
      "2bd4b6acd8004c1e98c064708108938e",
      "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "923869a5864c4d3d80fb76c99fff24e2",
      "25e1f3d72230485c9b84cae4f685a69a"
     ]
    },
    "id": "8zundaSzx90r",
    "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.39it/s]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "# re-load the model to remove any previous PEFT tuners\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
    "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "MJ_hq4fwyPVR"
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
    "    def __init__(self, module: nn.Linear, rank: int):\n",
    "        super().__init__()\n",
    "        self.module = module  # pre-trained (frozen) linear layer\n",
    "        #print(module.weight.shape)\n",
    "        #d = module.weight.shape[0]\n",
    "        #self.linearA = nn.Linear(d, rank)\n",
    "        #self.linearB = nn.Linear(rank, d)\n",
    "        \n",
    "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
    "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
    "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
    "        #self.linearA.weights = self.adapter_A\n",
    "        #self.linearB.weights = self.adapter_B\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        #print(self.module.in_features, self.module.out_features)\n",
    "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
    "        #  <YOUR CODE HERE>\n",
    "        '''\n",
    "        print('INPUT', input[0].shape)\n",
    "        print('A', self.adapter_A.T.shape)\n",
    "        print('B', self.adapter_B.shape)\n",
    "        print('AB', (self.adapter_B * self.adapter_A.T).shape)\n",
    "        print('MODULE', self.module(input).shape)\n",
    "        print(input.matmul(self.adapter_A).shape)\n",
    "        print(input.matmul(self.adapter_A).matmul(self.adapter_B).shape)\n",
    "        '''\n",
    "        return self.module(input) + input.matmul(self.adapter_A).matmul(self.adapter_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTzOs65JydcS",
    "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# test your implementation\n",
    "test_linear = nn.Linear(128, 128)\n",
    "test_linear.weight.data[...] = torch.eye(128)\n",
    "test_adapter = LoRALayer(test_linear, rank=8)\n",
    "\n",
    "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
    "\n",
    "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
    "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
    "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
    "\n",
    "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
    "#print('DUMMY LOSS', dummy_loss)\n",
    "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
    "dummy_loss.backward()\n",
    "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
    "#print(test_adapter.adapter_A.grad.sum())\n",
    "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
    "#print(test_adapter.adapter_B.grad.sum())\n",
    "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
    "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
    "del dummy_loss, test_linear, test_adapter\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tajVTsvLulB6"
   },
   "source": [
    "### Apply LoRA to the model\n",
    "\n",
    "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
    "* self_attn.o_proj - attention output projection\n",
    "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
    "* lm_head - output LM head\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "davyUVEwulB6"
   },
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "for name, module in model.model.layers.named_modules():\n",
    "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
    "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
    "\n",
    "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWzfvc0EulB6",
    "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad check successful, well done!\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
    "# test a single training step, make sure we get meaningful gradients\n",
    "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "    out = model.forward(**batch)\n",
    "    (out.logits.norm() / 100).backward()\n",
    "\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if isinstance(module, LoRALayer):\n",
    "        assert module.adapter_B.grad is not None\n",
    "        assert module.adapter_B.grad.norm().item() > 0\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "print(\"Grad check successful, well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjIJ1vkUulB7"
   },
   "source": [
    "### (example) How to train your model\n",
    "\n",
    "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a868f8a190f74df2a99a50e2ca9a7cb3",
      "895c44b30fd24b8b9bbedc631a7934ec",
      "92f3ee2ed68c4defbed2fe9bef0f20b2",
      "3321598939fd4d76957155f58097fadd",
      "f0fe9da3411840ef8d7eff80883cb8e9",
      "8ad2b69a25304e5f903f2fd43b538340",
      "7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "f72b2d490b04440b800ac3c8ab05e625",
      "c6ddf10ea9ef499f917c81fde3e63cd2",
      "31c4ef0dab98410d88891a2c27fdb5c1",
      "54b23e64bbc444b4abc3f25832e3677d"
     ]
    },
    "id": "r9mIpntHulB8",
    "outputId": "21b0c176-b6b8-4b4e-c193-c4c8c61a3bf7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 32/32 [00:00<00:00, 3354.10 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:30, Epoch 6.25/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "/home/daozerova/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'active_adapters' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3095\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3097\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3727\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3834\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3832\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3834\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:2835\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _hf_peft_config_loaded:\n\u001b[1;32m   2832\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2834\u001b[0m     )\n\u001b[0;32m-> 2835\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adapter_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_peft_format:\n\u001b[1;32m   2838\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2840\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:444\u001b[0m, in \u001b[0;36mPeftAdapterMixin.get_adapter_state_dict\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m adapter_state_dict \u001b[38;5;241m=\u001b[39m get_peft_model_state_dict(\u001b[38;5;28mself\u001b[39m, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_state_dict\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:422\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_adapter\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    418\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `active_adapter` method is deprecated and will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:412\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# For previous PEFT versions\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mactive_adapters\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    413\u001b[0m     active_adapters \u001b[38;5;241m=\u001b[39m [active_adapters]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m active_adapters\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'active_adapters' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# checking if the model can learn. Change max_steps for proper training\n",
    "import datasets\n",
    "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
    "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
    "        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n",
    "        logging_steps=1, output_dir='outputs', report_to=None),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQUlqoEAulB8"
   },
   "source": [
    "### Final task: *actually* train the model (4 points)\n",
    "\n",
    "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
    "\n",
    "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
    "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
    "* __short lines:__ please take the first 512 characters of each line\n",
    "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
    "   - extra adapter on lm_head\n",
    "   - extra adapter on MLP components (mlp.*)\n",
    "   - trainable input embeddings (requires tweaking memory usage)\n",
    "\n",
    "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
    "\n",
    "\n",
    "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
    "\n",
    "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'ahmedbodi/AutobahnPython',\n",
       " 'path': 'examples/asyncio/websocket/echo/client_coroutines.py',\n",
       " 'copies': '13',\n",
       " 'size': '2044',\n",
       " 'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n##  See the License for the specific language governing permissions and\\n##  limitations under the License.\\n##\\n###############################################################################\\n\\nfrom autobahn.asyncio.websocket import WebSocketClientProtocol, \\\\\\n                                       WebSocketClientFactory\\n\\nimport asyncio\\n\\n\\n\\nclass MyClientProtocol(WebSocketClientProtocol):\\n\\n   def onConnect(self, response):\\n      print(\"Server connected: {0}\".format(response.peer))\\n\\n   @asyncio.coroutine\\n   def onOpen(self):\\n      print(\"WebSocket connection open.\")\\n\\n      ## start sending messages every second ..\\n      while True:\\n         self.sendMessage(u\"Hello, world!\".encode(\\'utf8\\'))\\n         self.sendMessage(b\"\\\\x00\\\\x01\\\\x03\\\\x04\", isBinary = True)\\n         yield from asyncio.sleep(1)\\n\\n   def onMessage(self, payload, isBinary):\\n      if isBinary:\\n         print(\"Binary message received: {0} bytes\".format(len(payload)))\\n      else:\\n         print(\"Text message received: {0}\".format(payload.decode(\\'utf8\\')))\\n\\n   def onClose(self, wasClean, code, reason):\\n      print(\"WebSocket connection closed: {0}\".format(reason))\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n   import asyncio\\n\\n   factory = WebSocketClientFactory(\"ws://localhost:9000\", debug = False)\\n   factory.protocol = MyClientProtocol\\n\\n   loop = asyncio.get_event_loop()\\n   coro = loop.create_connection(factory, \\'127.0.0.1\\', 9000)\\n   loop.run_until_complete(coro)\\n   loop.run_forever()\\n   loop.close()\\n',\n",
       " 'license': 'apache-2.0',\n",
       " 'hash': 7822061744094950801,\n",
       " 'line_mean': 31.4444444444,\n",
       " 'line_max': 79,\n",
       " 'alpha_frac': 0.6232876712,\n",
       " 'autogenerated': False}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"codeparrot/codeparrot-clean-valid\", split='train[:5000]')\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5361373/5361373 [07:34<00:00, 11789.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'repo_name': 'ahmedbodi/AutobahnPython',\n",
       "  'path': 'examples/asyncio/websocket/echo/client_coroutines.py',\n",
       "  'copies': '13',\n",
       "  'size': '2044',\n",
       "  'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  W',\n",
       "  'license': 'apache-2.0',\n",
       "  'hash': 7822061744094950801,\n",
       "  'line_mean': 31.4444444444,\n",
       "  'line_max': 79,\n",
       "  'alpha_frac': 0.6232876712,\n",
       "  'autogenerated': False},\n",
       " 11)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_characters(line):\n",
    "    line['content'] = line['content'][:512]\n",
    "    return line\n",
    "\n",
    "ds = ds.map(truncate_characters)\n",
    "ds[0], len(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "0.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "0.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "0.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "0.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "0.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "0.mlp.act_fn SiLU()\n",
      "0.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "0.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "1.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "1.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "1.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "1.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "1.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "1.mlp.act_fn SiLU()\n",
      "1.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "1.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "2.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "2.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "2.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "2.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "2.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "2.mlp.act_fn SiLU()\n",
      "2.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "2.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "3.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "3.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "3.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "3.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "3.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "3.mlp.act_fn SiLU()\n",
      "3.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "3.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "4.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "4.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "4.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "4.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "4.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "4.mlp.act_fn SiLU()\n",
      "4.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "4.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "5.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "5.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "5.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "5.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "5.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "5.mlp.act_fn SiLU()\n",
      "5.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "5.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "6.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "6.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "6.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "6.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "6.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "6.mlp.act_fn SiLU()\n",
      "6.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "6.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "7.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "7.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "7.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "7.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "7.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "7.mlp.act_fn SiLU()\n",
      "7.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "7.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "8.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "8.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "8.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "8.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "8.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "8.mlp.act_fn SiLU()\n",
      "8.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "8.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "9.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "9.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "9.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "9.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "9.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "9.mlp.act_fn SiLU()\n",
      "9.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "9.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "10.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "10.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "10.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "10.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "10.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "10.mlp.act_fn SiLU()\n",
      "10.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "10.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "11.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "11.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "11.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "11.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "11.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "11.mlp.act_fn SiLU()\n",
      "11.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "11.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "12.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "12.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "12.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "12.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "12.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "12.mlp.act_fn SiLU()\n",
      "12.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "12.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "13.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "13.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "13.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "13.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "13.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "13.mlp.act_fn SiLU()\n",
      "13.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "13.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "14.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "14.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "14.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "14.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "14.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "14.mlp.act_fn SiLU()\n",
      "14.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "14.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "15.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "15.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "15.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "15.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "15.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "15.mlp.act_fn SiLU()\n",
      "15.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "15.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "16.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "16.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "16.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "16.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "16.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "16.mlp.act_fn SiLU()\n",
      "16.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "16.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "17.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "17.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "17.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "17.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "17.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "17.mlp.act_fn SiLU()\n",
      "17.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "17.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "18.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "18.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "18.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "18.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "18.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "18.mlp.act_fn SiLU()\n",
      "18.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "18.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "19.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "19.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "19.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "19.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "19.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "19.mlp.act_fn SiLU()\n",
      "19.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "19.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "20.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "20.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "20.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "20.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "20.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "20.mlp.act_fn SiLU()\n",
      "20.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "20.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "21.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "21.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "21.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "21.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "21.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "21.mlp.act_fn SiLU()\n",
      "21.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "21.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "22.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "22.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "22.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "22.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "22.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "22.mlp.act_fn SiLU()\n",
      "22.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "22.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "23.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "23.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "23.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "23.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "23.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "23.mlp.act_fn SiLU()\n",
      "23.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "23.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "24.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "24.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "24.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "24.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "24.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "24.mlp.act_fn SiLU()\n",
      "24.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "24.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "25.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "25.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "25.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "25.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "25.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "25.mlp.act_fn SiLU()\n",
      "25.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "25.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "26.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "26.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "26.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "26.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "26.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "26.mlp.act_fn SiLU()\n",
      "26.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "26.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "27.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "27.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "27.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "27.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "27.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "27.mlp.act_fn SiLU()\n",
      "27.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "27.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "28.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "28.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "28.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "28.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "28.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "28.mlp.act_fn SiLU()\n",
      "28.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "28.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "29.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "29.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "29.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "29.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "29.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "29.mlp.act_fn SiLU()\n",
      "29.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "29.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "30.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "30.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "30.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "30.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "30.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "30.mlp.act_fn SiLU()\n",
      "30.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "30.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "31.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "31.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "31.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "31.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "31.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "31.mlp.act_fn SiLU()\n",
      "31.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "31.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.layers.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8\n",
    "for name, module in model.model.layers.named_modules():\n",
    "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
    "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
    "        #continue\n",
    "    \n",
    "    if 'LlamaMLP' in repr(type(module)):\n",
    "        print(name, module)\n",
    "        module.gate_proj = LoRALayer(module.gate_proj, rank=lora_rank).to(device)\n",
    "        module.up_proj = LoRALayer(module.up_proj, rank=lora_rank).to(device)\n",
    "        module.down_proj = LoRALayer(module.down_proj, rank=lora_rank).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   7%|▋         | 377000/5361373 [02:44<36:12, 2294.23 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3543\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3544\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3545\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3547\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3556\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[282], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(samples)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m samples: \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3109\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3107\u001b[0m         )\n\u001b[1;32m   3108\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3132\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3133\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3152\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3303\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3304\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3309\u001b[0m )\n\u001b[0;32m-> 3311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils.py:892\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 892\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils.py:944\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    942\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m first_ids, second_ids \u001b[38;5;129;01min\u001b[39;00m batch_ids_pairs:\n\u001b[0;32m--> 944\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecond_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPaddingStrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDO_NOT_PAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We convert the whole batch to tensors at the end\u001b[39;49;00m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3623\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3607\u001b[0m \u001b[38;5;124;03mPrepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\u001b[39;00m\n\u001b[1;32m   3608\u001b[0m \u001b[38;5;124;03madds special tokens, truncates sequences if overflowing while taking into account the special tokens and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3619\u001b[0m \u001b[38;5;124;03m        and `convert_tokens_to_ids` methods.\u001b[39;00m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3623\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3629\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3632\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3633\u001b[0m len_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2922\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2919\u001b[0m             max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m-> 2922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   2923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2924\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2927\u001b[0m     )\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = ds.map(lambda samples: tokenizer(samples['content']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking if the model can learn. Change max_steps for proper training\n",
    "#import datasets\n",
    "#data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
    "\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
    "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
    "        warmup_steps=250, max_steps=500, learning_rate=2e-4, fp16=True,\n",
    "        logging_steps=1, output_dir='outputs', report_to=None),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LfFWSYhulB8"
   },
   "outputs": [],
   "source": [
    "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "# <A WHOLE LOT OF YOUR CODE>\n",
    "# generate baseline samples with the selected prompts before finetuning\n",
    "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
    "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
    "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
    "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSucUeB4ulB9",
    "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"
   },
   "outputs": [],
   "source": [
    "# This template helps to compare generated code samples in pretty table form\n",
    "# feel free to present your work in other forms\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = '''  <tr>\n",
    "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "  </tr>'''\n",
    "\n",
    "rows = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # replace placeholders in the format() arguments\n",
    "    rows.append(row_template.format(prompt, \"BEFORE FINETUNING\", \"TO BE GENERATED AFTER FINETUNING\"))\n",
    "\n",
    "display(HTML(table_template.format('\\n'.join(rows))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrKidv5KulB9"
   },
   "source": [
    "If you reach this: congratulations! you've completed everything in this practice session.\n",
    "\n",
    "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
    "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
    "\n",
    "\n",
    "\n",
    "### Read more\n",
    "\n",
    "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
    "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
    "* A general library for different adapter types: https://adapterhub.ml/\n",
    "\n",
    "\n",
    "### [extra info] Running other models.\n",
    "\n",
    "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
    "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
    "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
    "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25e1f3d72230485c9b84cae4f685a69a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bd4b6acd8004c1e98c064708108938e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31c4ef0dab98410d88891a2c27fdb5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3321598939fd4d76957155f58097fadd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31c4ef0dab98410d88891a2c27fdb5c1",
      "placeholder": "​",
      "style": "IPY_MODEL_54b23e64bbc444b4abc3f25832e3677d",
      "value": " 32/32 [00:00&lt;00:00, 325.58 examples/s]"
     }
    },
    "3ee1280bc2b6439e8298f0ea8c74d30e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "530d66e4732b4d5486165654415bd2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54b23e64bbc444b4abc3f25832e3677d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6aad5b046def4a7db1048434e874b5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7220ba464c234cbba33068f18f68e7c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aea6cd9a18b4dd9b40bbe65fb0b9069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cc587f710c94a72976f67013c0d18f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a83adb34773a459a89ae687f328b1aa2",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ee1280bc2b6439e8298f0ea8c74d30e",
      "value": 33
     }
    },
    "7db40f2dcb2e46309b29e137eac7bba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ed39f5849c493eaa8035bd3d11047b",
      "placeholder": "​",
      "style": "IPY_MODEL_845a855f36124f03a30829e21df98702",
      "value": " 33/33 [01:52&lt;00:00,  3.35s/it]"
     }
    },
    "845a855f36124f03a30829e21df98702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "895c44b30fd24b8b9bbedc631a7934ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ad2b69a25304e5f903f2fd43b538340",
      "placeholder": "​",
      "style": "IPY_MODEL_7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "value": "Map: 100%"
     }
    },
    "8aaa22971b3e416eae8394ef3b3b3f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
      "placeholder": "​",
      "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8ad2b69a25304e5f903f2fd43b538340": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923869a5864c4d3d80fb76c99fff24e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f3ee2ed68c4defbed2fe9bef0f20b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f72b2d490b04440b800ac3c8ab05e625",
      "max": 32,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6ddf10ea9ef499f917c81fde3e63cd2",
      "value": 32
     }
    },
    "93ed39f5849c493eaa8035bd3d11047b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94aa4f70041942639c8759a9e371b80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7adb06901b34e03893f30ecc23b97ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d94f975c69b7421f9851edeff1acbc1d",
       "IPY_MODEL_7cc587f710c94a72976f67013c0d18f1",
       "IPY_MODEL_7db40f2dcb2e46309b29e137eac7bba2"
      ],
      "layout": "IPY_MODEL_b6013ba4a99743b3a00f7a51a366a507"
     }
    },
    "a83adb34773a459a89ae687f328b1aa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a868f8a190f74df2a99a50e2ca9a7cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_895c44b30fd24b8b9bbedc631a7934ec",
       "IPY_MODEL_92f3ee2ed68c4defbed2fe9bef0f20b2",
       "IPY_MODEL_3321598939fd4d76957155f58097fadd"
      ],
      "layout": "IPY_MODEL_f0fe9da3411840ef8d7eff80883cb8e9"
     }
    },
    "b6013ba4a99743b3a00f7a51a366a507": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b87c54c3bed847ab933eae8175359169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
       "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
       "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
      ],
      "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
     }
    },
    "c6ddf10ea9ef499f917c81fde3e63cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d94f975c69b7421f9851edeff1acbc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7220ba464c234cbba33068f18f68e7c8",
      "placeholder": "​",
      "style": "IPY_MODEL_94aa4f70041942639c8759a9e371b80e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "dba48e929a2e43ec8e4bb3d4b32475ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f9064e6ec545debe2e90163f4c712c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
      "placeholder": "​",
      "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
      "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
     }
    },
    "e9c4ba0d262c4b76baa00532e209b92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "value": 33
     }
    },
    "f0fe9da3411840ef8d7eff80883cb8e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72b2d490b04440b800ac3c8ab05e625": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
