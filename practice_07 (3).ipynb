{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSWEcS2XKgzi"
   },
   "source": [
    "### Practice: Parameter Efficient Fine-Tuning\n",
    "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7xeRF_hSKgzs",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daozerova/.conda/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%pip install --quiet torch==2.1.0 transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.46.3', '2.1.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "a7adb06901b34e03893f30ecc23b97ee",
      "d94f975c69b7421f9851edeff1acbc1d",
      "7cc587f710c94a72976f67013c0d18f1",
      "7db40f2dcb2e46309b29e137eac7bba2",
      "b6013ba4a99743b3a00f7a51a366a507",
      "7220ba464c234cbba33068f18f68e7c8",
      "94aa4f70041942639c8759a9e371b80e",
      "a83adb34773a459a89ae687f328b1aa2",
      "3ee1280bc2b6439e8298f0ea8c74d30e",
      "93ed39f5849c493eaa8035bd3d11047b",
      "845a855f36124f03a30829e21df98702"
     ]
    },
    "id": "VMzFwx29Kgzu",
    "outputId": "7077979d-a419-4b4b-af7f-ff34d54697ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "# loading Llama tokenizer ...\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ... and the model itself\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
    "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
    "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
    "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgspB2JwSIS2"
   },
   "source": [
    "### Prompt tuning: the story of a fox (2 pts)\n",
    "\n",
    "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H13pYFRxQi4U",
    "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH {'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Output: <s>A quick brown fox jumps over the lazy dog.\n",
      "A quick\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "print('BATCH', batch)\n",
    "\n",
    "for i in range(10):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhZACT6SgLq"
   },
   "source": [
    "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_r6UVDl4NEua",
    "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916,  1258,   451, 12500,   975,\n",
      "           278, 17366, 11203, 29889, 19065, 29892,   393, 11203,   553,  9841,\n",
      "           372,  8763, 29991]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')} torch.Size([1, 23])\n",
      "next_word_logits torch.Size([1, 22, 32000])\n",
      "Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "print(batch, batch.input_ids.shape)\n",
    "outputs = model(**batch)\n",
    "\n",
    "next_word_logits = outputs.logits[:, :-1]\n",
    "#print('outputs', outputs)\n",
    "print('next_word_logits', next_word_logits.shape)\n",
    "true_next_tokens = batch['input_ids'][:, 1:]\n",
    "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amvNufS8WXa0"
   },
   "source": [
    "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
    "\n",
    "![img](https://i.imgur.com/VwNNKnb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "73ZOCFRZWR98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
    "    \"\"\"\n",
    "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n",
    "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n",
    "\n",
    "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
    "        super().__init__()\n",
    "        self.original_word_embeddings = word_embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        self.learnable_prompts = nn.Parameter(\n",
    "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # input_ids shape: [batch_size, seq length]\n",
    "        assert input_ids.dtype == torch.int64\n",
    "        assert input_ids.shape[1] > self.num_prompts\n",
    "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n",
    "\n",
    "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n",
    "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n",
    "\n",
    "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n",
    "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n",
    "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n",
    "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n",
    "\n",
    "        #<YOUR CODE HERE>\n",
    "        #print(input_ids, input_ids.shape)\n",
    "        embedded_prompt = self.original_word_embeddings(input_ids)\n",
    "        #print(embedded_prompt, embedded_prompt.shape, embedded_prompt[:, self.num_prompts:].shape)\n",
    "        replaced_prompt = torch.cat([self.learnable_prompts, embedded_prompt[:, self.num_prompts:]], dim = 1)\n",
    "        #print(replaced_prompt.shape)\n",
    "        return replaced_prompt #your_outputs_with_prompts_as_per_instructions_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxUyUU2uT2f1",
    "outputId": "9d0de5c1-162a-4a6f-92c1-1a7f41069464",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    }
   ],
   "source": [
    "num_prompts = 16\n",
    "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "                               dtype=torch.int64, device=device)\n",
    "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
    "\n",
    "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
    "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
    "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
    "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
    "print(\"Looks legit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbKPgfT-crqW"
   },
   "source": [
    "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QRe0lpREV49G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
    "save_emb_tokens_matr = model.model.embed_tokens\n",
    "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3gVQzgdka-Bm",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.2487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.1169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.2927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.4734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.5732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.2477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.8375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.1540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.4458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.8554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.8072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "                               dtype=torch.int64, device=device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "while loss.item() > 0.1:\n",
    "    opt.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
    "    true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "#raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
    "\n",
    "\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F7DkWHD-r1Xo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
    "\n",
    "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32000, 4096]), torch.Size([16, 4096]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompts = torch.clone(model.model.embed_tokens.learnable_prompts[0]).cpu()\n",
    "emb_lookup = torch.clone(model.model.embed_tokens.original_word_embeddings.weight).cpu()\n",
    "emb_lookup.shape, learned_prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompts[i].unsqueeze(0).repeat(emb_lookup.shape[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23799,\n",
       " 14566,\n",
       " 11699,\n",
       " 17018,\n",
       " 22366,\n",
       " 12471,\n",
       " 58,\n",
       " 27158,\n",
       " 28635,\n",
       " 4369,\n",
       " 1,\n",
       " 3022,\n",
       " 27878,\n",
       " 1008,\n",
       " 26273,\n",
       " 18712]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "similar_vecs, cos_similarity_save = [], []\n",
    "for i in range(learned_prompts.shape[0]):\n",
    "    cos_sim = cos(learned_prompts[i].unsqueeze(0).repeat(emb_lookup.shape[0], 1), emb_lookup)\n",
    "    max_idx = cos_sim.argmax()\n",
    "    cos_similarity_save.append(cos_sim[max_idx])\n",
    "    similar_vecs.append(max_idx.item())\n",
    "\n",
    "similar_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0678, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0684, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0776, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0722, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0625, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0687, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0709, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0638, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0620, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0605, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0616, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0634, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0690, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0677, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0573, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0633, grad_fn=<SelectBackward0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' GT ihren Has rif imply environ7 Fernández bridolf<s>conc recomater Muséeburn'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_prompt = tokenizer.decode(similar_vecs)\n",
    "learned_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:  GT ihren Has rif imply environ7 Fernández bridolf<s> conc recomater Muséeburn<s>A quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(learned_prompt + '<s>' + prompt, return_tensors='pt', return_token_type_ids=False, add_special_tokens=False).to(device)\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:  GT ihren Has rif imply environ7 Fernández bridolf<s> conc recomater Muséeburn<s>A quick brown fox jumps over the lazy dog.\n",
      "The fox is a member of\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'] = torch.cat([torch.tensor(similar_vecs, device=batch['input_ids'].device).unsqueeze(0), batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(torch.tensor(similar_vecs, device=batch['input_ids'].device).unsqueeze(0)), batch['attention_mask']], dim=1)\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEkoFNdlshv_"
   },
   "source": [
    "### Using HuggingFace PEFT (2 points)\n",
    "\n",
    "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqEEpZm2Q4UC",
    "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 65536\n",
      "Total parameters (excluding quantization): 3500478464\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
    "\n",
    "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
    "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW54GnzCwVpp"
   },
   "outputs": [],
   "source": [
    "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
    "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
    "# Finally, generate the sentence to make sure that the model learned the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71vJ9Mq7w67f"
   },
   "outputs": [],
   "source": [
    "# Feel free to structure your code as you see fit - as long as it's legible :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.9844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.4433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.4269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.0675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.4031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.2203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.9693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.8791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.1522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.9342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.5114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.4616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.4132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.3210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.2313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.1410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.9006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.01)\n",
    "#space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
    "#                               dtype=torch.int64, device=device)\n",
    "#batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "#batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "num_virtual_tokens = 16\n",
    "while loss.item() > 0.1:\n",
    "    opt.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    next_word_logits = outputs.logits[:, num_virtual_tokens:-1, :]\n",
    "    true_next_tokens = batch['input_ids'][:, 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "#raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
    "\n",
    "\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "for i in range(15):\n",
    "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCkpKYjWxfhk"
   },
   "source": [
    "### Parameter-efficient finetuning with LoRA (2 points)\n",
    "\n",
    "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
    "\n",
    "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
    "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
    "\n",
    "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b87c54c3bed847ab933eae8175359169",
      "8aaa22971b3e416eae8394ef3b3b3f0f",
      "e9c4ba0d262c4b76baa00532e209b92f",
      "e6f9064e6ec545debe2e90163f4c712c",
      "6aad5b046def4a7db1048434e874b5d5",
      "dba48e929a2e43ec8e4bb3d4b32475ca",
      "530d66e4732b4d5486165654415bd2dc",
      "2bd4b6acd8004c1e98c064708108938e",
      "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "923869a5864c4d3d80fb76c99fff24e2",
      "25e1f3d72230485c9b84cae4f685a69a"
     ]
    },
    "id": "8zundaSzx90r",
    "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:13<00:00,  2.39it/s]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "# re-load the model to remove any previous PEFT tuners\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
    "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MJ_hq4fwyPVR"
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
    "    def __init__(self, module: nn.Linear, rank: int):\n",
    "        super().__init__()\n",
    "        self.module = module  # pre-trained (frozen) linear layer\n",
    "        #print(module.weight.shape)\n",
    "        #d = module.weight.shape[0]\n",
    "        #self.linearA = nn.Linear(d, rank)\n",
    "        #self.linearB = nn.Linear(rank, d)\n",
    "        \n",
    "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
    "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
    "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
    "        #self.linearA.weights = self.adapter_A\n",
    "        #self.linearB.weights = self.adapter_B\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        #print(self.module.in_features, self.module.out_features)\n",
    "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
    "        #  <YOUR CODE HERE>\n",
    "        \n",
    "        return self.module(input) + input.matmul(self.adapter_A).matmul(self.adapter_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTzOs65JydcS",
    "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# test your implementation\n",
    "test_linear = nn.Linear(128, 128)\n",
    "test_linear.weight.data[...] = torch.eye(128)\n",
    "test_adapter = LoRALayer(test_linear, rank=8)\n",
    "\n",
    "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
    "\n",
    "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
    "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
    "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
    "\n",
    "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
    "#print('DUMMY LOSS', dummy_loss)\n",
    "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
    "dummy_loss.backward()\n",
    "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
    "#print(test_adapter.adapter_A.grad.sum())\n",
    "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
    "#print(test_adapter.adapter_B.grad.sum())\n",
    "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
    "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
    "del dummy_loss, test_linear, test_adapter\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tajVTsvLulB6"
   },
   "source": [
    "### Apply LoRA to the model\n",
    "\n",
    "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
    "* self_attn.o_proj - attention output projection\n",
    "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
    "* lm_head - output LM head\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "davyUVEwulB6"
   },
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "for name, module in model.model.layers.named_modules():\n",
    "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
    "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
    "\n",
    "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWzfvc0EulB6",
    "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad check successful, well done!\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
    "# test a single training step, make sure we get meaningful gradients\n",
    "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "    out = model.forward(**batch)\n",
    "    (out.logits.norm() / 100).backward()\n",
    "\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if isinstance(module, LoRALayer):\n",
    "        assert module.adapter_B.grad is not None\n",
    "        assert module.adapter_B.grad.norm().item() > 0\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "print(\"Grad check successful, well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjIJ1vkUulB7"
   },
   "source": [
    "### (example) How to train your model\n",
    "\n",
    "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a868f8a190f74df2a99a50e2ca9a7cb3",
      "895c44b30fd24b8b9bbedc631a7934ec",
      "92f3ee2ed68c4defbed2fe9bef0f20b2",
      "3321598939fd4d76957155f58097fadd",
      "f0fe9da3411840ef8d7eff80883cb8e9",
      "8ad2b69a25304e5f903f2fd43b538340",
      "7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "f72b2d490b04440b800ac3c8ab05e625",
      "c6ddf10ea9ef499f917c81fde3e63cd2",
      "31c4ef0dab98410d88891a2c27fdb5c1",
      "54b23e64bbc444b4abc3f25832e3677d"
     ]
    },
    "id": "r9mIpntHulB8",
    "outputId": "21b0c176-b6b8-4b4e-c193-c4c8c61a3bf7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 32/32 [00:00<00:00, 3354.10 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:30, Epoch 6.25/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "/home/daozerova/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'active_adapters' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3095\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3097\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3727\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3834\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3832\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3834\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:2835\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _hf_peft_config_loaded:\n\u001b[1;32m   2832\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2834\u001b[0m     )\n\u001b[0;32m-> 2835\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adapter_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_peft_format:\n\u001b[1;32m   2838\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2840\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:444\u001b[0m, in \u001b[0;36mPeftAdapterMixin.get_adapter_state_dict\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m adapter_state_dict \u001b[38;5;241m=\u001b[39m get_peft_model_state_dict(\u001b[38;5;28mself\u001b[39m, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_state_dict\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:422\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_adapter\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    418\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `active_adapter` method is deprecated and will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:412\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# For previous PEFT versions\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mactive_adapters\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    413\u001b[0m     active_adapters \u001b[38;5;241m=\u001b[39m [active_adapters]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m active_adapters\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'active_adapters' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# checking if the model can learn. Change max_steps for proper training\n",
    "import datasets\n",
    "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
    "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
    "        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n",
    "        logging_steps=1, output_dir='outputs', report_to=None),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQUlqoEAulB8"
   },
   "source": [
    "### Final task: *actually* train the model (4 points)\n",
    "\n",
    "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
    "\n",
    "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
    "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
    "* __short lines:__ please take the first 512 characters of each line\n",
    "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
    "   - extra adapter on lm_head\n",
    "   - extra adapter on MLP components (mlp.*)\n",
    "   - trainable input embeddings (requires tweaking memory usage)\n",
    "\n",
    "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
    "\n",
    "\n",
    "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
    "\n",
    "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'pansapiens/mytardis',\n",
       " 'path': 'tardis/apps/mx_views/views.py',\n",
       " 'copies': '3',\n",
       " 'size': '2892',\n",
       " 'content': 'from django.conf import settings\\nfrom django.core.paginator import Paginator, InvalidPage, EmptyPage\\nfrom django.http import HttpResponse\\n\\nfrom tardis.tardis_portal.auth import decorators as authz\\nfrom tardis.tardis_portal.models import Dataset\\nfrom tardis.tardis_portal.shortcuts import get_experiment_referer\\nfrom tardis.tardis_portal.shortcuts import render_response_index\\n\\n\\n@authz.dataset_access_required\\ndef view_full_dataset(request, dataset_id):\\n    \"\"\"Displays a MX Dataset and associated information.\\n\\n    Shows a full (hundreds of images) dataset its metadata and a list\\n    of associated files with the option to show metadata of each file\\n    and ways to download those files.  With write permission this page\\n    also allows uploading and metadata editing.\\n\\n    Settings for this view:\\n    INSTALLED_APPS += (\"tardis.apps.mx_views\",)\\n    DATASET_VIEWS = [(\"http://synchrotron.org.au/views/dataset/full\",\\n                      \"tardis.apps.mx_views.views.view_full_dataset\"),]\\n\\n    \"\"\"\\n    dataset = Dataset.objects.get(id=dataset_id)\\n\\n    def get_datafiles_page():\\n        # pagination was removed by someone in the interface but not here.\\n        # need to fix.\\n        pgresults = 100\\n\\n        paginator = Paginator(dataset.datafile_set.all(), pgresults)\\n\\n        try:\\n            page = int(request.GET.get(\\'page\\', \\'1\\'))\\n        except ValueError:\\n            page = 1\\n\\n        # If page request (9999) is out of range, deliver last page of results.\\n\\n        try:\\n            return paginator.page(page)\\n        except (EmptyPage, InvalidPage):\\n            return paginator.page(paginator.num_pages)\\n\\n    display_images = dataset.get_images()\\n    image_count = len(display_images)\\n    if image_count > 4:\\n        # take 4 evenly spaced images from the set\\n        display_images = display_images[0::image_count / 4][:4]\\n\\n    upload_method = getattr(settings, \"UPLOAD_METHOD\", \"uploadify\")\\n\\n    c = {\\n        \\'dataset\\': dataset,\\n        \\'datafiles\\': get_datafiles_page(),\\n        \\'parametersets\\': dataset.getParameterSets()\\n                                .exclude(schema__hidden=True),\\n        \\'has_download_permissions\\':\\n            authz.has_dataset_download_access(request, dataset_id),\\n        \\'has_write_permissions\\':\\n            authz.has_dataset_write(request, dataset_id),\\n        \\'from_experiment\\': \\\\\\n            get_experiment_referer(request, dataset_id),\\n        \\'other_experiments\\': \\\\\\n            authz.get_accessible_experiments_for_dataset(request, dataset_id),\\n        \\'display_images\\': display_images,\\n        \\'upload_method\\': upload_method,\\n        \\'default_organization\\':\\n            getattr(settings, \\'DEFAULT_ARCHIVE_ORGANIZATION\\', \\'classic\\'),\\n        \\'default_format\\':\\n            getattr(settings, \\'DEFAULT_ARCHIVE_FORMATS\\', [\\'tgz\\', \\'tar\\'])[0]\\n    }\\n    return HttpResponse(render_response_index(\\n        request, \\'mx_views/view_full_dataset.html\\', c))\\n',\n",
       " 'license': 'bsd-3-clause',\n",
       " 'hash': -8726488663588781404,\n",
       " 'line_mean': 37.0526315789,\n",
       " 'line_max': 79,\n",
       " 'alpha_frac': 0.6549100968,\n",
       " 'autogenerated': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"codeparrot/codeparrot-clean-valid\", split='train[:5000]')\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8659.09 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'repo_name': 'pansapiens/mytardis',\n",
       "  'path': 'tardis/apps/mx_views/views.py',\n",
       "  'copies': '3',\n",
       "  'size': '2892',\n",
       "  'content': 'from django.conf import settings\\nfrom django.core.paginator import Paginator, InvalidPage, EmptyPage\\nfrom django.http import HttpResponse\\n\\nfrom tardis.tardis_portal.auth import decorators as authz\\nfrom tardis.tardis_portal.models import Dataset\\nfrom tardis.tardis_portal.shortcuts import get_experiment_referer\\nfrom tardis.tardis_portal.shortcuts import render_response_index\\n\\n\\n@authz.dataset_access_required\\ndef view_full_dataset(request, dataset_id):\\n    \"\"\"Displays a MX Dataset and associated information.\\n\\n ',\n",
       "  'license': 'bsd-3-clause',\n",
       "  'hash': -8726488663588781404,\n",
       "  'line_mean': 37.0526315789,\n",
       "  'line_max': 79,\n",
       "  'alpha_frac': 0.6549100968,\n",
       "  'autogenerated': False},\n",
       " 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_characters(line):\n",
    "    line['content'] = line['content'][:512]\n",
    "    return line\n",
    "\n",
    "ds = ds.map(truncate_characters)\n",
    "ds[0], len(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "0.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "0.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "0.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "0.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "0.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "0.mlp.act_fn SiLU()\n",
      "0.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "0.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "1.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "1.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "1.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "1.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "1.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "1.mlp.act_fn SiLU()\n",
      "1.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "1.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "2.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "2.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "2.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "2.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "2.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "2.mlp.act_fn SiLU()\n",
      "2.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "2.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "3.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "3.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "3.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "3.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "3.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "3.mlp.act_fn SiLU()\n",
      "3.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "3.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "4.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "4.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "4.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "4.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "4.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "4.mlp.act_fn SiLU()\n",
      "4.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "4.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "5.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "5.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "5.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "5.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "5.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "5.mlp.act_fn SiLU()\n",
      "5.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "5.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "6.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "6.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "6.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "6.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "6.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "6.mlp.act_fn SiLU()\n",
      "6.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "6.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "7.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "7.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "7.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "7.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "7.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "7.mlp.act_fn SiLU()\n",
      "7.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "7.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "8.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "8.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "8.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "8.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "8.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "8.mlp.act_fn SiLU()\n",
      "8.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "8.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "9.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "9.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "9.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "9.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "9.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "9.mlp.act_fn SiLU()\n",
      "9.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "9.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "10.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "10.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "10.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "10.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "10.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "10.mlp.act_fn SiLU()\n",
      "10.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "10.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "11.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "11.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "11.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "11.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "11.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "11.mlp.act_fn SiLU()\n",
      "11.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "11.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "12.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "12.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "12.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "12.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "12.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "12.mlp.act_fn SiLU()\n",
      "12.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "12.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "13.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "13.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "13.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "13.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "13.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "13.mlp.act_fn SiLU()\n",
      "13.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "13.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "14.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "14.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "14.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "14.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "14.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "14.mlp.act_fn SiLU()\n",
      "14.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "14.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "15.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "15.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "15.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "15.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "15.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "15.mlp.act_fn SiLU()\n",
      "15.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "15.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "16.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "16.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "16.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "16.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "16.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "16.mlp.act_fn SiLU()\n",
      "16.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "16.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "17.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "17.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "17.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "17.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "17.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "17.mlp.act_fn SiLU()\n",
      "17.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "17.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "18.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "18.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "18.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "18.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "18.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "18.mlp.act_fn SiLU()\n",
      "18.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "18.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "19.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "19.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "19.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "19.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "19.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "19.mlp.act_fn SiLU()\n",
      "19.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "19.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "20.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "20.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "20.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "20.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "20.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "20.mlp.act_fn SiLU()\n",
      "20.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "20.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "21.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "21.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "21.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "21.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "21.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "21.mlp.act_fn SiLU()\n",
      "21.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "21.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "22.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "22.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "22.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "22.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "22.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "22.mlp.act_fn SiLU()\n",
      "22.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "22.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "23.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "23.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "23.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "23.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "23.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "23.mlp.act_fn SiLU()\n",
      "23.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "23.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "24.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "24.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "24.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "24.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "24.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "24.mlp.act_fn SiLU()\n",
      "24.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "24.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "25.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "25.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "25.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "25.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "25.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "25.mlp.act_fn SiLU()\n",
      "25.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "25.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "26.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "26.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "26.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "26.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "26.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "26.mlp.act_fn SiLU()\n",
      "26.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "26.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "27.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "27.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "27.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "27.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "27.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "27.mlp.act_fn SiLU()\n",
      "27.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "27.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "28.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "28.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "28.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "28.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "28.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "28.mlp.act_fn SiLU()\n",
      "28.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "28.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "29.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "29.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "29.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "29.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "29.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "29.mlp.act_fn SiLU()\n",
      "29.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "29.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "30.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "30.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "30.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "30.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "30.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "30.mlp.act_fn SiLU()\n",
      "30.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "30.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      ")\n",
      "31.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "31.self_attn.q_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.k_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.v_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.o_proj Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "31.mlp.gate_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "31.mlp.up_proj Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "31.mlp.down_proj Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "31.mlp.act_fn SiLU()\n",
      "31.input_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n",
      "31.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-06)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.layers.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8\n",
    "for name, module in model.model.layers.named_modules():\n",
    "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
    "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
    "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
    "        #continue\n",
    "    \n",
    "    if 'LlamaMLP' in repr(type(module)):\n",
    "        print(name, module)\n",
    "        module.gate_proj = LoRALayer(module.gate_proj, rank=lora_rank).to(device)\n",
    "        module.up_proj = LoRALayer(module.up_proj, rank=lora_rank).to(device)\n",
    "        module.down_proj = LoRALayer(module.down_proj, rank=lora_rank).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:02<00:00, 2363.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = ds.map(lambda samples: tokenizer(samples['content']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/daozerova/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 18:22, Epoch 1.60/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>1.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>1.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>1.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>1.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>1.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>1.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>1.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>1.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>1.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>1.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.928600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>1.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>1.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>1.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.777900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>1.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>1.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>1.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>1.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>1.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>1.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>1.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>1.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>0.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>0.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>1.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>1.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>1.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>1.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>1.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>0.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>1.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>0.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>1.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>1.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>0.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>1.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>1.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>1.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>0.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>1.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>1.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>0.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>1.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>1.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>1.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>1.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>0.792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>1.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>0.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>1.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>1.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>1.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>1.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>0.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>0.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>0.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>0.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>0.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>1.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>0.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>1.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>1.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>1.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>0.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>1.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>0.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>1.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>1.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>1.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "/home/daozerova/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'active_adapters' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3095\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3097\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3727\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3834\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3832\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3834\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:2835\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _hf_peft_config_loaded:\n\u001b[1;32m   2832\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2834\u001b[0m     )\n\u001b[0;32m-> 2835\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adapter_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_peft_format:\n\u001b[1;32m   2838\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2840\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:444\u001b[0m, in \u001b[0;36mPeftAdapterMixin.get_adapter_state_dict\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m adapter_state_dict \u001b[38;5;241m=\u001b[39m get_peft_model_state_dict(\u001b[38;5;28mself\u001b[39m, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_state_dict\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:422\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_adapter\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    418\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `active_adapter` method is deprecated and will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/integrations/peft.py:412\u001b[0m, in \u001b[0;36mPeftAdapterMixin.active_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# For previous PEFT versions\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mactive_adapters\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    413\u001b[0m     active_adapters \u001b[38;5;241m=\u001b[39m [active_adapters]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m active_adapters\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'active_adapters' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# checking if the model can learn. Change max_steps for proper training\n",
    "#import datasets\n",
    "#data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
    "\n",
    "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
    "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
    "        warmup_steps=250, max_steps=500, learning_rate=2e-4, fp16=True,\n",
    "        logging_steps=1, output_dir='outputs', report_to=None),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_LfFWSYhulB8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given prompt \n",
      "\n",
      "Output: <s># -*- coding: utf-8 -*-\n",
      "#\n",
      "# Copyright 2014-2015 The Chromium Authors. All rights reserved.\n",
      "# Use of this source code is governed by a BSD-style license that can be\n",
      "# found in the LICENSE file.\n",
      "\n",
      "\"\"\"\n",
      "Tests for the 'chrome' command.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "import\n",
      "Given prompt import\n",
      "\n",
      "Output: <s>import os\n",
      "import sys\n",
      "import time\n",
      "import json\n",
      "import logging\n",
      "import threading\n",
      "import Queue\n",
      "import thread\n",
      "import traceback\n",
      "import socket\n",
      "import struct\n",
      "import re\n",
      "import urllib\n",
      "import urllib2\n",
      "import urlparse\n",
      "import httplib\n",
      "import httplib2\n",
      "import ssl\n",
      "import base64\n",
      "import random\n",
      "import datetime\n",
      "import hashlib\n",
      "import hmac\n",
      "import binascii\n",
      "import email.utils\n",
      "import email.message\n",
      "import email.\n",
      "Given prompt from\n",
      "\n",
      "Output: <s>from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "from __future__ import unicode_literals\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import json\n",
      "import logging\n",
      "import threading\n",
      "import traceback\n",
      "import re\n",
      "import datetime\n",
      "import functools\n",
      "import inspect\n",
      "import signal\n",
      "import signal\n",
      "import traceback\n",
      "import logging.handlers\n",
      "import logging.config\n",
      "import logging.\n",
      "Given prompt while\n",
      "\n",
      "Output: <s>while True:\n",
      "    try:\n",
      "        line = input(\"> \")\n",
      "        if line == \"exit\":\n",
      "            break\n",
      "        if line == \"help\":\n",
      "            print(\"Usage: python3 -m python3_game\")\n",
      "            break\n",
      "        if line == \"start\":\n",
      "            print(\"Starting game...\")\n",
      "            game_loop()\n",
      "        elif line == \"stop\":\n",
      "            print(\"Stopping game...\")\n",
      "            game_loop()\n",
      "        elif\n",
      "Given prompt try\n",
      "\n",
      "Output: <s>try:\n",
      "    from urllib.parse import urlparse\n",
      "except ImportError:\n",
      "    from urlparse import urlparse\n",
      "\n",
      "from django.conf import settings\n",
      "from django.core.exceptions import ImproperlyConfigured\n",
      "from django.http import HttpResponse\n",
      "from django.utils.translation import ugettext_lazy as _\n",
      "\n",
      "from oscar.core.loading import get_model\n",
      "from oscar.core.utils import get_file_size\n",
      "\n",
      "Given prompt if\n",
      "\n",
      "Output: <s>if ( ! defined( 'ABSPATH' ) ) exit;\n",
      "\n",
      "/**\n",
      " * Class WPML_Translation_Management_Controller\n",
      " *\n",
      " * @since 1.2.0\n",
      " * @author Piotr Koczanowski <piotr@koczanowski.us>\n",
      " */\n",
      "class WPML_Translation_Management_Controller {\n",
      "\n",
      "\t/**\n",
      "\t * @var WPML_Translation_Management_Model\n",
      "\t */\n",
      "\n",
      "Given prompt for\n",
      "\n",
      "Output: <s>for i in range(10):\n",
      "    print(i)\n",
      "    time.sleep(1)\n",
      "\\end{code}\n",
      "\n",
      "Comment: Thanks for the help. I'm still not sure why the first example works and the second doesn't.\n",
      "\n",
      "Comment: @user3142919 The first example works because it's not blocking. The second example works because it's blocking.\n",
      "\n",
      "Comment: @user314291\n",
      "Given prompt torch\n",
      "\n",
      "Output: <s>torch.manual('Torch-Tutorial')\n",
      "\n",
      "# Import the library\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# Import the dataset\n",
      "import datasets\n",
      "import utils\n",
      "\n",
      "# Import the model\n",
      "import model\n",
      "\n",
      "# Import the optimizer\n",
      "import optimizer\n",
      "\n",
      "# Import the loss\n",
      "import loss\n",
      "\n",
      "# Import the preprocessing\n",
      "import preprocessing\n",
      "\n",
      "# Import the evaluation\n",
      "import\n"
     ]
    }
   ],
   "source": [
    "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "after_finetuning = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    print('Given prompt', prompt)\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "    for i in range(100):\n",
    "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "    after_finetuning.append(tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n",
    "    print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n",
    "    \n",
    "# <A WHOLE LOT OF YOUR CODE>\n",
    "# generate baseline samples with the selected prompts before finetuning\n",
    "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
    "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
    "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
    "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given prompt \n",
      "\n",
      "Output: <s> ▶▶ 2019-2020 School Year\n",
      "The 2019-2020 school year is here! We are so excited to welcome our new students and families to the school. We are also excited to welcome back our returning families.\n",
      "We are looking forward to another great year at the school. We are also looking forward to another great year of learning and growing together.\n",
      "We are also looking forward to another great year of learning and growing\n",
      "Given prompt import\n",
      "\n",
      "Output: <s> import Foundation\n",
      "\n",
      "public extension NSURL {\n",
      "    public var absoluteString: String {\n",
      "        return String(cString: CFBundleGetBundleWithURL(self).UTF8String)\n",
      "    }\n",
      "}</s>\n",
      "Given prompt from\n",
      "\n",
      "Output: <s>from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "from absl import flags\n",
      "\n",
      "from tensorflow.python import pywrap_tensorflow\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.eager import function\n",
      "from tensorflow.python.eager import test\n",
      "from tensorflow.python.eager import backprop\n",
      "from tensorflow.python\n",
      "Given prompt while\n",
      "\n",
      "Output: <s>while(1)\n",
      "while(1) {\n",
      "    // do something\n",
      "}\n",
      "\\end{code}\n",
      "\n",
      "Comment: This is not the same as the OP's code.\n",
      "\n",
      "Comment: @Jeffrey: It's the same as the OP's code, except that it's not a function.\n",
      "\n",
      "Comment: @Jeffrey: The OP's code is a function, but it's not a function declaration.\n",
      "\n",
      "Comment\n",
      "Given prompt try\n",
      "\n",
      "Output: <s>try to find the best solution for your needs.\n",
      "We are a team of professionals with a long experience in the field of web development.\n",
      "We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of\n",
      "Given prompt if\n",
      "\n",
      "Output: <s>if ( !window.atmosphere ) {\n",
      "    window.atmosphere = {};\n",
      "}\n",
      "\n",
      "(function () {\n",
      "    var o = atmosphere.util,\n",
      "        atmosphere = atmosphere.atmosphere = function () {\n",
      "\n",
      "            var _isClosed = false,\n",
      "                _isOpening = false,\n",
      "                _isOpen = false,\n",
      "                _isClosing = false,\n",
      "                _isError = false,\n",
      "                _is\n",
      "Given prompt for\n",
      "\n",
      "Output: <s>for the 2019-2020 school year.\n",
      "The application process for the 2019-2020 school year is now open.\n",
      "The application process for the 2019-2020 school year is now open. Please click here to apply.\n",
      "The application process for the 2019-2020 school year is now open. Please click here to apply.\n",
      "The application process for the\n",
      "Given prompt torch\n",
      "\n",
      "Output: <s>torchbearer 2017-05-18 19:55:25 UTC #1\n",
      "I’m a newbie to the world of RPGs, and I’m looking for a game that I can play with my wife. We’re both in our 30s, and we’re looking for a game that we can play together. We’re both new to the world of RPGs, and we\n"
     ]
    }
   ],
   "source": [
    "before_finetuning = []\n",
    "for prompt in prompts:\n",
    "    print('Given prompt', prompt)\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ans = model.generate(**batch, max_length=100)\n",
    "        #print(generated_ans, tokenizer.decode(generated_ans.cpu().numpy()[0]))\n",
    "        print(\"\\nOutput:\", tokenizer.decode(generated_ans.cpu().numpy()[0]))\n",
    "        before_finetuning.append(tokenizer.decode(generated_ans.cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SSucUeB4ulB9",
    "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s> ▶▶ 2019-2020 School Year\n",
       "The 2019-2020 school year is here! We are so excited to welcome our new students and families to the school. We are also excited to welcome back our returning families.\n",
       "We are looking forward to another great year at the school. We are also looking forward to another great year of learning and growing together.\n",
       "We are also looking forward to another great year of learning and growing</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s># -*- coding: utf-8 -*-\n",
       "#\n",
       "# Copyright 2014-2015 The Chromium Authors. All rights reserved.\n",
       "# Use of this source code is governed by a BSD-style license that can be\n",
       "# found in the LICENSE file.\n",
       "\n",
       "\"\"\"\n",
       "Tests for the 'chrome' command.\n",
       "\"\"\"\n",
       "\n",
       "from __future__ import print_function\n",
       "\n",
       "import</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s> import Foundation\n",
       "\n",
       "public extension NSURL {\n",
       "    public var absoluteString: String {\n",
       "        return String(cString: CFBundleGetBundleWithURL(self).UTF8String)\n",
       "    }\n",
       "}</s></pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>import os\n",
       "import sys\n",
       "import time\n",
       "import json\n",
       "import logging\n",
       "import threading\n",
       "import Queue\n",
       "import thread\n",
       "import traceback\n",
       "import socket\n",
       "import struct\n",
       "import re\n",
       "import urllib\n",
       "import urllib2\n",
       "import urlparse\n",
       "import httplib\n",
       "import httplib2\n",
       "import ssl\n",
       "import base64\n",
       "import random\n",
       "import datetime\n",
       "import hashlib\n",
       "import hmac\n",
       "import binascii\n",
       "import email.utils\n",
       "import email.message\n",
       "import email.</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>from __future__ import absolute_import\n",
       "from __future__ import division\n",
       "from __future__ import print_function\n",
       "\n",
       "import os\n",
       "import sys\n",
       "\n",
       "from absl import flags\n",
       "\n",
       "from tensorflow.python import pywrap_tensorflow\n",
       "from tensorflow.python.eager import context\n",
       "from tensorflow.python.eager import function\n",
       "from tensorflow.python.eager import test\n",
       "from tensorflow.python.eager import backprop\n",
       "from tensorflow.python</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>from __future__ import absolute_import\n",
       "from __future__ import division\n",
       "from __future__ import print_function\n",
       "from __future__ import unicode_literals\n",
       "\n",
       "import os\n",
       "import sys\n",
       "import time\n",
       "import json\n",
       "import logging\n",
       "import threading\n",
       "import traceback\n",
       "import re\n",
       "import datetime\n",
       "import functools\n",
       "import inspect\n",
       "import signal\n",
       "import signal\n",
       "import traceback\n",
       "import logging.handlers\n",
       "import logging.config\n",
       "import logging.</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>while(1)\n",
       "while(1) {\n",
       "    // do something\n",
       "}\n",
       "\\end{code}\n",
       "\n",
       "Comment: This is not the same as the OP's code.\n",
       "\n",
       "Comment: @Jeffrey: It's the same as the OP's code, except that it's not a function.\n",
       "\n",
       "Comment: @Jeffrey: The OP's code is a function, but it's not a function declaration.\n",
       "\n",
       "Comment</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>while True:\n",
       "    try:\n",
       "        line = input(\"> \")\n",
       "        if line == \"exit\":\n",
       "            break\n",
       "        if line == \"help\":\n",
       "            print(\"Usage: python3 -m python3_game\")\n",
       "            break\n",
       "        if line == \"start\":\n",
       "            print(\"Starting game...\")\n",
       "            game_loop()\n",
       "        elif line == \"stop\":\n",
       "            print(\"Stopping game...\")\n",
       "            game_loop()\n",
       "        elif</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>try to find the best solution for your needs.\n",
       "We are a team of professionals with a long experience in the field of web development.\n",
       "We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of web development. We are a team of professionals with a long experience in the field of</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>try:\n",
       "    from urllib.parse import urlparse\n",
       "except ImportError:\n",
       "    from urlparse import urlparse\n",
       "\n",
       "from django.conf import settings\n",
       "from django.core.exceptions import ImproperlyConfigured\n",
       "from django.http import HttpResponse\n",
       "from django.utils.translation import ugettext_lazy as _\n",
       "\n",
       "from oscar.core.loading import get_model\n",
       "from oscar.core.utils import get_file_size\n",
       "</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>if ( !window.atmosphere ) {\n",
       "    window.atmosphere = {};\n",
       "}\n",
       "\n",
       "(function () {\n",
       "    var o = atmosphere.util,\n",
       "        atmosphere = atmosphere.atmosphere = function () {\n",
       "\n",
       "            var _isClosed = false,\n",
       "                _isOpening = false,\n",
       "                _isOpen = false,\n",
       "                _isClosing = false,\n",
       "                _isError = false,\n",
       "                _is</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>if ( ! defined( 'ABSPATH' ) ) exit;\n",
       "\n",
       "/**\n",
       " * Class WPML_Translation_Management_Controller\n",
       " *\n",
       " * @since 1.2.0\n",
       " * @author Piotr Koczanowski <piotr@koczanowski.us>\n",
       " */\n",
       "class WPML_Translation_Management_Controller {\n",
       "\n",
       "\t/**\n",
       "\t * @var WPML_Translation_Management_Model\n",
       "\t */\n",
       "</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>for the 2019-2020 school year.\n",
       "The application process for the 2019-2020 school year is now open.\n",
       "The application process for the 2019-2020 school year is now open. Please click here to apply.\n",
       "The application process for the 2019-2020 school year is now open. Please click here to apply.\n",
       "The application process for the</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>for i in range(10):\n",
       "    print(i)\n",
       "    time.sleep(1)\n",
       "\\end{code}\n",
       "\n",
       "Comment: Thanks for the help. I'm still not sure why the first example works and the second doesn't.\n",
       "\n",
       "Comment: @user3142919 The first example works because it's not blocking. The second example works because it's blocking.\n",
       "\n",
       "Comment: @user314291</pre></td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>torchbearer 2017-05-18 19:55:25 UTC #1\n",
       "I’m a newbie to the world of RPGs, and I’m looking for a game that I can play with my wife. We’re both in our 30s, and we’re looking for a game that we can play together. We’re both new to the world of RPGs, and we</pre></td>\n",
       "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><s>torch.manual('Torch-Tutorial')\n",
       "\n",
       "# Import the library\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "# Import the dataset\n",
       "import datasets\n",
       "import utils\n",
       "\n",
       "# Import the model\n",
       "import model\n",
       "\n",
       "# Import the optimizer\n",
       "import optimizer\n",
       "\n",
       "# Import the loss\n",
       "import loss\n",
       "\n",
       "# Import the preprocessing\n",
       "import preprocessing\n",
       "\n",
       "# Import the evaluation\n",
       "import</pre></td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This template helps to compare generated code samples in pretty table form\n",
    "# feel free to present your work in other forms\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = '''  <tr>\n",
    "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
    "  </tr>'''\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    # replace placeholders in the format() arguments\n",
    "    rows.append(row_template.format(prompts[i], before_finetuning[i], after_finetuning[i]))\n",
    "\n",
    "display(HTML(table_template.format('\\n'.join(rows))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrKidv5KulB9"
   },
   "source": [
    "If you reach this: congratulations! you've completed everything in this practice session.\n",
    "\n",
    "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
    "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
    "\n",
    "\n",
    "\n",
    "### Read more\n",
    "\n",
    "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
    "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
    "* A general library for different adapter types: https://adapterhub.ml/\n",
    "\n",
    "\n",
    "### [extra info] Running other models.\n",
    "\n",
    "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
    "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
    "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
    "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25e1f3d72230485c9b84cae4f685a69a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bd4b6acd8004c1e98c064708108938e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31c4ef0dab98410d88891a2c27fdb5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3321598939fd4d76957155f58097fadd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31c4ef0dab98410d88891a2c27fdb5c1",
      "placeholder": "​",
      "style": "IPY_MODEL_54b23e64bbc444b4abc3f25832e3677d",
      "value": " 32/32 [00:00&lt;00:00, 325.58 examples/s]"
     }
    },
    "3ee1280bc2b6439e8298f0ea8c74d30e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "530d66e4732b4d5486165654415bd2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54b23e64bbc444b4abc3f25832e3677d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6aad5b046def4a7db1048434e874b5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7220ba464c234cbba33068f18f68e7c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aea6cd9a18b4dd9b40bbe65fb0b9069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cc587f710c94a72976f67013c0d18f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a83adb34773a459a89ae687f328b1aa2",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ee1280bc2b6439e8298f0ea8c74d30e",
      "value": 33
     }
    },
    "7db40f2dcb2e46309b29e137eac7bba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ed39f5849c493eaa8035bd3d11047b",
      "placeholder": "​",
      "style": "IPY_MODEL_845a855f36124f03a30829e21df98702",
      "value": " 33/33 [01:52&lt;00:00,  3.35s/it]"
     }
    },
    "845a855f36124f03a30829e21df98702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "895c44b30fd24b8b9bbedc631a7934ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ad2b69a25304e5f903f2fd43b538340",
      "placeholder": "​",
      "style": "IPY_MODEL_7aea6cd9a18b4dd9b40bbe65fb0b9069",
      "value": "Map: 100%"
     }
    },
    "8aaa22971b3e416eae8394ef3b3b3f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
      "placeholder": "​",
      "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8ad2b69a25304e5f903f2fd43b538340": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923869a5864c4d3d80fb76c99fff24e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f3ee2ed68c4defbed2fe9bef0f20b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f72b2d490b04440b800ac3c8ab05e625",
      "max": 32,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6ddf10ea9ef499f917c81fde3e63cd2",
      "value": 32
     }
    },
    "93ed39f5849c493eaa8035bd3d11047b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94aa4f70041942639c8759a9e371b80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7adb06901b34e03893f30ecc23b97ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d94f975c69b7421f9851edeff1acbc1d",
       "IPY_MODEL_7cc587f710c94a72976f67013c0d18f1",
       "IPY_MODEL_7db40f2dcb2e46309b29e137eac7bba2"
      ],
      "layout": "IPY_MODEL_b6013ba4a99743b3a00f7a51a366a507"
     }
    },
    "a83adb34773a459a89ae687f328b1aa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a868f8a190f74df2a99a50e2ca9a7cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_895c44b30fd24b8b9bbedc631a7934ec",
       "IPY_MODEL_92f3ee2ed68c4defbed2fe9bef0f20b2",
       "IPY_MODEL_3321598939fd4d76957155f58097fadd"
      ],
      "layout": "IPY_MODEL_f0fe9da3411840ef8d7eff80883cb8e9"
     }
    },
    "b6013ba4a99743b3a00f7a51a366a507": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b87c54c3bed847ab933eae8175359169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
       "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
       "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
      ],
      "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
     }
    },
    "c6ddf10ea9ef499f917c81fde3e63cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d94f975c69b7421f9851edeff1acbc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7220ba464c234cbba33068f18f68e7c8",
      "placeholder": "​",
      "style": "IPY_MODEL_94aa4f70041942639c8759a9e371b80e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "dba48e929a2e43ec8e4bb3d4b32475ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f9064e6ec545debe2e90163f4c712c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
      "placeholder": "​",
      "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
      "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
     }
    },
    "e9c4ba0d262c4b76baa00532e209b92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
      "value": 33
     }
    },
    "f0fe9da3411840ef8d7eff80883cb8e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72b2d490b04440b800ac3c8ab05e625": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
