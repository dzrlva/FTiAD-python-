{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXGDZO31IqcQ",
    "outputId": "75d06458-fd00-484e-9fa2-b63cc6ca7710",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available\n",
      "NVIDIA A100 80GB PCIe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "  print('There are %d GPU(s) available' % torch.cuda.device_count())\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('No GPU available, CPU instaed')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq3_LIFKSqpL",
    "outputId": "eefdebba-b030-4161-cf2c-c83f5ecc2afc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/daozerova/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/daozerova/.local/lib/python3.11/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix, \\\n",
    "                            precision_score, recall_score, accuracy_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Номер*</th>\n",
       "      <th>Дата открытия**</th>\n",
       "      <th>Решено в</th>\n",
       "      <th>Услуга</th>\n",
       "      <th>Модель запроса</th>\n",
       "      <th>Тема</th>\n",
       "      <th>Описание</th>\n",
       "      <th>ID1 я Группа</th>\n",
       "      <th>Группа</th>\n",
       "      <th>Код закрытия</th>\n",
       "      <th>Информация о закрытии</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC0299994</td>\n",
       "      <td>2023-03-31 23:59:05</td>\n",
       "      <td>2023-04-03 11:40:36</td>\n",
       "      <td>Сопровождение МТС Банк Premium (Премиум)</td>\n",
       "      <td>МТС Банк Premium Просьба Персональному менедже...</td>\n",
       "      <td>Зибель. [Тип запроса]: Просьба Персональному м...</td>\n",
       "      <td>[Тип запроса]: Просьба Персональному менеджеру...</td>\n",
       "      <td>PMPremium</td>\n",
       "      <td>PMPremium</td>\n",
       "      <td>Решён полностью</td>\n",
       "      <td>ПМ Сбитинков Антон Анатольевич</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INC0299993</td>\n",
       "      <td>2023-03-31 23:53:49</td>\n",
       "      <td>2023-04-01 00:23:36</td>\n",
       "      <td>Сопровождение МТС Банк</td>\n",
       "      <td>МТС Банк Не приходит СМС при авторизации</td>\n",
       "      <td>Зибель. [Тип запроса]: Во время авторизации не...</td>\n",
       "      <td>[Тип запроса]: Во время авторизации не приходи...</td>\n",
       "      <td>Отдел поддержки карточных технологий</td>\n",
       "      <td>Отдел поддержки карточных технологий</td>\n",
       "      <td>Решён полностью</td>\n",
       "      <td>Мы получили от оператора связи статус доставки...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INC0299992</td>\n",
       "      <td>2023-03-31 23:53:04</td>\n",
       "      <td>2023-04-18 18:58:55</td>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при пополнении из другого Банка</td>\n",
       "      <td>Зибель. просьба помочь исправить у клиента в п...</td>\n",
       "      <td>просьба помочь исправить у клиента в приложени...</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "      <td>Известная ошибка</td>\n",
       "      <td>Клиент подключен к СБП. Переводы проходят успе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INC0299991</td>\n",
       "      <td>2023-03-31 23:51:01</td>\n",
       "      <td>2023-04-05 12:03:08</td>\n",
       "      <td>Обеспечение переводов по номеру телефона в МТС...</td>\n",
       "      <td>МТС Банк После ввода реквизитов и нажатию на к...</td>\n",
       "      <td>Зибель. [Тип запроса]: Проблема с переводом по...</td>\n",
       "      <td>[Тип запроса]: Проблема с переводом по реквизи...</td>\n",
       "      <td>Поддержка FTB</td>\n",
       "      <td>Приложение МТС Банк</td>\n",
       "      <td>Решён полностью</td>\n",
       "      <td>В связи с введением ограничений на стороне про...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INC0299990</td>\n",
       "      <td>2023-03-31 23:48:09</td>\n",
       "      <td>2023-04-03 06:19:15</td>\n",
       "      <td>Обеспечение исправления некорректной информаци...</td>\n",
       "      <td>МТС Банк Некорректный баланс по картам клиента</td>\n",
       "      <td>Зибель. [Тип запроса]: Неверный баланс по кред...</td>\n",
       "      <td>[Тип запроса]: Неверный баланс по кредитной ка...</td>\n",
       "      <td>Корректировки баланса</td>\n",
       "      <td>Корректировки баланса</td>\n",
       "      <td>Решён обходным решением</td>\n",
       "      <td>Закрыто по результатам сверки счетов. Если бал...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Номер*      Дата открытия**             Решено в  \\\n",
       "0  INC0299994  2023-03-31 23:59:05  2023-04-03 11:40:36   \n",
       "1  INC0299993  2023-03-31 23:53:49  2023-04-01 00:23:36   \n",
       "2  INC0299992  2023-03-31 23:53:04  2023-04-18 18:58:55   \n",
       "3  INC0299991  2023-03-31 23:51:01  2023-04-05 12:03:08   \n",
       "4  INC0299990  2023-03-31 23:48:09  2023-04-03 06:19:15   \n",
       "\n",
       "                                              Услуга  \\\n",
       "0           Сопровождение МТС Банк Premium (Премиум)   \n",
       "1                             Сопровождение МТС Банк   \n",
       "2                                  Сопровождение СБП   \n",
       "3  Обеспечение переводов по номеру телефона в МТС...   \n",
       "4  Обеспечение исправления некорректной информаци...   \n",
       "\n",
       "                                      Модель запроса  \\\n",
       "0  МТС Банк Premium Просьба Персональному менедже...   \n",
       "1           МТС Банк Не приходит СМС при авторизации   \n",
       "2       СБП Проблемы при пополнении из другого Банка   \n",
       "3  МТС Банк После ввода реквизитов и нажатию на к...   \n",
       "4     МТС Банк Некорректный баланс по картам клиента   \n",
       "\n",
       "                                                Тема  \\\n",
       "0  Зибель. [Тип запроса]: Просьба Персональному м...   \n",
       "1  Зибель. [Тип запроса]: Во время авторизации не...   \n",
       "2  Зибель. просьба помочь исправить у клиента в п...   \n",
       "3  Зибель. [Тип запроса]: Проблема с переводом по...   \n",
       "4  Зибель. [Тип запроса]: Неверный баланс по кред...   \n",
       "\n",
       "                                            Описание  \\\n",
       "0  [Тип запроса]: Просьба Персональному менеджеру...   \n",
       "1  [Тип запроса]: Во время авторизации не приходи...   \n",
       "2  просьба помочь исправить у клиента в приложени...   \n",
       "3  [Тип запроса]: Проблема с переводом по реквизи...   \n",
       "4  [Тип запроса]: Неверный баланс по кредитной ка...   \n",
       "\n",
       "                           ID1 я Группа                                Группа  \\\n",
       "0                             PMPremium                             PMPremium   \n",
       "1  Отдел поддержки карточных технологий  Отдел поддержки карточных технологий   \n",
       "2                            SBPSUPPORT                            SBPSUPPORT   \n",
       "3                         Поддержка FTB                   Приложение МТС Банк   \n",
       "4                 Корректировки баланса                 Корректировки баланса   \n",
       "\n",
       "              Код закрытия                              Информация о закрытии  \n",
       "0          Решён полностью                     ПМ Сбитинков Антон Анатольевич  \n",
       "1          Решён полностью  Мы получили от оператора связи статус доставки...  \n",
       "2         Известная ошибка  Клиент подключен к СБП. Переводы проходят успе...  \n",
       "3          Решён полностью  В связи с введением ограничений на стороне про...  \n",
       "4  Решён обходным решением  Закрыто по результатам сверки счетов. Если бал...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident_1 = pd.read_excel('/opt/share/DAOzerova/nlp/data/Инциденты 01-03.2023.xlsx')\n",
    "incident_2 = pd.read_excel('/opt/share/DAOzerova/nlp/data/Инциденты 04-06.2023.xlsx')\n",
    "incident_3 = pd.read_excel('/opt/share/DAOzerova/nlp/data/Инциденты 07-09.2023.xlsx')\n",
    "incident_4 = pd.read_excel('/opt/share/DAOzerova/nlp/data/Инциденты 10-12.2023.xlsx')\n",
    "incident_5 = pd.read_excel('/opt/share/DAOzerova/nlp/data/ЕИ_2024_01-04.xlsx')\n",
    "\n",
    "incident_1 = incident_1.rename(columns={\"Когда создано\": \"Дата открытия**\", \"Ответственная группа\": \"ID1 я Группа\", \"Назначено на группу\": \"Группа\", \"Номер\": \"Номер*\"})\n",
    "incident_2 = incident_2.rename(columns={\"Когда создано\": \"Дата открытия**\", \"Ответственная группа\": \"ID1 я Группа\", \"Назначено на группу\": \"Группа\", \"Номер\": \"Номер*\"})\n",
    "incident_3 = incident_3.rename(columns={\"Когда создано\": \"Дата открытия**\", \"Ответственная группа\": \"ID1 я Группа\", \"Назначено на группу\": \"Группа\", \"Номер\": \"Номер*\"})\n",
    "incident_4 = incident_4.rename(columns={\"Когда создано\": \"Дата открытия**\", \"Ответственная группа\": \"ID1 я Группа\", \"Назначено на группу\": \"Группа\", \"Номер\": \"Номер*\"})\n",
    "incident_5 = incident_5.rename(columns={\"Когда создано\": \"Дата открытия**\", \"Ответственная группа\": \"ID1 я Группа\", \"Назначено на группу\": \"Группа\", \"Номер\": \"Номер*\"})\n",
    "\n",
    "data = pd.concat([incident_1, incident_2, incident_3, incident_4, incident_5], ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481096, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_to_drop = [\n",
    "    'Первая линия Сервисдеск', '1ЛТП анализ процессов',  \n",
    "    'MB. SalesTools', 'CreditCards', 'NewHistory', \n",
    "    'Отдел контроля рисков', 'КЦ ВП Stream PFM'\n",
    "]\n",
    "\n",
    "df = data.copy()[~data['Группа'].isin(groups_to_drop)].reset_index(drop=True)\n",
    "\n",
    "df.loc[:, ['Услуга', 'Модель запроса', 'ID1 я Группа']] =\\\n",
    "df.loc[:, ['Услуга', 'Модель запроса', 'ID1 я Группа']].fillna('')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INC0299994    1\n",
       "INC0650940    1\n",
       "INC0650924    1\n",
       "INC0650927    1\n",
       "INC0650928    1\n",
       "Name: Номер*, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "INC0299994    1\n",
       "INC0650197    1\n",
       "INC0650199    1\n",
       "INC0650200    1\n",
       "INC0650202    1\n",
       "Name: Номер*, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df['Номер*'].value_counts()[:5])\n",
    "\n",
    "df.drop_duplicates(subset=['Номер*'], inplace=True, ignore_index=True)\n",
    "df.drop_duplicates(subset=['Описание'], inplace=True, ignore_index=True)\n",
    "\n",
    "df = df.dropna(subset=['Описание']).reset_index(drop=True)\n",
    "\n",
    "display(df['Номер*'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474570, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocesing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def drop_duplicates(txt):\n",
    "    try:\n",
    "        txt = re.split('\\. |: |\\n', txt)\n",
    "        counter_txt = Counter(txt)\n",
    "        if len(txt) < 2: return ''\n",
    "        i = len(txt)-2\n",
    "        while counter_txt[txt[i]] > 1:\n",
    "            i-=1\n",
    "    except:\n",
    "        return ''\n",
    "    if i==len(txt)-2:\n",
    "        return \" \".join(txt)\n",
    "    return \" \".join(txt[:i+1])\n",
    "\n",
    "def replace_numbers(txt):\n",
    "    client_pattern = r\"\\[ФИО клиента] \\b[А-ЯЁ]+ [А-ЯЁ]+ [А-ЯЁ]+\\b\" \n",
    "    initiator_pattern = r'(\\[Инициатор] [А-ЯЁ][а-яё]+ [А-ЯЁ][а-яё]+ [А-ЯЁ][а-яё]+)'\n",
    "    phone_pattern = r'\\b(\\+)?([78]\\d{10})\\b'\n",
    "    date_pattern = r'\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}'\n",
    "    time_pattern = r'(\\d{1,2}\\:)?\\d{1,2}\\:\\d{1,2}'\n",
    "    passport_pattern = r'\\d{4} \\d{6}'\n",
    "    nums_pattern = r'(\\d{1,2} )?\\d+(\\,\\d{1,2})?'\n",
    "    trash_pattern = r'УКАЖИ'\n",
    "    \n",
    "    new_text = re.sub(client_pattern, ' client_number ', txt)\n",
    "    new_text = re.sub(initiator_pattern, ' initiator_number ', new_text)\n",
    "    new_text = re.sub(phone_pattern, ' phone_number ', new_text)\n",
    "    new_text = re.sub(date_pattern, 'date ', new_text)\n",
    "    new_text = re.sub(time_pattern, 'time ', new_text)\n",
    "    new_text = re.sub(passport_pattern, 'passport ', new_text)\n",
    "    new_text = re.sub(nums_pattern, 'number ', new_text)\n",
    "    new_text = re.sub(trash_pattern, '', new_text)\n",
    "    \n",
    "    new_text = re.sub(r'[^\\w\\s]','', new_text).lower()\n",
    "    new_text = re.sub(r'[\\\\]',' ', new_text).lower()\n",
    "    return new_text\n",
    "\n",
    "def full_describe_request(txt, add_feature_txt, add_txt):\n",
    "    return txt + ' [' + str(add_feature_txt) + ' ]:' + str(add_txt)\n",
    "\n",
    "def parse(series, features, text_col = 'Описание'):\n",
    "    txt = '' if not isinstance(series[text_col], str) else series[text_col] \n",
    "    uniq_txt = drop_duplicates(txt)\n",
    "    \n",
    "    for feature in features:\n",
    "        uniq_txt = full_describe_request(uniq_txt, feature, series[feature])\n",
    "    txt = replace_numbers(uniq_txt)\n",
    "    return txt.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010886669158935547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 474570,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45071fdc0984b3fae72816790d473e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/474570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         тип запроса просьба персональному менеджеру св...\n",
      "1         тип запроса во время авторизации не приходит с...\n",
      "2         просьба помочь исправить у клиента в приложени...\n",
      "3         тип запроса проблема с переводом по реквизитам...\n",
      "4         тип запроса неверный баланс по кредитной карте...\n",
      "                                ...                        \n",
      "460668     описание проблемы клиента у клиента некоррект...\n",
      "460669     описание проблемы клиента не поступают запрос...\n",
      "460670     описание проблемы клиента у клиента при подпи...\n",
      "460671     описание проблемы клиента  телефон отправител...\n",
      "460672     описание проблемы клиента перевод по сбп не п...\n",
      "Name: Описание заявки, Length: 460673, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Описание заявки'] = df.progress_apply(\n",
    "    parse, \n",
    "    args = (\n",
    "     ['Услуга', 'Модель запроса', 'ID1 я Группа'],\n",
    "    ), \n",
    "    axis = 1,\n",
    ")\n",
    "\n",
    "df.drop_duplicates(subset=['Описание заявки'], inplace=True, ignore_index=True)\n",
    "\n",
    "request_txts = df['Описание заявки']\n",
    "\n",
    "print(df['Описание заявки'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TARGET ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Правильная заявка                       381047\n",
       "Другое: неправильная заявка              10297\n",
       "Авторизация. Неактуальная                 9415\n",
       "Поддержка ПЦ ЭК                           8712\n",
       "Приложение МТС Банк                       8317\n",
       "Поддержка бизнес процессов                7818\n",
       "Отдел активно-пассивных операций          6321\n",
       "Отдел поддержки карточных технологий      5452\n",
       "MB.oplata_dbo                             3928\n",
       "Платёжный Хаб                             3848\n",
       "DebitCardsSupport                         3140\n",
       "Переводы между своими счетами             3110\n",
       "Поддержка СДП.Платежи                     3015\n",
       "МТС Деньги ВЕБ                            2709\n",
       "Антифрод                                  2691\n",
       "Возврат средств                            853\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'] = df['Группа']\n",
    "df.loc[df['Группа'] == df['ID1 я Группа'], 'TARGET'] = 'Правильная заявка'\n",
    "df.loc[df['TARGET'].isin(df['TARGET'].value_counts()[15:].index), 'TARGET'] = 'Другое: неправильная заявка'\n",
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Услуга</th>\n",
       "      <th>Модель запроса</th>\n",
       "      <th>ID1 я Группа</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Сопровождение МТС Банк Premium (Премиум)</td>\n",
       "      <td>МТС Банк Premium Просьба Персональному менедже...</td>\n",
       "      <td>PMPremium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Сопровождение МТС Банк</td>\n",
       "      <td>МТС Банк Не приходит СМС при авторизации</td>\n",
       "      <td>Отдел поддержки карточных технологий</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при пополнении из другого Банка</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Обеспечение переводов по номеру телефона в МТС...</td>\n",
       "      <td>МТС Банк После ввода реквизитов и нажатию на к...</td>\n",
       "      <td>Поддержка FTB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обеспечение исправления некорректной информаци...</td>\n",
       "      <td>МТС Банк Некорректный баланс по картам клиента</td>\n",
       "      <td>Корректировки баланса</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460668</th>\n",
       "      <td>Обеспечение открытия продуктов в МТС Банк</td>\n",
       "      <td>МТС Банк Проблемы с дебетовыми картами</td>\n",
       "      <td>Приложение МТС Банк</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460669</th>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460670</th>\n",
       "      <td>Сопровождение CRM Siebel</td>\n",
       "      <td>CRM Siebel Проблема оформления и выдачи карты ...</td>\n",
       "      <td>Поддержка бизнес процессов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460671</th>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460672</th>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460673 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Услуга  \\\n",
       "0                Сопровождение МТС Банк Premium (Премиум)   \n",
       "1                                  Сопровождение МТС Банк   \n",
       "2                                       Сопровождение СБП   \n",
       "3       Обеспечение переводов по номеру телефона в МТС...   \n",
       "4       Обеспечение исправления некорректной информаци...   \n",
       "...                                                   ...   \n",
       "460668          Обеспечение открытия продуктов в МТС Банк   \n",
       "460669                                  Сопровождение СБП   \n",
       "460670                           Сопровождение CRM Siebel   \n",
       "460671                                  Сопровождение СБП   \n",
       "460672                                  Сопровождение СБП   \n",
       "\n",
       "                                           Модель запроса  \\\n",
       "0       МТС Банк Premium Просьба Персональному менедже...   \n",
       "1                МТС Банк Не приходит СМС при авторизации   \n",
       "2            СБП Проблемы при пополнении из другого Банка   \n",
       "3       МТС Банк После ввода реквизитов и нажатию на к...   \n",
       "4          МТС Банк Некорректный баланс по картам клиента   \n",
       "...                                                   ...   \n",
       "460668             МТС Банк Проблемы с дебетовыми картами   \n",
       "460669   СБП Проблемы при отправке перевода в другой Банк   \n",
       "460670  CRM Siebel Проблема оформления и выдачи карты ...   \n",
       "460671   СБП Проблемы при отправке перевода в другой Банк   \n",
       "460672   СБП Проблемы при отправке перевода в другой Банк   \n",
       "\n",
       "                                ID1 я Группа  \n",
       "0                                  PMPremium  \n",
       "1       Отдел поддержки карточных технологий  \n",
       "2                                 SBPSUPPORT  \n",
       "3                              Поддержка FTB  \n",
       "4                      Корректировки баланса  \n",
       "...                                      ...  \n",
       "460668                   Приложение МТС Банк  \n",
       "460669                            SBPSUPPORT  \n",
       "460670            Поддержка бизнес процессов  \n",
       "460671                            SBPSUPPORT  \n",
       "460672                            SBPSUPPORT  \n",
       "\n",
       "[460673 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Услуга', 'Модель запроса', 'ID1 я Группа']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Описание заявки</th>\n",
       "      <th>Тема</th>\n",
       "      <th>Услуга</th>\n",
       "      <th>Модель запроса</th>\n",
       "      <th>ID1 я Группа</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тип запроса просьба персональному менеджеру св...</td>\n",
       "      <td>Зибель. [Тип запроса]: Просьба Персональному м...</td>\n",
       "      <td>Сопровождение МТС Банк Premium (Премиум)</td>\n",
       "      <td>МТС Банк Premium Просьба Персональному менедже...</td>\n",
       "      <td>PMPremium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тип запроса во время авторизации не приходит с...</td>\n",
       "      <td>Зибель. [Тип запроса]: Во время авторизации не...</td>\n",
       "      <td>Сопровождение МТС Банк</td>\n",
       "      <td>МТС Банк Не приходит СМС при авторизации</td>\n",
       "      <td>Отдел поддержки карточных технологий</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>просьба помочь исправить у клиента в приложени...</td>\n",
       "      <td>Зибель. просьба помочь исправить у клиента в п...</td>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при пополнении из другого Банка</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>тип запроса проблема с переводом по реквизитам...</td>\n",
       "      <td>Зибель. [Тип запроса]: Проблема с переводом по...</td>\n",
       "      <td>Обеспечение переводов по номеру телефона в МТС...</td>\n",
       "      <td>МТС Банк После ввода реквизитов и нажатию на к...</td>\n",
       "      <td>Поддержка FTB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тип запроса неверный баланс по кредитной карте...</td>\n",
       "      <td>Зибель. [Тип запроса]: Неверный баланс по кред...</td>\n",
       "      <td>Обеспечение исправления некорректной информаци...</td>\n",
       "      <td>МТС Банк Некорректный баланс по картам клиента</td>\n",
       "      <td>Корректировки баланса</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460668</th>\n",
       "      <td>описание проблемы клиента у клиента некоррект...</td>\n",
       "      <td>Зибель. УКАЖИ\\n[Описание проблемы клиента]: У ...</td>\n",
       "      <td>Обеспечение открытия продуктов в МТС Банк</td>\n",
       "      <td>МТС Банк Проблемы с дебетовыми картами</td>\n",
       "      <td>Приложение МТС Банк</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460669</th>\n",
       "      <td>описание проблемы клиента не поступают запрос...</td>\n",
       "      <td>Зибель. УКАЖИ\\n[Описание проблемы клиента]: не...</td>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460670</th>\n",
       "      <td>описание проблемы клиента у клиента при подпи...</td>\n",
       "      <td>Зибель. УКАЖИ\\n[Описание проблемы клиента]: у ...</td>\n",
       "      <td>Сопровождение CRM Siebel</td>\n",
       "      <td>CRM Siebel Проблема оформления и выдачи карты ...</td>\n",
       "      <td>Поддержка бизнес процессов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460671</th>\n",
       "      <td>описание проблемы клиента  телефон отправител...</td>\n",
       "      <td>Зибель. УКАЖИ\\n[Описание проблемы клиента]: \\n...</td>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460672</th>\n",
       "      <td>описание проблемы клиента перевод по сбп не п...</td>\n",
       "      <td>Зибель. УКАЖИ\\n[Описание проблемы клиента]: Пе...</td>\n",
       "      <td>Сопровождение СБП</td>\n",
       "      <td>СБП Проблемы при отправке перевода в другой Банк</td>\n",
       "      <td>SBPSUPPORT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Описание заявки  \\\n",
       "0       тип запроса просьба персональному менеджеру св...   \n",
       "1       тип запроса во время авторизации не приходит с...   \n",
       "2       просьба помочь исправить у клиента в приложени...   \n",
       "3       тип запроса проблема с переводом по реквизитам...   \n",
       "4       тип запроса неверный баланс по кредитной карте...   \n",
       "...                                                   ...   \n",
       "460668   описание проблемы клиента у клиента некоррект...   \n",
       "460669   описание проблемы клиента не поступают запрос...   \n",
       "460670   описание проблемы клиента у клиента при подпи...   \n",
       "460671   описание проблемы клиента  телефон отправител...   \n",
       "460672   описание проблемы клиента перевод по сбп не п...   \n",
       "\n",
       "                                                     Тема  \\\n",
       "0       Зибель. [Тип запроса]: Просьба Персональному м...   \n",
       "1       Зибель. [Тип запроса]: Во время авторизации не...   \n",
       "2       Зибель. просьба помочь исправить у клиента в п...   \n",
       "3       Зибель. [Тип запроса]: Проблема с переводом по...   \n",
       "4       Зибель. [Тип запроса]: Неверный баланс по кред...   \n",
       "...                                                   ...   \n",
       "460668  Зибель. УКАЖИ\\n[Описание проблемы клиента]: У ...   \n",
       "460669  Зибель. УКАЖИ\\n[Описание проблемы клиента]: не...   \n",
       "460670  Зибель. УКАЖИ\\n[Описание проблемы клиента]: у ...   \n",
       "460671  Зибель. УКАЖИ\\n[Описание проблемы клиента]: \\n...   \n",
       "460672  Зибель. УКАЖИ\\n[Описание проблемы клиента]: Пе...   \n",
       "\n",
       "                                                   Услуга  \\\n",
       "0                Сопровождение МТС Банк Premium (Премиум)   \n",
       "1                                  Сопровождение МТС Банк   \n",
       "2                                       Сопровождение СБП   \n",
       "3       Обеспечение переводов по номеру телефона в МТС...   \n",
       "4       Обеспечение исправления некорректной информаци...   \n",
       "...                                                   ...   \n",
       "460668          Обеспечение открытия продуктов в МТС Банк   \n",
       "460669                                  Сопровождение СБП   \n",
       "460670                           Сопровождение CRM Siebel   \n",
       "460671                                  Сопровождение СБП   \n",
       "460672                                  Сопровождение СБП   \n",
       "\n",
       "                                           Модель запроса  \\\n",
       "0       МТС Банк Premium Просьба Персональному менедже...   \n",
       "1                МТС Банк Не приходит СМС при авторизации   \n",
       "2            СБП Проблемы при пополнении из другого Банка   \n",
       "3       МТС Банк После ввода реквизитов и нажатию на к...   \n",
       "4          МТС Банк Некорректный баланс по картам клиента   \n",
       "...                                                   ...   \n",
       "460668             МТС Банк Проблемы с дебетовыми картами   \n",
       "460669   СБП Проблемы при отправке перевода в другой Банк   \n",
       "460670  CRM Siebel Проблема оформления и выдачи карты ...   \n",
       "460671   СБП Проблемы при отправке перевода в другой Банк   \n",
       "460672   СБП Проблемы при отправке перевода в другой Банк   \n",
       "\n",
       "                                ID1 я Группа  \n",
       "0                                  PMPremium  \n",
       "1       Отдел поддержки карточных технологий  \n",
       "2                                 SBPSUPPORT  \n",
       "3                              Поддержка FTB  \n",
       "4                      Корректировки баланса  \n",
       "...                                      ...  \n",
       "460668                   Приложение МТС Банк  \n",
       "460669                            SBPSUPPORT  \n",
       "460670            Поддержка бизнес процессов  \n",
       "460671                            SBPSUPPORT  \n",
       "460672                            SBPSUPPORT  \n",
       "\n",
       "[460673 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'Описание заявки' not in df.columns:\n",
    "    df = df.join(request_txts.rename('Описание заявки'))\n",
    "    \n",
    "df[['Номер*', 'Описание заявки']].to_parquet(\"requests_bert.parquet\", index=False, compression='gzip')\n",
    "request_txts = pd.read_parquet(\"requests_bert.parquet\").reset_index(drop=True)\n",
    "\n",
    "if 'Описание заявки' in df.columns:\n",
    "    df.drop(columns=['Описание заявки'], inplace=True)\n",
    "    \n",
    "df = df.merge(request_txts, on = 'Номер*', how = 'left')\n",
    "df[['Описание заявки', 'Тема', 'Услуга', 'Модель запроса', 'ID1 я Группа']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0         14\n",
       " 1         14\n",
       " 2         14\n",
       " 3         15\n",
       " 4         14\n",
       "           ..\n",
       " 460668    14\n",
       " 460669    14\n",
       " 460670    14\n",
       " 460671    14\n",
       " 460672    14\n",
       " Name: TARGET_NUM, Length: 460673, dtype: int64,\n",
       " 0         1\n",
       " 1         1\n",
       " 2         1\n",
       " 3         0\n",
       " 4         1\n",
       "          ..\n",
       " 460668    1\n",
       " 460669    1\n",
       " 460670    1\n",
       " 460671    1\n",
       " 460672    1\n",
       " Name: TARGET_BINARY, Length: 460673, dtype: int64,\n",
       " ['DebitCardsSupport',\n",
       "  'MB.oplata_dbo',\n",
       "  'Авторизация. Неактуальная',\n",
       "  'Антифрод',\n",
       "  'Возврат средств',\n",
       "  'Другое: неправильная заявка',\n",
       "  'МТС Деньги ВЕБ',\n",
       "  'Отдел активно-пассивных операций',\n",
       "  'Отдел поддержки карточных технологий',\n",
       "  'Переводы между своими счетами',\n",
       "  'Платёжный Хаб',\n",
       "  'Поддержка ПЦ ЭК',\n",
       "  'Поддержка СДП.Платежи',\n",
       "  'Поддержка бизнес процессов',\n",
       "  'Правильная заявка',\n",
       "  'Приложение МТС Банк'],\n",
       " 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df['TARGET'].value_counts().index)\n",
    "df['TARGET_NUM'] = le.transform(df['TARGET'])\n",
    "\n",
    "df['TARGET_BINARY'] = df['TARGET_NUM'].eq(14).astype(int)\n",
    "\n",
    "df['TARGET_NUM'], df['TARGET_BINARY'], list(le.classes_), len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a5wWsPtEuE3w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences= df['Описание заявки'].values\n",
    "labels = df['TARGET_NUM'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['Описание заявки','TARGET_NUM']].to_parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('data/x_train.parquet')\n",
    "y_train = pd.read_parquet('data/y_train.parquet')\n",
    "X_test = pd.read_parquet('data/x_test.parquet')\n",
    "y_test = pd.read_parquet('data/y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['мр сибирь омск платеж  электронный кошелек числиться операция  отклонять банк банк сообщать    денежный средство  отправлять    банк  возвращаться  выписка  мочь предоставлять  банк скопировать полностью строка  рм платеж  электронный кошелек   время  дата событие  number number  номер транзакция  number  сумма перевод  number  способ перевод выбирать значение  мтс  источник перевод выбирать значение  карта  банк  направление перевод выбирать значение  баланс телефон  баланс  баланс  первый number   последний number  цифра номер карта  который  попытка перевод  number xxxxxxnumber  пошаговый действие  оплата   мтс реквизит получатель платеж   phone_number  номер абонент  phone_number  imsi number  номер simкарты number  категория надежность категория надежность number  биллинговый группа мобильный связь стандартный название биллингаасра foris услуга  тп приложение мтс банк инициатор remedy мтс тема мтс деньги непоступление платеж услуга обеспечение перевод  мтс банк модель запрос мтс банк ошибка  списание средство  кошелек  номер телефон idnumber   группа поддержка пц эк',\n",
       "       'инн number  наименование компания лак общество  ограниченный ответственность контактный лицо матвеев елена павловна контактный номер телефон number number  статус клиент корпоративный клиент срочность запрос высокий  initiator_number  тема заявл  изготовление сертификат описание дд просить принимать заявление клиент_xnumber d_  услуга сопровождение дбо юр лицо модель запрос клиент банк юла консультация  установкенастройка idnumber   группа отдел поддержка дбо юла',\n",
       "       'тип запрос проблема  услуга управлять кредит описание проблема клиент  находить подходящий тематика клиент  приходить сообщение  банк начинать  date     видеть сообщение  отправлять  статус доставлять  клиент говорить  сообщение переставать поступать просить исправлять описание заявка id рбо number  омт номер телефон  phone_number   initiator_number  тема зибель услуга обеспечение нецелевой кредит  мтс банк модель запрос мтс банк проблема  оформление  обслуживание кредит нцпк рефинансирование кпз ипотека idnumber   группа приложение кредит нцпк',\n",
       "       ...,\n",
       "       'тип запрос проблема  платеж  оплата услуга steam жкх  др описание проблема клиент  клиент возникать ошибка  перевод денежный средство  карта авторизованный зона указывать данета  дата  время попыткиперевод date  год time  источник списание карта number  сумма операция number  рубль модель телефон ios  client_number  rbo_id number  номер телефон  приложение  phone_number   initiator_number  тема зибель услуга обеспечение платежейавтоплатеж  мтс банк модель запрос мтс банк ошибка  проведение оплата услуга idnumber   группа платежный хаб',\n",
       "       'мр северозапад мурманск  получаться подключать автоплатеж точный дата возникновение проблема  date  способ подключение выбирать значение приложение  мтс  маскировать номер банковский карта  оплата  карта  number   number  номер абонент  phone_number  imsi number  номер simкарты number  категория надежность категория доверие number  биллинговый группа _мтс мурманск murnumber  название биллингаасра foris услуга  тп приложение мтс банк услуга servicedesk модель запрос  idnumber   группа ',\n",
       "       'тип запрос указывать название акция  описание проблема клиент  клиент      отображаться информация  акция приветственный кешбек     клиент  отображаться информация удаваться   приложение зарегистрироваться   акция   номер счет депозит number  номер счет кредит number   initiator_number  тема зибель услуга сопровождение маркетинговый кампания акция модель запрос ошибка  портфельный акция idnumber   группа портфельный акция'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train['Описание заявки'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((308471,), (308471,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train['Описание заявки'].shape, y_train['TARGET_NUM'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences= X_train['Описание заявки'].values\n",
    "labels = y_train['TARGET_NUM'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS634tQa0Gb5"
   },
   "source": [
    "# DATA prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ai-forever/sbert_large_nlu_ru\", \n",
    "    do_lower_case=True,\n",
    ")\n",
    "    \n",
    "replace_tokens = {\n",
    "        '[unused1]': 'мтс',\n",
    "        '[unused2]': 'client_number',\n",
    "        '[unused3]': 'initiator_number',\n",
    "        '[unused4]': 'phone_number',\n",
    "        '[unused5]': 'date',\n",
    "        '[unused6]': 'passport',\n",
    "        '[unused7]': 'number',\n",
    "        '[unused8]': 'premium',\n",
    "}\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "    \n",
    "for old_token, new_token in replace_tokens.items():\n",
    "    idx = vocab[old_token]\n",
    "    del vocab[old_token]\n",
    "    vocab[new_token] = idx\n",
    "    \n",
    "# Тут надо отсортировать, иначе далее при инициализации токенайзера порядок ломается\n",
    "vocab = pd.Series(vocab).sort_values()\n",
    "vocab = vocab.index.tolist()\n",
    "    \n",
    "with open('vocab.tmp', 'w', encoding = 'utf-8') as tmp_vocab_file:\n",
    "    tmp_vocab_file.write('\\n'.join(vocab))\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ai-forever/sbert_large_nlu_ru\", \n",
    "    vocab_file = 'vocab.tmp', do_lower_case=True, do_basic_tokenize=False\n",
    ")\n",
    "    \n",
    "os.remove('vocab.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original мр сибирь омск платеж  электронный кошелек числиться операция  отклонять банк банк сообщать    денежный средство  отправлять    банк  возвращаться  выписка  мочь предоставлять  банк скопировать полностью строка  рм платеж  электронный кошелек   время  дата событие  number number  номер транзакция  number  сумма перевод  number  способ перевод выбирать значение  мтс  источник перевод выбирать значение  карта  банк  направление перевод выбирать значение  баланс телефон  баланс  баланс  первый number   последний number  цифра номер карта  который  попытка перевод  number xxxxxxnumber  пошаговый действие  оплата   мтс реквизит получатель платеж   phone_number  номер абонент  phone_number  imsi number  номер simкарты number  категория надежность категория надежность number  биллинговый группа мобильный связь стандартный название биллингаасра foris услуга  тп приложение мтс банк инициатор remedy мтс тема мтс деньги непоступление платеж услуга обеспечение перевод  мтс банк модель запрос мтс банк ошибка  списание средство  кошелек  номер телефон idnumber   группа поддержка пц эк\n",
      "tokenised ['[CLS]', 'м', '##р', 'сибир', '##ь', 'ом', '##ск', 'платеж', 'электрон', '##ны', '##и', 'кошелек', 'числи', '##ться', 'операция', 'отклон', '##ять', 'банк', 'банк', 'сообщать', 'дене', '##жны', '##и', 'средство', 'отправлять', 'банк', 'возвращаться', 'выпи', '##ска', 'мо', '##чь', 'предоставлять', 'банк', 'скопировать', 'полностью', 'строка', 'р', '##м', 'платеж', 'электрон', '##ны', '##и', 'кошелек', 'время', 'дата', 'событие', 'number', 'number', 'номер', 'транзак', '##ция', 'number', 'сумма', 'перевод', 'number', 'способ', 'перевод', 'выбирать', 'значение', 'мтс', 'источник', 'перевод', 'выбирать', 'значение', 'карта', 'банк', 'направление', 'перевод', 'выбирать', 'значение', 'баланс', 'телефон', 'баланс', 'баланс', 'пер', '##вы', '##и', 'number', 'послед', '##нии', 'number', 'цифра', 'номер', 'карта', 'которы', '##и', 'попытка', 'перевод', 'number', 'x', '##xx', '##xx', '##x', '##num', '##ber', 'поша', '##гов', '##ы', '##и', 'де', '##ист', '##вие', 'оплата', 'мтс', 'реквизи', '##т', 'получат', '##ель', 'платеж', 'ph', '##one', '_', 'number', 'номер', 'абонент', 'ph', '##one', '_', 'number', 'im', '##s', '##i', 'number', 'номер', 'sim', '##карты', 'number', 'категория', 'надежность', 'категория', 'надежность', 'number', 'бил', '##лингов', '##ы', '##и', 'группа', 'моби', '##льны', '##и', 'связь', 'стандарт', '##ны', '##и', 'название', 'бил', '##линга', '##ас', '##ра', 'for', '##is', 'услуга', 'тп', 'приложение', 'мтс', 'банк', 'инициатор', 'rem', '##edy', 'мтс', 'тема', 'мтс', 'деньги', 'непо', '##ступление', 'платеж', 'услуга', 'обеспечение', 'перевод', 'мтс', 'банк', 'модель', 'запрос', 'мтс', 'банк', 'ошибка', 'списание', 'средство', 'кошелек', 'номер', 'телефон', 'id', '##num', '##ber', 'группа', 'поддержка', 'п', '##ц', 'эк', '[SEP]']\n",
      "toden IDs [101, 115, 382, 24836, 394, 16959, 932, 54570, 70016, 667, 378, 41486, 17090, 816, 11803, 43456, 1389, 6739, 6739, 32614, 3008, 10787, 378, 14443, 28610, 6739, 14716, 12537, 841, 726, 1165, 28161, 6739, 88334, 3177, 63594, 111, 386, 54570, 70016, 667, 378, 41486, 1012, 14919, 13331, 7, 7, 5529, 62173, 977, 7, 9464, 8925, 7, 2119, 8925, 16098, 6436, 1, 5500, 8925, 16098, 6436, 12608, 6739, 11223, 8925, 16098, 6436, 22178, 5431, 22178, 22178, 944, 752, 378, 7, 1388, 939, 7, 23499, 5529, 12608, 862, 378, 11990, 8925, 7, 239, 87284, 87284, 510, 88581, 7334, 32854, 1761, 391, 378, 784, 34083, 41674, 45616, 1, 49303, 380, 13679, 2502, 54570, 35745, 7500, 233, 7, 5529, 111919, 35745, 7500, 233, 7, 18854, 454, 441, 7, 5529, 77546, 84924, 7, 23536, 27425, 23536, 27425, 7, 20725, 27558, 391, 378, 3513, 6625, 20900, 378, 6800, 30899, 667, 378, 4277, 20725, 35643, 15060, 649, 4917, 2384, 52460, 79082, 23256, 1, 6739, 94880, 55651, 68507, 1, 9176, 1, 2713, 2164, 21451, 54570, 52460, 11708, 8925, 1, 6739, 7537, 15529, 1, 6739, 15988, 85436, 14443, 41486, 5529, 5431, 8628, 88581, 7334, 3513, 14282, 117, 405, 1267, 102]\n"
     ]
    }
   ],
   "source": [
    "print('original', sentences[0])\n",
    "print('tokenised', tokenizer.tokenize(sentences[0], add_special_tokens=True))\n",
    "print('toden IDs', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0], add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A72OmrwfzU7H",
    "outputId": "39645b0c-ddff-4f20-c34f-db7a02280518",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(sentences[0], add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALe8kSUQy7GV",
    "outputId": "bbb0fcd7-ce96-4621-9b54-9ef5ec29324a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011325836181640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 308471,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1922c57eae48fea957796c50d5efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6982"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "input_ids_all = []\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    input_ids_all.append(input_ids)\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TLvX12nztw3",
    "outputId": "66b3b143-90da-4123-aa89-ce39cc460588",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010382413864135742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 308471,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69f7e3e7779435fb3b82521131863ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/daozerova/.conda/envs/fl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мр сибирь омск платеж  электронный кошелек числиться операция  отклонять банк банк сообщать    денежный средство  отправлять    банк  возвращаться  выписка  мочь предоставлять  банк скопировать полностью строка  рм платеж  электронный кошелек   время  дата событие  number number  номер транзакция  number  сумма перевод  number  способ перевод выбирать значение  мтс  источник перевод выбирать значение  карта  банк  направление перевод выбирать значение  баланс телефон  баланс  баланс  первый number   последний number  цифра номер карта  который  попытка перевод  number xxxxxxnumber  пошаговый действие  оплата   мтс реквизит получатель платеж   phone_number  номер абонент  phone_number  imsi number  номер simкарты number  категория надежность категория надежность number  биллинговый группа мобильный связь стандартный название биллингаасра foris услуга  тп приложение мтс банк инициатор remedy мтс тема мтс деньги непоступление платеж услуга обеспечение перевод  мтс банк модель запрос мтс банк ошибка  списание средство  кошелек  номер телефон idnumber   группа поддержка пц эк\n",
      "tensor([   101,    115,    382,  24836,    394,  16959,    932,  54570,  70016,\n",
      "           667,    378,  41486,  17090,    816,  11803,  43456,   1389,   6739,\n",
      "          6739,  32614,   3008,  10787,    378,  14443,  28610,   6739,  14716,\n",
      "         12537,    841,    726,   1165,  28161,   6739,  88334,   3177,  63594,\n",
      "           111,    386,  54570,  70016,    667,    378,  41486,   1012,  14919,\n",
      "         13331,      7,      7,   5529,  62173,    977,      7,   9464,   8925,\n",
      "             7,   2119,   8925,  16098,   6436,      1,   5500,   8925,  16098,\n",
      "          6436,  12608,   6739,  11223,   8925,  16098,   6436,  22178,   5431,\n",
      "         22178,  22178,    944,    752,    378,      7,   1388,    939,      7,\n",
      "         23499,   5529,  12608,    862,    378,  11990,   8925,      7,    239,\n",
      "         87284,  87284,    510,  88581,   7334,  32854,   1761,    391,    378,\n",
      "           784,  34083,  41674,  45616,      1,  49303,    380,  13679,   2502,\n",
      "         54570,  35745,   7500,    233,      7,   5529, 111919,  35745,   7500,\n",
      "           233,      7,  18854,    454,    441,      7,   5529,  77546,  84924,\n",
      "             7,  23536,  27425,  23536,  27425,      7,  20725,  27558,    391,\n",
      "           378,   3513,   6625,  20900,    378,   6800,  30899,    667,    378,\n",
      "          4277,  20725,  35643,  15060,    649,   4917,   2384,  52460,  79082,\n",
      "         23256,      1,   6739,  94880,  55651,  68507,      1,   9176,      1,\n",
      "          2713,   2164,  21451,  54570,  52460,  11708,   8925,      1,   6739,\n",
      "          7537,  15529,      1,   6739,  15988,  85436,  14443,  41486,   5529,\n",
      "          5431,   8628,  88581,   7334,   3513,  14282,    117,    405,   1267,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "      sent,\n",
    "      add_special_tokens=True,\n",
    "      max_length=256,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "  )\n",
    "  input_ids.append(encoded_dict['input_ids'])\n",
    "  attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(sentences[0])\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_-5tlCN7LHt",
    "outputId": "bf491e12-4713-49fa-d634-a9ea7c90c4fc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308471, 308471, 308471)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids), len(attention_masks), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293,047 training samples\n",
      "15,424 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pa-B7kMq54in",
    "outputId": "77f3c291-e249-4037-814e-aff8a9c6a9bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368,538 training samples\n",
      "46,067 validation samples\n",
      "46,068 testing samples\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.5 * (len(dataset) - train_size))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size + test_size])\n",
    "val_data, test_data = random_split(val_data, [val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} testing samples'.format(test_size))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uPHIKv2vEvaL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler = RandomSampler(train_data),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    sampler = SequentialSampler(val_data),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K31x6eRKz9rX"
   },
   "source": [
    "# BERT prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.23052978515625\n",
      "79.138427734375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "a, b = torch.cuda.mem_get_info()\n",
    "print(a / 1024 / 1024 / 1024)\n",
    "print(b / 1024 / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VWxn1LwvVXd",
    "outputId": "d1634714-0d1e-4587-f543-91b60a0462bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/sbert_large_nlu_ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"ai-forever/sbert_large_nlu_ru\", #'bert-base-uncased',\n",
    "    num_labels = 16,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_FunTkCHmQH",
    "outputId": "ee777c94-406a-4420-8ef5-b8ba6531060a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daozerova/.conda/envs/fl/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YyGwKYeIIPe_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler =  get_linear_schedule_with_warmup(optimizer,\n",
    "                                             num_warmup_steps = 0,\n",
    "                                             num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data.indices, train_data.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvwtTC66ztkJ"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82R2Pvc60kI9"
   },
   "source": [
    "**Training:**\n",
    "* Распаковать обучающие данные и лейблы\n",
    "* Загрузить данные на GPU для ускорения\n",
    "* Занулить градиенты с предыдущего шага\n",
    "* Forward pass (скормить данные в нейросеть и пробросить их вперед)\n",
    "* Backward pass (back propagation - посчитать градиенты по всем параметрам с помощью обратного распространения ошибки)\n",
    "* Обновить параметры с помощью optimizer.step()\n",
    "* Посчитать статистики, чтобы следить за обучением\n",
    "⛅\n",
    "\n",
    "**Evaluation:**\n",
    "* Распаковать валидационные данные и лейблы\n",
    "* Загрузить данные на GPU для ускорения\n",
    "* Forward pass (скормить данные в нейросеть и пробросить вперед)\n",
    "* Посчиатть loss и статистики для валиданционных данных, чтобы следить за обучением\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.13677978515625\n",
      "79.138427734375\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "a, b = torch.cuda.mem_get_info()\n",
    "print(a / 1024 / 1024 / 1024)\n",
    "print(b / 1024 / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nWIeCSvJJBoh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def  format_time(elapsed):\n",
    "  elapsed_rounded = int(round(elapsed))\n",
    "  return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVgWXgcUPrb0",
    "outputId": "3608f8a5-0c08-4c8e-a348-263206d4ddd1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= EPOCH 1 / 3 ======\n",
      "Training...\n",
      "   Batch    40 of 293047. Elapsed: 0:00:42 Loss: 0.6315  Average_Train_Loss: 0.0337\n",
      "   Batch    80 of 293047. Elapsed: 0:01:25 Loss: 0.8407  Average_Train_Loss: 0.0302\n",
      "   Batch   120 of 293047. Elapsed: 0:02:08 Loss: 0.2523  Average_Train_Loss: 0.0275\n",
      "   Batch   160 of 293047. Elapsed: 0:02:50 Loss: 0.6265  Average_Train_Loss: 0.0262\n",
      "   Batch   200 of 293047. Elapsed: 0:03:32 Loss: 0.6306  Average_Train_Loss: 0.0255\n",
      "   Batch   240 of 293047. Elapsed: 0:04:14 Loss: 0.4877  Average_Train_Loss: 0.0247\n",
      "   Batch   280 of 293047. Elapsed: 0:04:57 Loss: 0.5659  Average_Train_Loss: 0.0238\n",
      "   Batch   320 of 293047. Elapsed: 0:05:39 Loss: 0.5523  Average_Train_Loss: 0.0233\n",
      "   Batch   360 of 293047. Elapsed: 0:06:21 Loss: 0.5753  Average_Train_Loss: 0.0226\n",
      "   Batch   400 of 293047. Elapsed: 0:07:03 Loss: 0.6183  Average_Train_Loss: 0.0221\n",
      "   Batch   440 of 293047. Elapsed: 0:07:46 Loss: 0.5305  Average_Train_Loss: 0.0215\n",
      "   Batch   480 of 293047. Elapsed: 0:08:28 Loss: 0.4064  Average_Train_Loss: 0.0212\n",
      "   Batch   520 of 293047. Elapsed: 0:09:10 Loss: 0.4505  Average_Train_Loss: 0.0210\n",
      "   Batch   560 of 293047. Elapsed: 0:09:52 Loss: 0.9204  Average_Train_Loss: 0.0206\n",
      "   Batch   600 of 293047. Elapsed: 0:10:35 Loss: 0.5660  Average_Train_Loss: 0.0204\n",
      "   Batch   640 of 293047. Elapsed: 0:11:17 Loss: 0.4204  Average_Train_Loss: 0.0201\n",
      "   Batch   680 of 293047. Elapsed: 0:11:59 Loss: 0.3446  Average_Train_Loss: 0.0198\n",
      "   Batch   720 of 293047. Elapsed: 0:12:42 Loss: 0.3232  Average_Train_Loss: 0.0196\n",
      "   Batch   760 of 293047. Elapsed: 0:13:24 Loss: 0.2735  Average_Train_Loss: 0.0194\n",
      "   Batch   800 of 293047. Elapsed: 0:14:06 Loss: 0.3168  Average_Train_Loss: 0.0192\n",
      "   Batch   840 of 293047. Elapsed: 0:14:49 Loss: 0.6205  Average_Train_Loss: 0.0191\n",
      "   Batch   880 of 293047. Elapsed: 0:15:31 Loss: 0.3732  Average_Train_Loss: 0.0189\n",
      "   Batch   920 of 293047. Elapsed: 0:16:13 Loss: 0.1047  Average_Train_Loss: 0.0187\n",
      "   Batch   960 of 293047. Elapsed: 0:16:55 Loss: 0.2785  Average_Train_Loss: 0.0186\n",
      "   Batch  1000 of 293047. Elapsed: 0:17:38 Loss: 0.6586  Average_Train_Loss: 0.0185\n",
      "   Batch  1040 of 293047. Elapsed: 0:18:20 Loss: 0.3661  Average_Train_Loss: 0.0183\n",
      "   Batch  1080 of 293047. Elapsed: 0:19:02 Loss: 0.2438  Average_Train_Loss: 0.0182\n",
      "   Batch  1120 of 293047. Elapsed: 0:19:44 Loss: 0.8545  Average_Train_Loss: 0.0181\n",
      "   Batch  1160 of 293047. Elapsed: 0:20:27 Loss: 0.3792  Average_Train_Loss: 0.0181\n",
      "   Batch  1200 of 293047. Elapsed: 0:21:09 Loss: 0.3415  Average_Train_Loss: 0.0180\n",
      "   Batch  1240 of 293047. Elapsed: 0:21:51 Loss: 0.9565  Average_Train_Loss: 0.0179\n",
      "   Batch  1280 of 293047. Elapsed: 0:22:34 Loss: 0.3483  Average_Train_Loss: 0.0178\n",
      "   Batch  1320 of 293047. Elapsed: 0:23:16 Loss: 0.4260  Average_Train_Loss: 0.0177\n",
      "   Batch  1360 of 293047. Elapsed: 0:23:58 Loss: 0.5774  Average_Train_Loss: 0.0176\n",
      "   Batch  1400 of 293047. Elapsed: 0:24:41 Loss: 0.2515  Average_Train_Loss: 0.0176\n",
      "   Batch  1440 of 293047. Elapsed: 0:25:23 Loss: 0.6047  Average_Train_Loss: 0.0175\n",
      "   Batch  1480 of 293047. Elapsed: 0:26:05 Loss: 0.2578  Average_Train_Loss: 0.0174\n",
      "   Batch  1520 of 293047. Elapsed: 0:26:47 Loss: 0.3162  Average_Train_Loss: 0.0173\n",
      "   Batch  1560 of 293047. Elapsed: 0:27:30 Loss: 0.5725  Average_Train_Loss: 0.0173\n",
      "   Batch  1600 of 293047. Elapsed: 0:28:12 Loss: 0.6670  Average_Train_Loss: 0.0171\n",
      "   Batch  1640 of 293047. Elapsed: 0:28:54 Loss: 0.2935  Average_Train_Loss: 0.0170\n",
      "   Batch  1680 of 293047. Elapsed: 0:29:36 Loss: 0.5000  Average_Train_Loss: 0.0170\n",
      "   Batch  1720 of 293047. Elapsed: 0:30:19 Loss: 0.2056  Average_Train_Loss: 0.0169\n",
      "   Batch  1760 of 293047. Elapsed: 0:31:01 Loss: 0.4122  Average_Train_Loss: 0.0168\n",
      "   Batch  1800 of 293047. Elapsed: 0:31:43 Loss: 0.1925  Average_Train_Loss: 0.0168\n",
      "   Batch  1840 of 293047. Elapsed: 0:32:26 Loss: 0.8283  Average_Train_Loss: 0.0168\n",
      "   Batch  1880 of 293047. Elapsed: 0:33:08 Loss: 0.1296  Average_Train_Loss: 0.0167\n",
      "   Batch  1920 of 293047. Elapsed: 0:33:50 Loss: 0.3707  Average_Train_Loss: 0.0166\n",
      "   Batch  1960 of 293047. Elapsed: 0:34:33 Loss: 0.2423  Average_Train_Loss: 0.0166\n",
      "   Batch  2000 of 293047. Elapsed: 0:35:15 Loss: 0.4558  Average_Train_Loss: 0.0165\n",
      "   Batch  2040 of 293047. Elapsed: 0:35:57 Loss: 0.5472  Average_Train_Loss: 0.0164\n",
      "   Batch  2080 of 293047. Elapsed: 0:36:39 Loss: 0.2045  Average_Train_Loss: 0.0164\n",
      "   Batch  2120 of 293047. Elapsed: 0:37:22 Loss: 0.5268  Average_Train_Loss: 0.0163\n",
      "   Batch  2160 of 293047. Elapsed: 0:38:04 Loss: 0.6192  Average_Train_Loss: 0.0163\n",
      "   Batch  2200 of 293047. Elapsed: 0:38:46 Loss: 0.6381  Average_Train_Loss: 0.0162\n",
      "   Batch  2240 of 293047. Elapsed: 0:39:28 Loss: 0.4886  Average_Train_Loss: 0.0162\n",
      "   Batch  2280 of 293047. Elapsed: 0:40:11 Loss: 0.3143  Average_Train_Loss: 0.0161\n",
      "   Batch  2320 of 293047. Elapsed: 0:40:53 Loss: 0.3189  Average_Train_Loss: 0.0161\n",
      "   Batch  2360 of 293047. Elapsed: 0:41:35 Loss: 0.6991  Average_Train_Loss: 0.0160\n",
      "   Batch  2400 of 293047. Elapsed: 0:42:18 Loss: 0.8795  Average_Train_Loss: 0.0160\n",
      "   Batch  2440 of 293047. Elapsed: 0:43:00 Loss: 0.2932  Average_Train_Loss: 0.0159\n",
      "   Batch  2480 of 293047. Elapsed: 0:43:42 Loss: 0.9822  Average_Train_Loss: 0.0159\n",
      "   Batch  2520 of 293047. Elapsed: 0:44:25 Loss: 0.6037  Average_Train_Loss: 0.0159\n",
      "   Batch  2560 of 293047. Elapsed: 0:45:07 Loss: 0.1194  Average_Train_Loss: 0.0158\n",
      "   Batch  2600 of 293047. Elapsed: 0:45:49 Loss: 0.4467  Average_Train_Loss: 0.0158\n",
      "   Batch  2640 of 293047. Elapsed: 0:46:31 Loss: 0.3300  Average_Train_Loss: 0.0157\n",
      "   Batch  2680 of 293047. Elapsed: 0:47:14 Loss: 0.4806  Average_Train_Loss: 0.0157\n",
      "   Batch  2720 of 293047. Elapsed: 0:47:56 Loss: 0.6617  Average_Train_Loss: 0.0157\n",
      "   Batch  2760 of 293047. Elapsed: 0:48:38 Loss: 0.2180  Average_Train_Loss: 0.0157\n",
      "   Batch  2800 of 293047. Elapsed: 0:49:20 Loss: 0.7167  Average_Train_Loss: 0.0156\n",
      "   Batch  2840 of 293047. Elapsed: 0:50:03 Loss: 0.3078  Average_Train_Loss: 0.0156\n",
      "   Batch  2880 of 293047. Elapsed: 0:50:45 Loss: 0.8079  Average_Train_Loss: 0.0156\n",
      "   Batch  2920 of 293047. Elapsed: 0:51:27 Loss: 0.4137  Average_Train_Loss: 0.0155\n",
      "   Batch  2960 of 293047. Elapsed: 0:52:10 Loss: 0.5771  Average_Train_Loss: 0.0155\n",
      "   Batch  3000 of 293047. Elapsed: 0:52:52 Loss: 0.4697  Average_Train_Loss: 0.0155\n",
      "   Batch  3040 of 293047. Elapsed: 0:53:34 Loss: 0.5218  Average_Train_Loss: 0.0155\n",
      "   Batch  3080 of 293047. Elapsed: 0:54:16 Loss: 0.4954  Average_Train_Loss: 0.0154\n",
      "   Batch  3120 of 293047. Elapsed: 0:54:59 Loss: 0.4141  Average_Train_Loss: 0.0154\n",
      "   Batch  3160 of 293047. Elapsed: 0:55:41 Loss: 0.3680  Average_Train_Loss: 0.0154\n",
      "   Batch  3200 of 293047. Elapsed: 0:56:23 Loss: 0.2345  Average_Train_Loss: 0.0154\n",
      "   Batch  3240 of 293047. Elapsed: 0:57:06 Loss: 0.3456  Average_Train_Loss: 0.0153\n",
      "   Batch  3280 of 293047. Elapsed: 0:57:48 Loss: 0.3339  Average_Train_Loss: 0.0153\n",
      "   Batch  3320 of 293047. Elapsed: 0:58:30 Loss: 0.2471  Average_Train_Loss: 0.0153\n",
      "   Batch  3360 of 293047. Elapsed: 0:59:12 Loss: 0.9001  Average_Train_Loss: 0.0153\n",
      "   Batch  3400 of 293047. Elapsed: 0:59:55 Loss: 0.5490  Average_Train_Loss: 0.0153\n",
      "   Batch  3440 of 293047. Elapsed: 1:00:37 Loss: 0.3257  Average_Train_Loss: 0.0152\n",
      "   Batch  3480 of 293047. Elapsed: 1:01:19 Loss: 0.4172  Average_Train_Loss: 0.0152\n",
      "   Batch  3520 of 293047. Elapsed: 1:02:01 Loss: 0.4555  Average_Train_Loss: 0.0152\n",
      "   Batch  3560 of 293047. Elapsed: 1:02:44 Loss: 0.5181  Average_Train_Loss: 0.0152\n",
      "   Batch  3600 of 293047. Elapsed: 1:03:26 Loss: 0.5655  Average_Train_Loss: 0.0152\n",
      "   Batch  3640 of 293047. Elapsed: 1:04:08 Loss: 0.3661  Average_Train_Loss: 0.0151\n",
      "   Batch  3680 of 293047. Elapsed: 1:04:50 Loss: 0.4337  Average_Train_Loss: 0.0151\n",
      "   Batch  3720 of 293047. Elapsed: 1:05:33 Loss: 0.6700  Average_Train_Loss: 0.0151\n",
      "   Batch  3760 of 293047. Elapsed: 1:06:15 Loss: 0.4293  Average_Train_Loss: 0.0151\n",
      "   Batch  3800 of 293047. Elapsed: 1:06:57 Loss: 0.1931  Average_Train_Loss: 0.0150\n",
      "   Batch  3840 of 293047. Elapsed: 1:07:40 Loss: 0.3047  Average_Train_Loss: 0.0150\n",
      "   Batch  3880 of 293047. Elapsed: 1:08:22 Loss: 0.1766  Average_Train_Loss: 0.0150\n",
      "   Batch  3920 of 293047. Elapsed: 1:09:04 Loss: 0.4476  Average_Train_Loss: 0.0150\n",
      "   Batch  3960 of 293047. Elapsed: 1:09:46 Loss: 0.4358  Average_Train_Loss: 0.0149\n",
      "   Batch  4000 of 293047. Elapsed: 1:10:29 Loss: 0.6673  Average_Train_Loss: 0.0149\n",
      "   Batch  4040 of 293047. Elapsed: 1:11:11 Loss: 0.4936  Average_Train_Loss: 0.0149\n",
      "   Batch  4080 of 293047. Elapsed: 1:11:53 Loss: 0.4795  Average_Train_Loss: 0.0149\n",
      "   Batch  4120 of 293047. Elapsed: 1:12:36 Loss: 0.2891  Average_Train_Loss: 0.0149\n",
      "   Batch  4160 of 293047. Elapsed: 1:13:18 Loss: 0.2415  Average_Train_Loss: 0.0148\n",
      "   Batch  4200 of 293047. Elapsed: 1:14:00 Loss: 0.3558  Average_Train_Loss: 0.0148\n",
      "   Batch  4240 of 293047. Elapsed: 1:14:42 Loss: 0.6687  Average_Train_Loss: 0.0148\n",
      "   Batch  4280 of 293047. Elapsed: 1:15:25 Loss: 0.7238  Average_Train_Loss: 0.0148\n",
      "   Batch  4320 of 293047. Elapsed: 1:16:07 Loss: 0.8051  Average_Train_Loss: 0.0148\n",
      "   Batch  4360 of 293047. Elapsed: 1:16:49 Loss: 0.5069  Average_Train_Loss: 0.0147\n",
      "   Batch  4400 of 293047. Elapsed: 1:17:32 Loss: 0.1486  Average_Train_Loss: 0.0147\n",
      "   Batch  4440 of 293047. Elapsed: 1:18:14 Loss: 0.2470  Average_Train_Loss: 0.0147\n",
      "   Batch  4480 of 293047. Elapsed: 1:18:56 Loss: 0.3942  Average_Train_Loss: 0.0147\n",
      "   Batch  4520 of 293047. Elapsed: 1:19:38 Loss: 0.4584  Average_Train_Loss: 0.0147\n",
      "   Batch  4560 of 293047. Elapsed: 1:20:21 Loss: 0.3196  Average_Train_Loss: 0.0147\n",
      "   Batch  4600 of 293047. Elapsed: 1:21:03 Loss: 0.3850  Average_Train_Loss: 0.0146\n",
      "   Batch  4640 of 293047. Elapsed: 1:21:45 Loss: 0.0963  Average_Train_Loss: 0.0146\n",
      "   Batch  4680 of 293047. Elapsed: 1:22:28 Loss: 0.5783  Average_Train_Loss: 0.0146\n",
      "   Batch  4720 of 293047. Elapsed: 1:23:10 Loss: 0.6128  Average_Train_Loss: 0.0146\n",
      "   Batch  4760 of 293047. Elapsed: 1:23:52 Loss: 0.2173  Average_Train_Loss: 0.0146\n",
      "   Batch  4800 of 293047. Elapsed: 1:24:34 Loss: 0.9668  Average_Train_Loss: 0.0145\n",
      "   Batch  4840 of 293047. Elapsed: 1:25:17 Loss: 0.3622  Average_Train_Loss: 0.0145\n",
      "   Batch  4880 of 293047. Elapsed: 1:25:59 Loss: 0.6042  Average_Train_Loss: 0.0145\n",
      "   Batch  4920 of 293047. Elapsed: 1:26:41 Loss: 0.6668  Average_Train_Loss: 0.0145\n",
      "   Batch  4960 of 293047. Elapsed: 1:27:23 Loss: 0.3893  Average_Train_Loss: 0.0145\n",
      "   Batch  5000 of 293047. Elapsed: 1:28:06 Loss: 0.2194  Average_Train_Loss: 0.0145\n",
      "   Batch  5040 of 293047. Elapsed: 1:28:48 Loss: 0.4855  Average_Train_Loss: 0.0144\n",
      "   Batch  5080 of 293047. Elapsed: 1:29:30 Loss: 0.6727  Average_Train_Loss: 0.0144\n",
      "   Batch  5120 of 293047. Elapsed: 1:30:13 Loss: 0.1790  Average_Train_Loss: 0.0144\n",
      "   Batch  5160 of 293047. Elapsed: 1:30:55 Loss: 0.2442  Average_Train_Loss: 0.0144\n",
      "   Batch  5200 of 293047. Elapsed: 1:31:37 Loss: 0.1348  Average_Train_Loss: 0.0144\n",
      "   Batch  5240 of 293047. Elapsed: 1:32:20 Loss: 0.6476  Average_Train_Loss: 0.0144\n",
      "   Batch  5280 of 293047. Elapsed: 1:33:02 Loss: 0.6407  Average_Train_Loss: 0.0144\n",
      "   Batch  5320 of 293047. Elapsed: 1:33:44 Loss: 0.3510  Average_Train_Loss: 0.0143\n",
      "   Batch  5360 of 293047. Elapsed: 1:34:26 Loss: 0.3619  Average_Train_Loss: 0.0143\n",
      "   Batch  5400 of 293047. Elapsed: 1:35:09 Loss: 0.4382  Average_Train_Loss: 0.0143\n",
      "   Batch  5440 of 293047. Elapsed: 1:35:51 Loss: 0.3536  Average_Train_Loss: 0.0143\n",
      "   Batch  5480 of 293047. Elapsed: 1:36:33 Loss: 0.2983  Average_Train_Loss: 0.0143\n",
      "   Batch  5520 of 293047. Elapsed: 1:37:15 Loss: 0.4002  Average_Train_Loss: 0.0143\n",
      "   Batch  5560 of 293047. Elapsed: 1:37:58 Loss: 0.1234  Average_Train_Loss: 0.0143\n",
      "   Batch  5600 of 293047. Elapsed: 1:38:40 Loss: 0.5829  Average_Train_Loss: 0.0142\n",
      "   Batch  5640 of 293047. Elapsed: 1:39:22 Loss: 0.5122  Average_Train_Loss: 0.0142\n",
      "   Batch  5680 of 293047. Elapsed: 1:40:05 Loss: 0.2874  Average_Train_Loss: 0.0142\n",
      "   Batch  5720 of 293047. Elapsed: 1:40:47 Loss: 0.2090  Average_Train_Loss: 0.0142\n",
      "   Batch  5760 of 293047. Elapsed: 1:41:29 Loss: 0.5060  Average_Train_Loss: 0.0142\n",
      "   Batch  5800 of 293047. Elapsed: 1:42:12 Loss: 0.5094  Average_Train_Loss: 0.0142\n",
      "   Batch  5840 of 293047. Elapsed: 1:42:54 Loss: 0.5503  Average_Train_Loss: 0.0142\n",
      "   Batch  5880 of 293047. Elapsed: 1:43:36 Loss: 0.1615  Average_Train_Loss: 0.0142\n",
      "   Batch  5920 of 293047. Elapsed: 1:44:19 Loss: 0.2750  Average_Train_Loss: 0.0141\n",
      "   Batch  5960 of 293047. Elapsed: 1:45:01 Loss: 0.4765  Average_Train_Loss: 0.0141\n",
      "   Batch  6000 of 293047. Elapsed: 1:45:43 Loss: 0.4184  Average_Train_Loss: 0.0141\n",
      "   Batch  6040 of 293047. Elapsed: 1:46:26 Loss: 0.3733  Average_Train_Loss: 0.0141\n",
      "   Batch  6080 of 293047. Elapsed: 1:47:08 Loss: 0.9365  Average_Train_Loss: 0.0141\n",
      "   Batch  6120 of 293047. Elapsed: 1:47:50 Loss: 0.1597  Average_Train_Loss: 0.0141\n",
      "   Batch  6160 of 293047. Elapsed: 1:48:33 Loss: 0.3190  Average_Train_Loss: 0.0141\n",
      "   Batch  6200 of 293047. Elapsed: 1:49:15 Loss: 0.1102  Average_Train_Loss: 0.0141\n",
      "   Batch  6240 of 293047. Elapsed: 1:49:57 Loss: 0.2590  Average_Train_Loss: 0.0140\n",
      "   Batch  6280 of 293047. Elapsed: 1:50:40 Loss: 0.5793  Average_Train_Loss: 0.0140\n",
      "   Batch  6320 of 293047. Elapsed: 1:51:22 Loss: 0.2632  Average_Train_Loss: 0.0140\n",
      "   Batch  6360 of 293047. Elapsed: 1:52:04 Loss: 0.3376  Average_Train_Loss: 0.0140\n",
      "   Batch  6400 of 293047. Elapsed: 1:52:46 Loss: 0.4613  Average_Train_Loss: 0.0140\n",
      "   Batch  6440 of 293047. Elapsed: 1:53:29 Loss: 0.3299  Average_Train_Loss: 0.0140\n",
      "   Batch  6480 of 293047. Elapsed: 1:54:11 Loss: 0.1990  Average_Train_Loss: 0.0140\n",
      "   Batch  6520 of 293047. Elapsed: 1:54:53 Loss: 0.3343  Average_Train_Loss: 0.0139\n",
      "   Batch  6560 of 293047. Elapsed: 1:55:36 Loss: 0.4752  Average_Train_Loss: 0.0139\n",
      "   Batch  6600 of 293047. Elapsed: 1:56:18 Loss: 0.7030  Average_Train_Loss: 0.0139\n",
      "   Batch  6640 of 293047. Elapsed: 1:57:00 Loss: 0.2670  Average_Train_Loss: 0.0139\n",
      "   Batch  6680 of 293047. Elapsed: 1:57:43 Loss: 0.4620  Average_Train_Loss: 0.0139\n",
      "   Batch  6720 of 293047. Elapsed: 1:58:25 Loss: 0.5738  Average_Train_Loss: 0.0139\n",
      "   Batch  6760 of 293047. Elapsed: 1:59:07 Loss: 0.4440  Average_Train_Loss: 0.0139\n",
      "   Batch  6800 of 293047. Elapsed: 1:59:49 Loss: 0.4074  Average_Train_Loss: 0.0139\n",
      "   Batch  6840 of 293047. Elapsed: 2:00:32 Loss: 0.2564  Average_Train_Loss: 0.0139\n",
      "   Batch  6880 of 293047. Elapsed: 2:01:14 Loss: 0.2524  Average_Train_Loss: 0.0139\n",
      "   Batch  6920 of 293047. Elapsed: 2:01:56 Loss: 0.1416  Average_Train_Loss: 0.0138\n",
      "   Batch  6960 of 293047. Elapsed: 2:02:39 Loss: 0.3195  Average_Train_Loss: 0.0138\n",
      "   Batch  7000 of 293047. Elapsed: 2:03:21 Loss: 0.3082  Average_Train_Loss: 0.0138\n",
      "   Batch  7040 of 293047. Elapsed: 2:04:03 Loss: 0.2729  Average_Train_Loss: 0.0138\n",
      "   Batch  7080 of 293047. Elapsed: 2:04:46 Loss: 0.3709  Average_Train_Loss: 0.0138\n",
      "   Batch  7120 of 293047. Elapsed: 2:05:28 Loss: 0.2926  Average_Train_Loss: 0.0138\n",
      "   Batch  7160 of 293047. Elapsed: 2:06:10 Loss: 0.2508  Average_Train_Loss: 0.0138\n",
      "   Batch  7200 of 293047. Elapsed: 2:06:52 Loss: 0.3239  Average_Train_Loss: 0.0138\n",
      "   Batch  7240 of 293047. Elapsed: 2:07:35 Loss: 0.5335  Average_Train_Loss: 0.0138\n",
      "   Batch  7280 of 293047. Elapsed: 2:08:17 Loss: 0.2259  Average_Train_Loss: 0.0137\n",
      "   Batch  7320 of 293047. Elapsed: 2:08:59 Loss: 0.1545  Average_Train_Loss: 0.0137\n",
      "   Batch  7360 of 293047. Elapsed: 2:09:42 Loss: 0.3293  Average_Train_Loss: 0.0137\n",
      "   Batch  7400 of 293047. Elapsed: 2:10:24 Loss: 0.2002  Average_Train_Loss: 0.0137\n",
      "   Batch  7440 of 293047. Elapsed: 2:11:06 Loss: 0.2594  Average_Train_Loss: 0.0137\n",
      "   Batch  7480 of 293047. Elapsed: 2:11:48 Loss: 0.3987  Average_Train_Loss: 0.0137\n",
      "   Batch  7520 of 293047. Elapsed: 2:12:31 Loss: 0.2662  Average_Train_Loss: 0.0137\n",
      "   Batch  7560 of 293047. Elapsed: 2:13:13 Loss: 0.3612  Average_Train_Loss: 0.0137\n",
      "   Batch  7600 of 293047. Elapsed: 2:13:55 Loss: 0.5938  Average_Train_Loss: 0.0136\n",
      "   Batch  7640 of 293047. Elapsed: 2:14:38 Loss: 0.2544  Average_Train_Loss: 0.0136\n",
      "   Batch  7680 of 293047. Elapsed: 2:15:20 Loss: 0.2711  Average_Train_Loss: 0.0136\n",
      "   Batch  7720 of 293047. Elapsed: 2:16:02 Loss: 0.4487  Average_Train_Loss: 0.0136\n",
      "   Batch  7760 of 293047. Elapsed: 2:16:45 Loss: 0.3877  Average_Train_Loss: 0.0136\n",
      "   Batch  7800 of 293047. Elapsed: 2:17:27 Loss: 0.6214  Average_Train_Loss: 0.0136\n",
      "   Batch  7840 of 293047. Elapsed: 2:18:09 Loss: 0.3272  Average_Train_Loss: 0.0136\n",
      "   Batch  7880 of 293047. Elapsed: 2:18:51 Loss: 0.3973  Average_Train_Loss: 0.0136\n",
      "   Batch  7920 of 293047. Elapsed: 2:19:34 Loss: 0.0720  Average_Train_Loss: 0.0136\n",
      "   Batch  7960 of 293047. Elapsed: 2:20:16 Loss: 0.3740  Average_Train_Loss: 0.0136\n",
      "   Batch  8000 of 293047. Elapsed: 2:20:58 Loss: 0.4025  Average_Train_Loss: 0.0135\n",
      "   Batch  8040 of 293047. Elapsed: 2:21:41 Loss: 0.1506  Average_Train_Loss: 0.0135\n",
      "   Batch  8080 of 293047. Elapsed: 2:22:23 Loss: 0.8912  Average_Train_Loss: 0.0135\n",
      "   Batch  8120 of 293047. Elapsed: 2:23:05 Loss: 0.6004  Average_Train_Loss: 0.0135\n",
      "   Batch  8160 of 293047. Elapsed: 2:23:48 Loss: 0.5573  Average_Train_Loss: 0.0135\n",
      "   Batch  8200 of 293047. Elapsed: 2:24:30 Loss: 0.2027  Average_Train_Loss: 0.0135\n",
      "   Batch  8240 of 293047. Elapsed: 2:25:12 Loss: 0.4473  Average_Train_Loss: 0.0135\n",
      "   Batch  8280 of 293047. Elapsed: 2:25:54 Loss: 0.3375  Average_Train_Loss: 0.0135\n",
      "   Batch  8320 of 293047. Elapsed: 2:26:37 Loss: 0.1785  Average_Train_Loss: 0.0135\n",
      "   Batch  8360 of 293047. Elapsed: 2:27:19 Loss: 0.4279  Average_Train_Loss: 0.0134\n",
      "   Batch  8400 of 293047. Elapsed: 2:28:01 Loss: 0.2795  Average_Train_Loss: 0.0134\n",
      "   Batch  8440 of 293047. Elapsed: 2:28:44 Loss: 0.4111  Average_Train_Loss: 0.0134\n",
      "   Batch  8480 of 293047. Elapsed: 2:29:26 Loss: 0.2255  Average_Train_Loss: 0.0134\n",
      "   Batch  8520 of 293047. Elapsed: 2:30:08 Loss: 0.4061  Average_Train_Loss: 0.0134\n",
      "   Batch  8560 of 293047. Elapsed: 2:30:50 Loss: 0.1626  Average_Train_Loss: 0.0134\n",
      "   Batch  8600 of 293047. Elapsed: 2:31:33 Loss: 0.4111  Average_Train_Loss: 0.0134\n",
      "   Batch  8640 of 293047. Elapsed: 2:32:15 Loss: 0.6184  Average_Train_Loss: 0.0134\n",
      "   Batch  8680 of 293047. Elapsed: 2:32:57 Loss: 0.1210  Average_Train_Loss: 0.0134\n",
      "   Batch  8720 of 293047. Elapsed: 2:33:40 Loss: 0.4845  Average_Train_Loss: 0.0134\n",
      "   Batch  8760 of 293047. Elapsed: 2:34:22 Loss: 0.6127  Average_Train_Loss: 0.0134\n",
      "   Batch  8800 of 293047. Elapsed: 2:35:04 Loss: 0.3609  Average_Train_Loss: 0.0134\n",
      "   Batch  8840 of 293047. Elapsed: 2:35:47 Loss: 0.6015  Average_Train_Loss: 0.0134\n",
      "   Batch  8880 of 293047. Elapsed: 2:36:29 Loss: 0.4000  Average_Train_Loss: 0.0134\n",
      "   Batch  8920 of 293047. Elapsed: 2:37:11 Loss: 0.1504  Average_Train_Loss: 0.0134\n",
      "   Batch  8960 of 293047. Elapsed: 2:37:54 Loss: 0.6154  Average_Train_Loss: 0.0133\n",
      "   Batch  9000 of 293047. Elapsed: 2:38:36 Loss: 0.1923  Average_Train_Loss: 0.0133\n",
      "   Batch  9040 of 293047. Elapsed: 2:39:18 Loss: 0.4342  Average_Train_Loss: 0.0133\n",
      "   Batch  9080 of 293047. Elapsed: 2:40:00 Loss: 0.2127  Average_Train_Loss: 0.0133\n",
      "   Batch  9120 of 293047. Elapsed: 2:40:43 Loss: 0.2417  Average_Train_Loss: 0.0133\n",
      " Average training loss: 0.43\n",
      " Training epoch took: 2:41:23\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:02:49\n",
      "\n",
      "======= EPOCH 2 / 3 ======\n",
      "Training...\n",
      "   Batch    40 of 293047. Elapsed: 0:00:42 Loss: 0.4949  Average_Train_Loss: 0.0103\n",
      "   Batch    80 of 293047. Elapsed: 0:01:25 Loss: 0.4724  Average_Train_Loss: 0.0109\n",
      "   Batch   120 of 293047. Elapsed: 0:02:07 Loss: 0.3181  Average_Train_Loss: 0.0111\n",
      "   Batch   160 of 293047. Elapsed: 0:02:49 Loss: 0.3480  Average_Train_Loss: 0.0108\n",
      "   Batch   200 of 293047. Elapsed: 0:03:32 Loss: 0.3716  Average_Train_Loss: 0.0109\n",
      "   Batch   240 of 293047. Elapsed: 0:04:14 Loss: 0.4701  Average_Train_Loss: 0.0109\n",
      "   Batch   280 of 293047. Elapsed: 0:04:56 Loss: 0.7639  Average_Train_Loss: 0.0109\n",
      "   Batch   320 of 293047. Elapsed: 0:05:38 Loss: 0.1370  Average_Train_Loss: 0.0109\n",
      "   Batch   360 of 293047. Elapsed: 0:06:21 Loss: 0.1617  Average_Train_Loss: 0.0108\n",
      "   Batch   400 of 293047. Elapsed: 0:07:03 Loss: 0.4583  Average_Train_Loss: 0.0107\n",
      "   Batch   440 of 293047. Elapsed: 0:07:45 Loss: 0.3068  Average_Train_Loss: 0.0108\n",
      "   Batch   480 of 293047. Elapsed: 0:08:27 Loss: 0.4281  Average_Train_Loss: 0.0107\n",
      "   Batch   520 of 293047. Elapsed: 0:09:10 Loss: 0.2934  Average_Train_Loss: 0.0107\n",
      "   Batch   560 of 293047. Elapsed: 0:09:52 Loss: 0.4221  Average_Train_Loss: 0.0108\n",
      "   Batch   600 of 293047. Elapsed: 0:10:34 Loss: 0.4737  Average_Train_Loss: 0.0108\n",
      "   Batch   640 of 293047. Elapsed: 0:11:17 Loss: 0.3138  Average_Train_Loss: 0.0107\n",
      "   Batch   680 of 293047. Elapsed: 0:11:59 Loss: 0.4637  Average_Train_Loss: 0.0108\n",
      "   Batch   720 of 293047. Elapsed: 0:12:41 Loss: 0.5268  Average_Train_Loss: 0.0108\n",
      "   Batch   760 of 293047. Elapsed: 0:13:24 Loss: 0.4056  Average_Train_Loss: 0.0107\n",
      "   Batch   800 of 293047. Elapsed: 0:14:06 Loss: 0.2897  Average_Train_Loss: 0.0108\n",
      "   Batch   840 of 293047. Elapsed: 0:14:48 Loss: 0.1577  Average_Train_Loss: 0.0107\n",
      "   Batch   880 of 293047. Elapsed: 0:15:30 Loss: 0.2843  Average_Train_Loss: 0.0107\n",
      "   Batch   920 of 293047. Elapsed: 0:16:13 Loss: 0.2992  Average_Train_Loss: 0.0107\n",
      "   Batch   960 of 293047. Elapsed: 0:16:55 Loss: 0.3661  Average_Train_Loss: 0.0107\n",
      "   Batch  1000 of 293047. Elapsed: 0:17:37 Loss: 0.3229  Average_Train_Loss: 0.0106\n",
      "   Batch  1040 of 293047. Elapsed: 0:18:20 Loss: 0.3388  Average_Train_Loss: 0.0106\n",
      "   Batch  1080 of 293047. Elapsed: 0:19:02 Loss: 0.1809  Average_Train_Loss: 0.0107\n",
      "   Batch  1120 of 293047. Elapsed: 0:19:44 Loss: 0.2599  Average_Train_Loss: 0.0106\n",
      "   Batch  1160 of 293047. Elapsed: 0:20:27 Loss: 0.4952  Average_Train_Loss: 0.0106\n",
      "   Batch  1200 of 293047. Elapsed: 0:21:09 Loss: 0.2235  Average_Train_Loss: 0.0106\n",
      "   Batch  1240 of 293047. Elapsed: 0:21:51 Loss: 0.2783  Average_Train_Loss: 0.0107\n",
      "   Batch  1280 of 293047. Elapsed: 0:22:33 Loss: 0.3183  Average_Train_Loss: 0.0107\n",
      "   Batch  1320 of 293047. Elapsed: 0:23:16 Loss: 0.1855  Average_Train_Loss: 0.0107\n",
      "   Batch  1360 of 293047. Elapsed: 0:23:58 Loss: 0.2657  Average_Train_Loss: 0.0106\n",
      "   Batch  1400 of 293047. Elapsed: 0:24:40 Loss: 0.4337  Average_Train_Loss: 0.0107\n",
      "   Batch  1440 of 293047. Elapsed: 0:25:23 Loss: 0.2906  Average_Train_Loss: 0.0107\n",
      "   Batch  1480 of 293047. Elapsed: 0:26:05 Loss: 0.4084  Average_Train_Loss: 0.0107\n",
      "   Batch  1520 of 293047. Elapsed: 0:26:47 Loss: 0.4013  Average_Train_Loss: 0.0107\n",
      "   Batch  1560 of 293047. Elapsed: 0:27:30 Loss: 0.3559  Average_Train_Loss: 0.0107\n",
      "   Batch  1600 of 293047. Elapsed: 0:28:12 Loss: 0.3465  Average_Train_Loss: 0.0107\n",
      "   Batch  1640 of 293047. Elapsed: 0:28:54 Loss: 0.3264  Average_Train_Loss: 0.0106\n",
      "   Batch  1680 of 293047. Elapsed: 0:29:36 Loss: 0.3700  Average_Train_Loss: 0.0106\n",
      "   Batch  1720 of 293047. Elapsed: 0:30:19 Loss: 0.5128  Average_Train_Loss: 0.0106\n",
      "   Batch  1760 of 293047. Elapsed: 0:31:01 Loss: 0.2832  Average_Train_Loss: 0.0106\n",
      "   Batch  1800 of 293047. Elapsed: 0:31:43 Loss: 0.5401  Average_Train_Loss: 0.0106\n",
      "   Batch  1840 of 293047. Elapsed: 0:32:26 Loss: 0.4855  Average_Train_Loss: 0.0106\n",
      "   Batch  1880 of 293047. Elapsed: 0:33:08 Loss: 0.3138  Average_Train_Loss: 0.0106\n",
      "   Batch  1920 of 293047. Elapsed: 0:33:50 Loss: 0.1252  Average_Train_Loss: 0.0106\n",
      "   Batch  1960 of 293047. Elapsed: 0:34:32 Loss: 0.3072  Average_Train_Loss: 0.0106\n",
      "   Batch  2000 of 293047. Elapsed: 0:35:15 Loss: 0.4688  Average_Train_Loss: 0.0106\n",
      "   Batch  2040 of 293047. Elapsed: 0:35:57 Loss: 0.3399  Average_Train_Loss: 0.0106\n",
      "   Batch  2080 of 293047. Elapsed: 0:36:39 Loss: 0.4324  Average_Train_Loss: 0.0106\n",
      "   Batch  2120 of 293047. Elapsed: 0:37:22 Loss: 0.1807  Average_Train_Loss: 0.0106\n",
      "   Batch  2160 of 293047. Elapsed: 0:38:04 Loss: 0.5584  Average_Train_Loss: 0.0106\n",
      "   Batch  2200 of 293047. Elapsed: 0:38:46 Loss: 0.3553  Average_Train_Loss: 0.0106\n",
      "   Batch  2240 of 293047. Elapsed: 0:39:29 Loss: 0.2464  Average_Train_Loss: 0.0106\n",
      "   Batch  2280 of 293047. Elapsed: 0:40:11 Loss: 0.7582  Average_Train_Loss: 0.0106\n",
      "   Batch  2320 of 293047. Elapsed: 0:40:53 Loss: 0.1846  Average_Train_Loss: 0.0106\n",
      "   Batch  2360 of 293047. Elapsed: 0:41:36 Loss: 0.1325  Average_Train_Loss: 0.0106\n",
      "   Batch  2400 of 293047. Elapsed: 0:42:18 Loss: 0.1426  Average_Train_Loss: 0.0105\n",
      "   Batch  2440 of 293047. Elapsed: 0:43:00 Loss: 0.2794  Average_Train_Loss: 0.0105\n",
      "   Batch  2480 of 293047. Elapsed: 0:43:43 Loss: 0.5547  Average_Train_Loss: 0.0105\n",
      "   Batch  2520 of 293047. Elapsed: 0:44:25 Loss: 0.3388  Average_Train_Loss: 0.0105\n",
      "   Batch  2560 of 293047. Elapsed: 0:45:07 Loss: 0.3287  Average_Train_Loss: 0.0105\n",
      "   Batch  2600 of 293047. Elapsed: 0:45:50 Loss: 0.4445  Average_Train_Loss: 0.0105\n",
      "   Batch  2640 of 293047. Elapsed: 0:46:32 Loss: 0.2991  Average_Train_Loss: 0.0105\n",
      "   Batch  2680 of 293047. Elapsed: 0:47:14 Loss: 0.2238  Average_Train_Loss: 0.0105\n",
      "   Batch  2720 of 293047. Elapsed: 0:47:56 Loss: 0.2808  Average_Train_Loss: 0.0105\n",
      "   Batch  2760 of 293047. Elapsed: 0:48:39 Loss: 0.3585  Average_Train_Loss: 0.0105\n",
      "   Batch  2800 of 293047. Elapsed: 0:49:21 Loss: 0.3564  Average_Train_Loss: 0.0105\n",
      "   Batch  2840 of 293047. Elapsed: 0:50:03 Loss: 0.3448  Average_Train_Loss: 0.0105\n",
      "   Batch  2880 of 293047. Elapsed: 0:50:45 Loss: 0.4118  Average_Train_Loss: 0.0105\n",
      "   Batch  2920 of 293047. Elapsed: 0:51:28 Loss: 0.4270  Average_Train_Loss: 0.0105\n",
      "   Batch  2960 of 293047. Elapsed: 0:52:10 Loss: 0.5711  Average_Train_Loss: 0.0105\n",
      "   Batch  3000 of 293047. Elapsed: 0:52:52 Loss: 0.1588  Average_Train_Loss: 0.0105\n",
      "   Batch  3040 of 293047. Elapsed: 0:53:35 Loss: 0.1432  Average_Train_Loss: 0.0105\n",
      "   Batch  3080 of 293047. Elapsed: 0:54:17 Loss: 0.2024  Average_Train_Loss: 0.0105\n",
      "   Batch  3120 of 293047. Elapsed: 0:54:59 Loss: 0.7320  Average_Train_Loss: 0.0105\n",
      "   Batch  3160 of 293047. Elapsed: 0:55:42 Loss: 0.5134  Average_Train_Loss: 0.0105\n",
      "   Batch  3200 of 293047. Elapsed: 0:56:24 Loss: 0.2045  Average_Train_Loss: 0.0105\n",
      "   Batch  3240 of 293047. Elapsed: 0:57:06 Loss: 0.0980  Average_Train_Loss: 0.0105\n",
      "   Batch  3280 of 293047. Elapsed: 0:57:48 Loss: 0.4700  Average_Train_Loss: 0.0105\n",
      "   Batch  3320 of 293047. Elapsed: 0:58:31 Loss: 0.1522  Average_Train_Loss: 0.0105\n",
      "   Batch  3360 of 293047. Elapsed: 0:59:13 Loss: 0.3916  Average_Train_Loss: 0.0105\n",
      "   Batch  3400 of 293047. Elapsed: 0:59:55 Loss: 0.2891  Average_Train_Loss: 0.0105\n",
      "   Batch  3440 of 293047. Elapsed: 1:00:38 Loss: 0.5899  Average_Train_Loss: 0.0105\n",
      "   Batch  3480 of 293047. Elapsed: 1:01:20 Loss: 0.2686  Average_Train_Loss: 0.0105\n",
      "   Batch  3520 of 293047. Elapsed: 1:02:02 Loss: 0.0734  Average_Train_Loss: 0.0105\n",
      "   Batch  3560 of 293047. Elapsed: 1:02:45 Loss: 0.3072  Average_Train_Loss: 0.0105\n",
      "   Batch  3600 of 293047. Elapsed: 1:03:27 Loss: 0.3226  Average_Train_Loss: 0.0105\n",
      "   Batch  3640 of 293047. Elapsed: 1:04:09 Loss: 0.2630  Average_Train_Loss: 0.0105\n",
      "   Batch  3680 of 293047. Elapsed: 1:04:51 Loss: 0.2389  Average_Train_Loss: 0.0105\n",
      "   Batch  3720 of 293047. Elapsed: 1:05:34 Loss: 0.1693  Average_Train_Loss: 0.0105\n",
      "   Batch  3760 of 293047. Elapsed: 1:06:16 Loss: 0.1566  Average_Train_Loss: 0.0105\n",
      "   Batch  3800 of 293047. Elapsed: 1:06:58 Loss: 0.3016  Average_Train_Loss: 0.0105\n",
      "   Batch  3840 of 293047. Elapsed: 1:07:41 Loss: 0.1931  Average_Train_Loss: 0.0105\n",
      "   Batch  3880 of 293047. Elapsed: 1:08:23 Loss: 0.0555  Average_Train_Loss: 0.0105\n",
      "   Batch  3920 of 293047. Elapsed: 1:09:05 Loss: 0.4623  Average_Train_Loss: 0.0105\n",
      "   Batch  3960 of 293047. Elapsed: 1:09:48 Loss: 0.5678  Average_Train_Loss: 0.0105\n",
      "   Batch  4000 of 293047. Elapsed: 1:10:30 Loss: 0.3757  Average_Train_Loss: 0.0105\n",
      "   Batch  4040 of 293047. Elapsed: 1:11:12 Loss: 0.4726  Average_Train_Loss: 0.0105\n",
      "   Batch  4080 of 293047. Elapsed: 1:11:54 Loss: 0.3698  Average_Train_Loss: 0.0105\n",
      "   Batch  4120 of 293047. Elapsed: 1:12:37 Loss: 0.4896  Average_Train_Loss: 0.0105\n",
      "   Batch  4160 of 293047. Elapsed: 1:13:19 Loss: 0.3032  Average_Train_Loss: 0.0105\n",
      "   Batch  4200 of 293047. Elapsed: 1:14:01 Loss: 0.3968  Average_Train_Loss: 0.0105\n",
      "   Batch  4240 of 293047. Elapsed: 1:14:44 Loss: 0.2921  Average_Train_Loss: 0.0105\n",
      "   Batch  4280 of 293047. Elapsed: 1:15:26 Loss: 0.3281  Average_Train_Loss: 0.0105\n",
      "   Batch  4320 of 293047. Elapsed: 1:16:08 Loss: 0.3348  Average_Train_Loss: 0.0105\n",
      "   Batch  4360 of 293047. Elapsed: 1:16:50 Loss: 0.4544  Average_Train_Loss: 0.0105\n",
      "   Batch  4400 of 293047. Elapsed: 1:17:33 Loss: 0.6161  Average_Train_Loss: 0.0105\n",
      "   Batch  4440 of 293047. Elapsed: 1:18:15 Loss: 0.5833  Average_Train_Loss: 0.0105\n",
      "   Batch  4480 of 293047. Elapsed: 1:18:57 Loss: 0.3405  Average_Train_Loss: 0.0105\n",
      "   Batch  4520 of 293047. Elapsed: 1:19:40 Loss: 0.2837  Average_Train_Loss: 0.0105\n",
      "   Batch  4560 of 293047. Elapsed: 1:20:22 Loss: 0.3817  Average_Train_Loss: 0.0105\n",
      "   Batch  4600 of 293047. Elapsed: 1:21:04 Loss: 0.2883  Average_Train_Loss: 0.0105\n",
      "   Batch  4640 of 293047. Elapsed: 1:21:47 Loss: 0.3682  Average_Train_Loss: 0.0105\n",
      "   Batch  4680 of 293047. Elapsed: 1:22:29 Loss: 0.5286  Average_Train_Loss: 0.0105\n",
      "   Batch  4720 of 293047. Elapsed: 1:23:11 Loss: 0.6135  Average_Train_Loss: 0.0105\n",
      "   Batch  4760 of 293047. Elapsed: 1:23:53 Loss: 0.0789  Average_Train_Loss: 0.0105\n",
      "   Batch  4800 of 293047. Elapsed: 1:24:36 Loss: 0.4910  Average_Train_Loss: 0.0105\n",
      "   Batch  4840 of 293047. Elapsed: 1:25:18 Loss: 0.3763  Average_Train_Loss: 0.0105\n",
      "   Batch  4880 of 293047. Elapsed: 1:26:00 Loss: 0.0985  Average_Train_Loss: 0.0105\n",
      "   Batch  4920 of 293047. Elapsed: 1:26:43 Loss: 0.1652  Average_Train_Loss: 0.0105\n",
      "   Batch  4960 of 293047. Elapsed: 1:27:25 Loss: 0.4316  Average_Train_Loss: 0.0105\n",
      "   Batch  5000 of 293047. Elapsed: 1:28:07 Loss: 0.2559  Average_Train_Loss: 0.0105\n",
      "   Batch  5040 of 293047. Elapsed: 1:28:49 Loss: 0.2319  Average_Train_Loss: 0.0105\n",
      "   Batch  5080 of 293047. Elapsed: 1:29:32 Loss: 0.5376  Average_Train_Loss: 0.0105\n",
      "   Batch  5120 of 293047. Elapsed: 1:30:14 Loss: 0.2741  Average_Train_Loss: 0.0105\n",
      "   Batch  5160 of 293047. Elapsed: 1:30:56 Loss: 0.4727  Average_Train_Loss: 0.0105\n",
      "   Batch  5200 of 293047. Elapsed: 1:31:39 Loss: 0.1398  Average_Train_Loss: 0.0105\n",
      "   Batch  5240 of 293047. Elapsed: 1:32:21 Loss: 0.1812  Average_Train_Loss: 0.0105\n",
      "   Batch  5280 of 293047. Elapsed: 1:33:03 Loss: 0.3601  Average_Train_Loss: 0.0105\n",
      "   Batch  5320 of 293047. Elapsed: 1:33:46 Loss: 0.2637  Average_Train_Loss: 0.0105\n",
      "   Batch  5360 of 293047. Elapsed: 1:34:28 Loss: 0.3707  Average_Train_Loss: 0.0105\n",
      "   Batch  5400 of 293047. Elapsed: 1:35:10 Loss: 0.2594  Average_Train_Loss: 0.0105\n",
      "   Batch  5440 of 293047. Elapsed: 1:35:52 Loss: 0.5901  Average_Train_Loss: 0.0105\n",
      "   Batch  5480 of 293047. Elapsed: 1:36:35 Loss: 0.5303  Average_Train_Loss: 0.0105\n",
      "   Batch  5520 of 293047. Elapsed: 1:37:17 Loss: 0.3297  Average_Train_Loss: 0.0105\n",
      "   Batch  5560 of 293047. Elapsed: 1:38:00 Loss: 0.2527  Average_Train_Loss: 0.0104\n",
      "   Batch  5600 of 293047. Elapsed: 1:38:42 Loss: 0.0851  Average_Train_Loss: 0.0104\n",
      "   Batch  5640 of 293047. Elapsed: 1:39:24 Loss: 0.3592  Average_Train_Loss: 0.0105\n",
      "   Batch  5680 of 293047. Elapsed: 1:40:06 Loss: 0.1111  Average_Train_Loss: 0.0105\n",
      "   Batch  5720 of 293047. Elapsed: 1:40:49 Loss: 0.5326  Average_Train_Loss: 0.0105\n",
      "   Batch  5760 of 293047. Elapsed: 1:41:31 Loss: 0.2538  Average_Train_Loss: 0.0105\n",
      "   Batch  5800 of 293047. Elapsed: 1:42:13 Loss: 0.4247  Average_Train_Loss: 0.0105\n",
      "   Batch  5840 of 293047. Elapsed: 1:42:55 Loss: 0.3696  Average_Train_Loss: 0.0105\n",
      "   Batch  5880 of 293047. Elapsed: 1:43:38 Loss: 0.4147  Average_Train_Loss: 0.0105\n",
      "   Batch  5920 of 293047. Elapsed: 1:44:20 Loss: 0.3311  Average_Train_Loss: 0.0105\n",
      "   Batch  5960 of 293047. Elapsed: 1:45:02 Loss: 0.2624  Average_Train_Loss: 0.0105\n",
      "   Batch  6000 of 293047. Elapsed: 1:45:45 Loss: 0.4689  Average_Train_Loss: 0.0105\n",
      "   Batch  6040 of 293047. Elapsed: 1:46:27 Loss: 0.3223  Average_Train_Loss: 0.0105\n",
      "   Batch  6080 of 293047. Elapsed: 1:47:09 Loss: 0.0987  Average_Train_Loss: 0.0105\n",
      "   Batch  6120 of 293047. Elapsed: 1:47:52 Loss: 0.0528  Average_Train_Loss: 0.0105\n",
      "   Batch  6160 of 293047. Elapsed: 1:48:34 Loss: 0.3319  Average_Train_Loss: 0.0104\n",
      "   Batch  6200 of 293047. Elapsed: 1:49:16 Loss: 0.1949  Average_Train_Loss: 0.0104\n",
      "   Batch  6240 of 293047. Elapsed: 1:49:59 Loss: 0.3814  Average_Train_Loss: 0.0104\n",
      "   Batch  6280 of 293047. Elapsed: 1:50:41 Loss: 0.0674  Average_Train_Loss: 0.0104\n",
      "   Batch  6320 of 293047. Elapsed: 1:51:23 Loss: 0.4180  Average_Train_Loss: 0.0104\n",
      "   Batch  6360 of 293047. Elapsed: 1:52:05 Loss: 0.3995  Average_Train_Loss: 0.0104\n",
      "   Batch  6400 of 293047. Elapsed: 1:52:48 Loss: 0.3725  Average_Train_Loss: 0.0104\n",
      "   Batch  6440 of 293047. Elapsed: 1:53:30 Loss: 0.3628  Average_Train_Loss: 0.0104\n",
      "   Batch  6480 of 293047. Elapsed: 1:54:12 Loss: 0.2548  Average_Train_Loss: 0.0104\n",
      "   Batch  6520 of 293047. Elapsed: 1:54:55 Loss: 0.3416  Average_Train_Loss: 0.0104\n",
      "   Batch  6560 of 293047. Elapsed: 1:55:37 Loss: 0.3567  Average_Train_Loss: 0.0104\n",
      "   Batch  6600 of 293047. Elapsed: 1:56:19 Loss: 0.4816  Average_Train_Loss: 0.0104\n",
      "   Batch  6640 of 293047. Elapsed: 1:57:01 Loss: 0.1951  Average_Train_Loss: 0.0104\n",
      "   Batch  6680 of 293047. Elapsed: 1:57:44 Loss: 0.2139  Average_Train_Loss: 0.0104\n",
      "   Batch  6720 of 293047. Elapsed: 1:58:26 Loss: 0.2025  Average_Train_Loss: 0.0104\n",
      "   Batch  6760 of 293047. Elapsed: 1:59:08 Loss: 0.4387  Average_Train_Loss: 0.0104\n",
      "   Batch  6800 of 293047. Elapsed: 1:59:51 Loss: 0.2630  Average_Train_Loss: 0.0104\n",
      "   Batch  6840 of 293047. Elapsed: 2:00:33 Loss: 0.3594  Average_Train_Loss: 0.0104\n",
      "   Batch  6880 of 293047. Elapsed: 2:01:15 Loss: 0.5102  Average_Train_Loss: 0.0104\n",
      "   Batch  6920 of 293047. Elapsed: 2:01:57 Loss: 0.1905  Average_Train_Loss: 0.0104\n",
      "   Batch  6960 of 293047. Elapsed: 2:02:40 Loss: 0.2673  Average_Train_Loss: 0.0104\n",
      "   Batch  7000 of 293047. Elapsed: 2:03:22 Loss: 0.3436  Average_Train_Loss: 0.0104\n",
      "   Batch  7040 of 293047. Elapsed: 2:04:04 Loss: 0.3271  Average_Train_Loss: 0.0104\n",
      "   Batch  7080 of 293047. Elapsed: 2:04:47 Loss: 0.0169  Average_Train_Loss: 0.0104\n",
      "   Batch  7120 of 293047. Elapsed: 2:05:29 Loss: 0.2737  Average_Train_Loss: 0.0104\n",
      "   Batch  7160 of 293047. Elapsed: 2:06:11 Loss: 0.1958  Average_Train_Loss: 0.0104\n",
      "   Batch  7200 of 293047. Elapsed: 2:06:54 Loss: 0.1488  Average_Train_Loss: 0.0104\n",
      "   Batch  7240 of 293047. Elapsed: 2:07:36 Loss: 0.5301  Average_Train_Loss: 0.0104\n",
      "   Batch  7280 of 293047. Elapsed: 2:08:18 Loss: 0.1817  Average_Train_Loss: 0.0104\n",
      "   Batch  7320 of 293047. Elapsed: 2:09:01 Loss: 0.2224  Average_Train_Loss: 0.0104\n",
      "   Batch  7360 of 293047. Elapsed: 2:09:43 Loss: 0.4655  Average_Train_Loss: 0.0104\n",
      "   Batch  7400 of 293047. Elapsed: 2:10:25 Loss: 0.4172  Average_Train_Loss: 0.0104\n",
      "   Batch  7440 of 293047. Elapsed: 2:11:07 Loss: 0.2266  Average_Train_Loss: 0.0104\n",
      "   Batch  7480 of 293047. Elapsed: 2:11:50 Loss: 0.3129  Average_Train_Loss: 0.0104\n",
      "   Batch  7520 of 293047. Elapsed: 2:12:32 Loss: 0.3900  Average_Train_Loss: 0.0104\n",
      "   Batch  7560 of 293047. Elapsed: 2:13:14 Loss: 0.4063  Average_Train_Loss: 0.0104\n",
      "   Batch  7600 of 293047. Elapsed: 2:13:57 Loss: 0.1776  Average_Train_Loss: 0.0104\n",
      "   Batch  7640 of 293047. Elapsed: 2:14:39 Loss: 0.2602  Average_Train_Loss: 0.0104\n",
      "   Batch  7680 of 293047. Elapsed: 2:15:21 Loss: 0.2283  Average_Train_Loss: 0.0104\n",
      "   Batch  7720 of 293047. Elapsed: 2:16:04 Loss: 0.3117  Average_Train_Loss: 0.0104\n",
      "   Batch  7760 of 293047. Elapsed: 2:16:46 Loss: 0.4383  Average_Train_Loss: 0.0104\n",
      "   Batch  7800 of 293047. Elapsed: 2:17:28 Loss: 0.0868  Average_Train_Loss: 0.0104\n",
      "   Batch  7840 of 293047. Elapsed: 2:18:10 Loss: 0.3973  Average_Train_Loss: 0.0104\n",
      "   Batch  7880 of 293047. Elapsed: 2:18:53 Loss: 0.4154  Average_Train_Loss: 0.0104\n",
      "   Batch  7920 of 293047. Elapsed: 2:19:35 Loss: 0.0795  Average_Train_Loss: 0.0104\n",
      "   Batch  7960 of 293047. Elapsed: 2:20:17 Loss: 0.5252  Average_Train_Loss: 0.0104\n",
      "   Batch  8000 of 293047. Elapsed: 2:21:00 Loss: 0.1879  Average_Train_Loss: 0.0104\n",
      "   Batch  8040 of 293047. Elapsed: 2:21:42 Loss: 0.3448  Average_Train_Loss: 0.0104\n",
      "   Batch  8080 of 293047. Elapsed: 2:22:24 Loss: 0.4414  Average_Train_Loss: 0.0104\n",
      "   Batch  8120 of 293047. Elapsed: 2:23:07 Loss: 0.2951  Average_Train_Loss: 0.0104\n",
      "   Batch  8160 of 293047. Elapsed: 2:23:49 Loss: 0.4606  Average_Train_Loss: 0.0104\n",
      "   Batch  8200 of 293047. Elapsed: 2:24:32 Loss: 0.1623  Average_Train_Loss: 0.0104\n",
      "   Batch  8240 of 293047. Elapsed: 2:25:14 Loss: 0.3270  Average_Train_Loss: 0.0104\n",
      "   Batch  8280 of 293047. Elapsed: 2:25:57 Loss: 0.3487  Average_Train_Loss: 0.0104\n",
      "   Batch  8320 of 293047. Elapsed: 2:26:39 Loss: 0.3990  Average_Train_Loss: 0.0104\n",
      "   Batch  8360 of 293047. Elapsed: 2:27:21 Loss: 0.2677  Average_Train_Loss: 0.0104\n",
      "   Batch  8400 of 293047. Elapsed: 2:28:04 Loss: 0.2290  Average_Train_Loss: 0.0104\n",
      "   Batch  8440 of 293047. Elapsed: 2:28:46 Loss: 0.2052  Average_Train_Loss: 0.0104\n",
      "   Batch  8480 of 293047. Elapsed: 2:29:29 Loss: 0.3295  Average_Train_Loss: 0.0104\n",
      "   Batch  8520 of 293047. Elapsed: 2:30:11 Loss: 0.3759  Average_Train_Loss: 0.0104\n",
      "   Batch  8560 of 293047. Elapsed: 2:30:53 Loss: 0.3090  Average_Train_Loss: 0.0104\n",
      "   Batch  8600 of 293047. Elapsed: 2:31:36 Loss: 0.3598  Average_Train_Loss: 0.0104\n",
      "   Batch  8640 of 293047. Elapsed: 2:32:18 Loss: 0.7340  Average_Train_Loss: 0.0104\n",
      "   Batch  8680 of 293047. Elapsed: 2:33:01 Loss: 0.1589  Average_Train_Loss: 0.0104\n",
      "   Batch  8720 of 293047. Elapsed: 2:33:43 Loss: 0.3026  Average_Train_Loss: 0.0104\n",
      "   Batch  8760 of 293047. Elapsed: 2:34:25 Loss: 1.1201  Average_Train_Loss: 0.0104\n",
      "   Batch  8800 of 293047. Elapsed: 2:35:08 Loss: 0.2037  Average_Train_Loss: 0.0104\n",
      "   Batch  8840 of 293047. Elapsed: 2:35:50 Loss: 0.2040  Average_Train_Loss: 0.0104\n",
      "   Batch  8880 of 293047. Elapsed: 2:36:32 Loss: 0.3741  Average_Train_Loss: 0.0104\n",
      "   Batch  8920 of 293047. Elapsed: 2:37:15 Loss: 0.0631  Average_Train_Loss: 0.0103\n",
      "   Batch  8960 of 293047. Elapsed: 2:37:57 Loss: 0.3945  Average_Train_Loss: 0.0103\n",
      "   Batch  9000 of 293047. Elapsed: 2:38:39 Loss: 0.5624  Average_Train_Loss: 0.0103\n",
      "   Batch  9040 of 293047. Elapsed: 2:39:22 Loss: 0.6001  Average_Train_Loss: 0.0103\n",
      "   Batch  9080 of 293047. Elapsed: 2:40:04 Loss: 0.3569  Average_Train_Loss: 0.0103\n",
      "   Batch  9120 of 293047. Elapsed: 2:40:46 Loss: 0.5741  Average_Train_Loss: 0.0103\n",
      " Average training loss: 0.33\n",
      " Training epoch took: 2:41:26\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.33\n",
      "  Validation took: 0:02:49\n",
      "\n",
      "======= EPOCH 3 / 3 ======\n",
      "Training...\n",
      "   Batch    40 of 293047. Elapsed: 0:00:42 Loss: 0.2434  Average_Train_Loss: 0.0092\n",
      "   Batch    80 of 293047. Elapsed: 0:01:24 Loss: 0.1994  Average_Train_Loss: 0.0085\n",
      "   Batch   120 of 293047. Elapsed: 0:02:07 Loss: 0.2132  Average_Train_Loss: 0.0085\n",
      "   Batch   160 of 293047. Elapsed: 0:02:49 Loss: 0.3917  Average_Train_Loss: 0.0087\n",
      "   Batch   200 of 293047. Elapsed: 0:03:32 Loss: 0.1658  Average_Train_Loss: 0.0087\n",
      "   Batch   240 of 293047. Elapsed: 0:04:14 Loss: 0.2251  Average_Train_Loss: 0.0087\n",
      "   Batch   280 of 293047. Elapsed: 0:04:56 Loss: 0.2318  Average_Train_Loss: 0.0086\n",
      "   Batch   320 of 293047. Elapsed: 0:05:38 Loss: 0.2663  Average_Train_Loss: 0.0085\n",
      "   Batch   360 of 293047. Elapsed: 0:06:21 Loss: 0.4042  Average_Train_Loss: 0.0086\n",
      "   Batch   400 of 293047. Elapsed: 0:07:03 Loss: 0.2630  Average_Train_Loss: 0.0087\n",
      "   Batch   440 of 293047. Elapsed: 0:07:45 Loss: 0.0867  Average_Train_Loss: 0.0088\n",
      "   Batch   480 of 293047. Elapsed: 0:08:28 Loss: 0.3337  Average_Train_Loss: 0.0087\n",
      "   Batch   520 of 293047. Elapsed: 0:09:10 Loss: 0.1412  Average_Train_Loss: 0.0086\n",
      "   Batch   560 of 293047. Elapsed: 0:09:52 Loss: 0.2568  Average_Train_Loss: 0.0085\n",
      "   Batch   600 of 293047. Elapsed: 0:10:35 Loss: 0.4530  Average_Train_Loss: 0.0085\n",
      "   Batch   640 of 293047. Elapsed: 0:11:17 Loss: 0.2929  Average_Train_Loss: 0.0084\n",
      "   Batch   680 of 293047. Elapsed: 0:11:59 Loss: 0.3017  Average_Train_Loss: 0.0084\n",
      "   Batch   720 of 293047. Elapsed: 0:12:41 Loss: 0.3014  Average_Train_Loss: 0.0085\n",
      "   Batch   760 of 293047. Elapsed: 0:13:24 Loss: 0.2745  Average_Train_Loss: 0.0085\n",
      "   Batch   800 of 293047. Elapsed: 0:14:06 Loss: 0.4506  Average_Train_Loss: 0.0085\n",
      "   Batch   840 of 293047. Elapsed: 0:14:48 Loss: 0.1738  Average_Train_Loss: 0.0085\n",
      "   Batch   880 of 293047. Elapsed: 0:15:31 Loss: 0.3495  Average_Train_Loss: 0.0085\n",
      "   Batch   920 of 293047. Elapsed: 0:16:13 Loss: 0.2918  Average_Train_Loss: 0.0085\n",
      "   Batch   960 of 293047. Elapsed: 0:16:55 Loss: 0.2708  Average_Train_Loss: 0.0085\n",
      "   Batch  1000 of 293047. Elapsed: 0:17:38 Loss: 0.3293  Average_Train_Loss: 0.0085\n",
      "   Batch  1040 of 293047. Elapsed: 0:18:20 Loss: 0.0447  Average_Train_Loss: 0.0085\n",
      "   Batch  1080 of 293047. Elapsed: 0:19:02 Loss: 0.1116  Average_Train_Loss: 0.0085\n",
      "   Batch  1120 of 293047. Elapsed: 0:19:45 Loss: 0.1761  Average_Train_Loss: 0.0085\n",
      "   Batch  1160 of 293047. Elapsed: 0:20:27 Loss: 0.0656  Average_Train_Loss: 0.0085\n",
      "   Batch  1200 of 293047. Elapsed: 0:21:09 Loss: 0.0628  Average_Train_Loss: 0.0085\n",
      "   Batch  1240 of 293047. Elapsed: 0:21:52 Loss: 0.3452  Average_Train_Loss: 0.0085\n",
      "   Batch  1280 of 293047. Elapsed: 0:22:34 Loss: 0.1760  Average_Train_Loss: 0.0085\n",
      "   Batch  1320 of 293047. Elapsed: 0:23:16 Loss: 0.0295  Average_Train_Loss: 0.0085\n",
      "   Batch  1360 of 293047. Elapsed: 0:23:58 Loss: 0.1789  Average_Train_Loss: 0.0085\n",
      "   Batch  1400 of 293047. Elapsed: 0:24:41 Loss: 0.1807  Average_Train_Loss: 0.0085\n",
      "   Batch  1440 of 293047. Elapsed: 0:25:23 Loss: 0.2595  Average_Train_Loss: 0.0085\n",
      "   Batch  1480 of 293047. Elapsed: 0:26:05 Loss: 0.3405  Average_Train_Loss: 0.0085\n",
      "   Batch  1520 of 293047. Elapsed: 0:26:48 Loss: 0.1864  Average_Train_Loss: 0.0085\n",
      "   Batch  1560 of 293047. Elapsed: 0:27:30 Loss: 0.1132  Average_Train_Loss: 0.0085\n",
      "   Batch  1600 of 293047. Elapsed: 0:28:12 Loss: 0.2764  Average_Train_Loss: 0.0085\n",
      "   Batch  1640 of 293047. Elapsed: 0:28:55 Loss: 0.2407  Average_Train_Loss: 0.0085\n",
      "   Batch  1680 of 293047. Elapsed: 0:29:37 Loss: 0.2215  Average_Train_Loss: 0.0085\n",
      "   Batch  1720 of 293047. Elapsed: 0:30:19 Loss: 0.3074  Average_Train_Loss: 0.0085\n",
      "   Batch  1760 of 293047. Elapsed: 0:31:01 Loss: 0.2424  Average_Train_Loss: 0.0085\n",
      "   Batch  1800 of 293047. Elapsed: 0:31:44 Loss: 0.2101  Average_Train_Loss: 0.0085\n",
      "   Batch  1840 of 293047. Elapsed: 0:32:26 Loss: 0.1807  Average_Train_Loss: 0.0085\n",
      "   Batch  1880 of 293047. Elapsed: 0:33:09 Loss: 0.2211  Average_Train_Loss: 0.0085\n",
      "   Batch  1920 of 293047. Elapsed: 0:33:51 Loss: 0.2727  Average_Train_Loss: 0.0085\n",
      "   Batch  1960 of 293047. Elapsed: 0:34:33 Loss: 0.0311  Average_Train_Loss: 0.0085\n",
      "   Batch  2000 of 293047. Elapsed: 0:35:15 Loss: 0.3890  Average_Train_Loss: 0.0085\n",
      "   Batch  2040 of 293047. Elapsed: 0:35:58 Loss: 0.2703  Average_Train_Loss: 0.0085\n",
      "   Batch  2080 of 293047. Elapsed: 0:36:40 Loss: 0.2420  Average_Train_Loss: 0.0085\n",
      "   Batch  2120 of 293047. Elapsed: 0:37:22 Loss: 0.1571  Average_Train_Loss: 0.0085\n",
      "   Batch  2160 of 293047. Elapsed: 0:38:05 Loss: 0.2370  Average_Train_Loss: 0.0085\n",
      "   Batch  2200 of 293047. Elapsed: 0:38:47 Loss: 0.2555  Average_Train_Loss: 0.0085\n",
      "   Batch  2240 of 293047. Elapsed: 0:39:29 Loss: 0.3250  Average_Train_Loss: 0.0086\n",
      "   Batch  2280 of 293047. Elapsed: 0:40:11 Loss: 0.1724  Average_Train_Loss: 0.0085\n",
      "   Batch  2320 of 293047. Elapsed: 0:40:54 Loss: 0.1851  Average_Train_Loss: 0.0085\n",
      "   Batch  2360 of 293047. Elapsed: 0:41:36 Loss: 0.1395  Average_Train_Loss: 0.0085\n",
      "   Batch  2400 of 293047. Elapsed: 0:42:18 Loss: 0.3665  Average_Train_Loss: 0.0085\n",
      "   Batch  2440 of 293047. Elapsed: 0:43:01 Loss: 0.3363  Average_Train_Loss: 0.0085\n",
      "   Batch  2480 of 293047. Elapsed: 0:43:43 Loss: 0.1814  Average_Train_Loss: 0.0085\n",
      "   Batch  2520 of 293047. Elapsed: 0:44:25 Loss: 0.1070  Average_Train_Loss: 0.0086\n",
      "   Batch  2560 of 293047. Elapsed: 0:45:08 Loss: 0.1705  Average_Train_Loss: 0.0086\n",
      "   Batch  2600 of 293047. Elapsed: 0:45:50 Loss: 0.1777  Average_Train_Loss: 0.0085\n",
      "   Batch  2640 of 293047. Elapsed: 0:46:32 Loss: 0.0960  Average_Train_Loss: 0.0085\n",
      "   Batch  2680 of 293047. Elapsed: 0:47:15 Loss: 0.1514  Average_Train_Loss: 0.0085\n",
      "   Batch  2720 of 293047. Elapsed: 0:47:57 Loss: 0.4264  Average_Train_Loss: 0.0086\n",
      "   Batch  2760 of 293047. Elapsed: 0:48:39 Loss: 0.4612  Average_Train_Loss: 0.0086\n",
      "   Batch  2800 of 293047. Elapsed: 0:49:22 Loss: 0.5689  Average_Train_Loss: 0.0086\n",
      "   Batch  2840 of 293047. Elapsed: 0:50:04 Loss: 0.1969  Average_Train_Loss: 0.0086\n",
      "   Batch  2880 of 293047. Elapsed: 0:50:46 Loss: 0.2642  Average_Train_Loss: 0.0086\n",
      "   Batch  2920 of 293047. Elapsed: 0:51:29 Loss: 0.2031  Average_Train_Loss: 0.0086\n",
      "   Batch  2960 of 293047. Elapsed: 0:52:11 Loss: 0.2948  Average_Train_Loss: 0.0086\n",
      "   Batch  3000 of 293047. Elapsed: 0:52:53 Loss: 0.1277  Average_Train_Loss: 0.0086\n",
      "   Batch  3040 of 293047. Elapsed: 0:53:35 Loss: 0.0718  Average_Train_Loss: 0.0086\n",
      "   Batch  3080 of 293047. Elapsed: 0:54:18 Loss: 0.2245  Average_Train_Loss: 0.0086\n",
      "   Batch  3120 of 293047. Elapsed: 0:55:00 Loss: 0.2017  Average_Train_Loss: 0.0086\n",
      "   Batch  3160 of 293047. Elapsed: 0:55:42 Loss: 0.1742  Average_Train_Loss: 0.0086\n",
      "   Batch  3200 of 293047. Elapsed: 0:56:25 Loss: 0.6677  Average_Train_Loss: 0.0086\n",
      "   Batch  3240 of 293047. Elapsed: 0:57:07 Loss: 0.1090  Average_Train_Loss: 0.0086\n",
      "   Batch  3280 of 293047. Elapsed: 0:57:49 Loss: 0.2711  Average_Train_Loss: 0.0086\n",
      "   Batch  3320 of 293047. Elapsed: 0:58:32 Loss: 0.0276  Average_Train_Loss: 0.0086\n",
      "   Batch  3360 of 293047. Elapsed: 0:59:14 Loss: 0.1773  Average_Train_Loss: 0.0086\n",
      "   Batch  3400 of 293047. Elapsed: 0:59:56 Loss: 0.1980  Average_Train_Loss: 0.0086\n",
      "   Batch  3440 of 293047. Elapsed: 1:00:38 Loss: 0.0409  Average_Train_Loss: 0.0086\n",
      "   Batch  3480 of 293047. Elapsed: 1:01:21 Loss: 0.1248  Average_Train_Loss: 0.0086\n",
      "   Batch  3520 of 293047. Elapsed: 1:02:03 Loss: 0.5074  Average_Train_Loss: 0.0086\n",
      "   Batch  3560 of 293047. Elapsed: 1:02:46 Loss: 0.2937  Average_Train_Loss: 0.0086\n",
      "   Batch  3600 of 293047. Elapsed: 1:03:28 Loss: 0.3574  Average_Train_Loss: 0.0086\n",
      "   Batch  3640 of 293047. Elapsed: 1:04:10 Loss: 0.1547  Average_Train_Loss: 0.0086\n",
      "   Batch  3680 of 293047. Elapsed: 1:04:52 Loss: 0.3347  Average_Train_Loss: 0.0086\n",
      "   Batch  3720 of 293047. Elapsed: 1:05:35 Loss: 0.1194  Average_Train_Loss: 0.0086\n",
      "   Batch  3760 of 293047. Elapsed: 1:06:17 Loss: 0.3688  Average_Train_Loss: 0.0086\n",
      "   Batch  3800 of 293047. Elapsed: 1:06:59 Loss: 0.4274  Average_Train_Loss: 0.0086\n",
      "   Batch  3840 of 293047. Elapsed: 1:07:42 Loss: 0.0823  Average_Train_Loss: 0.0086\n",
      "   Batch  3880 of 293047. Elapsed: 1:08:24 Loss: 0.2470  Average_Train_Loss: 0.0086\n",
      "   Batch  3920 of 293047. Elapsed: 1:09:06 Loss: 0.1069  Average_Train_Loss: 0.0086\n",
      "   Batch  3960 of 293047. Elapsed: 1:09:48 Loss: 0.1853  Average_Train_Loss: 0.0085\n",
      "   Batch  4000 of 293047. Elapsed: 1:10:31 Loss: 0.2339  Average_Train_Loss: 0.0086\n",
      "   Batch  4040 of 293047. Elapsed: 1:11:13 Loss: 0.2653  Average_Train_Loss: 0.0085\n",
      "   Batch  4080 of 293047. Elapsed: 1:11:55 Loss: 0.1870  Average_Train_Loss: 0.0085\n",
      "   Batch  4120 of 293047. Elapsed: 1:12:38 Loss: 0.4073  Average_Train_Loss: 0.0085\n",
      "   Batch  4160 of 293047. Elapsed: 1:13:20 Loss: 0.3858  Average_Train_Loss: 0.0085\n",
      "   Batch  4200 of 293047. Elapsed: 1:14:02 Loss: 0.7243  Average_Train_Loss: 0.0085\n",
      "   Batch  4240 of 293047. Elapsed: 1:14:45 Loss: 0.1348  Average_Train_Loss: 0.0085\n",
      "   Batch  4280 of 293047. Elapsed: 1:15:27 Loss: 0.1251  Average_Train_Loss: 0.0085\n",
      "   Batch  4320 of 293047. Elapsed: 1:16:09 Loss: 0.3997  Average_Train_Loss: 0.0085\n",
      "   Batch  4360 of 293047. Elapsed: 1:16:51 Loss: 0.0637  Average_Train_Loss: 0.0085\n",
      "   Batch  4400 of 293047. Elapsed: 1:17:34 Loss: 0.2589  Average_Train_Loss: 0.0085\n",
      "   Batch  4440 of 293047. Elapsed: 1:18:16 Loss: 0.5000  Average_Train_Loss: 0.0085\n",
      "   Batch  4480 of 293047. Elapsed: 1:18:58 Loss: 0.3544  Average_Train_Loss: 0.0085\n",
      "   Batch  4520 of 293047. Elapsed: 1:19:41 Loss: 0.3487  Average_Train_Loss: 0.0085\n",
      "   Batch  4560 of 293047. Elapsed: 1:20:23 Loss: 0.1473  Average_Train_Loss: 0.0085\n",
      "   Batch  4600 of 293047. Elapsed: 1:21:05 Loss: 0.1824  Average_Train_Loss: 0.0085\n",
      "   Batch  4640 of 293047. Elapsed: 1:21:48 Loss: 0.3060  Average_Train_Loss: 0.0085\n",
      "   Batch  4680 of 293047. Elapsed: 1:22:30 Loss: 0.7992  Average_Train_Loss: 0.0085\n",
      "   Batch  4720 of 293047. Elapsed: 1:23:12 Loss: 0.1247  Average_Train_Loss: 0.0085\n",
      "   Batch  4760 of 293047. Elapsed: 1:23:55 Loss: 0.1240  Average_Train_Loss: 0.0085\n",
      "   Batch  4800 of 293047. Elapsed: 1:24:37 Loss: 0.2854  Average_Train_Loss: 0.0085\n",
      "   Batch  4840 of 293047. Elapsed: 1:25:19 Loss: 0.3257  Average_Train_Loss: 0.0085\n",
      "   Batch  4880 of 293047. Elapsed: 1:26:01 Loss: 0.2390  Average_Train_Loss: 0.0085\n",
      "   Batch  4920 of 293047. Elapsed: 1:26:44 Loss: 0.2919  Average_Train_Loss: 0.0085\n",
      "   Batch  4960 of 293047. Elapsed: 1:27:26 Loss: 0.2876  Average_Train_Loss: 0.0085\n",
      "   Batch  5000 of 293047. Elapsed: 1:28:08 Loss: 0.2085  Average_Train_Loss: 0.0085\n",
      "   Batch  5040 of 293047. Elapsed: 1:28:51 Loss: 0.2055  Average_Train_Loss: 0.0085\n",
      "   Batch  5080 of 293047. Elapsed: 1:29:33 Loss: 0.0636  Average_Train_Loss: 0.0085\n",
      "   Batch  5120 of 293047. Elapsed: 1:30:15 Loss: 0.2116  Average_Train_Loss: 0.0085\n",
      "   Batch  5160 of 293047. Elapsed: 1:30:58 Loss: 0.2945  Average_Train_Loss: 0.0085\n",
      "   Batch  5200 of 293047. Elapsed: 1:31:40 Loss: 0.1260  Average_Train_Loss: 0.0085\n",
      "   Batch  5240 of 293047. Elapsed: 1:32:22 Loss: 0.3723  Average_Train_Loss: 0.0085\n",
      "   Batch  5280 of 293047. Elapsed: 1:33:04 Loss: 0.4793  Average_Train_Loss: 0.0085\n",
      "   Batch  5320 of 293047. Elapsed: 1:33:47 Loss: 0.2569  Average_Train_Loss: 0.0085\n",
      "   Batch  5360 of 293047. Elapsed: 1:34:29 Loss: 0.1581  Average_Train_Loss: 0.0085\n",
      "   Batch  5400 of 293047. Elapsed: 1:35:11 Loss: 0.3100  Average_Train_Loss: 0.0085\n",
      "   Batch  5440 of 293047. Elapsed: 1:35:54 Loss: 0.3106  Average_Train_Loss: 0.0085\n",
      "   Batch  5480 of 293047. Elapsed: 1:36:36 Loss: 0.0952  Average_Train_Loss: 0.0085\n",
      "   Batch  5520 of 293047. Elapsed: 1:37:18 Loss: 0.2345  Average_Train_Loss: 0.0085\n",
      "   Batch  5560 of 293047. Elapsed: 1:38:00 Loss: 0.3179  Average_Train_Loss: 0.0085\n",
      "   Batch  5600 of 293047. Elapsed: 1:38:43 Loss: 0.3968  Average_Train_Loss: 0.0085\n",
      "   Batch  5640 of 293047. Elapsed: 1:39:25 Loss: 0.2384  Average_Train_Loss: 0.0085\n",
      "   Batch  5680 of 293047. Elapsed: 1:40:07 Loss: 0.5038  Average_Train_Loss: 0.0085\n",
      "   Batch  5720 of 293047. Elapsed: 1:40:50 Loss: 0.2025  Average_Train_Loss: 0.0085\n",
      "   Batch  5760 of 293047. Elapsed: 1:41:32 Loss: 0.3088  Average_Train_Loss: 0.0085\n",
      "   Batch  5800 of 293047. Elapsed: 1:42:14 Loss: 0.4847  Average_Train_Loss: 0.0085\n",
      "   Batch  5840 of 293047. Elapsed: 1:42:57 Loss: 0.1187  Average_Train_Loss: 0.0085\n",
      "   Batch  5880 of 293047. Elapsed: 1:43:39 Loss: 0.5268  Average_Train_Loss: 0.0085\n",
      "   Batch  5920 of 293047. Elapsed: 1:44:21 Loss: 0.0693  Average_Train_Loss: 0.0085\n",
      "   Batch  5960 of 293047. Elapsed: 1:45:04 Loss: 0.3593  Average_Train_Loss: 0.0085\n",
      "   Batch  6000 of 293047. Elapsed: 1:45:46 Loss: 0.3746  Average_Train_Loss: 0.0085\n",
      "   Batch  6040 of 293047. Elapsed: 1:46:28 Loss: 0.2814  Average_Train_Loss: 0.0085\n",
      "   Batch  6080 of 293047. Elapsed: 1:47:10 Loss: 0.1303  Average_Train_Loss: 0.0085\n",
      "   Batch  6120 of 293047. Elapsed: 1:47:53 Loss: 0.2071  Average_Train_Loss: 0.0085\n",
      "   Batch  6160 of 293047. Elapsed: 1:48:35 Loss: 0.0460  Average_Train_Loss: 0.0085\n",
      "   Batch  6200 of 293047. Elapsed: 1:49:17 Loss: 0.2558  Average_Train_Loss: 0.0085\n",
      "   Batch  6240 of 293047. Elapsed: 1:50:00 Loss: 0.3475  Average_Train_Loss: 0.0085\n",
      "   Batch  6280 of 293047. Elapsed: 1:50:42 Loss: 0.2160  Average_Train_Loss: 0.0085\n",
      "   Batch  6320 of 293047. Elapsed: 1:51:24 Loss: 0.2132  Average_Train_Loss: 0.0085\n",
      "   Batch  6360 of 293047. Elapsed: 1:52:06 Loss: 0.4244  Average_Train_Loss: 0.0085\n",
      "   Batch  6400 of 293047. Elapsed: 1:52:49 Loss: 0.3604  Average_Train_Loss: 0.0085\n",
      "   Batch  6440 of 293047. Elapsed: 1:53:31 Loss: 0.3935  Average_Train_Loss: 0.0085\n",
      "   Batch  6480 of 293047. Elapsed: 1:54:13 Loss: 0.2615  Average_Train_Loss: 0.0085\n",
      "   Batch  6520 of 293047. Elapsed: 1:54:56 Loss: 0.2797  Average_Train_Loss: 0.0085\n",
      "   Batch  6560 of 293047. Elapsed: 1:55:38 Loss: 0.1216  Average_Train_Loss: 0.0085\n",
      "   Batch  6600 of 293047. Elapsed: 1:56:20 Loss: 0.1751  Average_Train_Loss: 0.0085\n",
      "   Batch  6640 of 293047. Elapsed: 1:57:02 Loss: 0.2010  Average_Train_Loss: 0.0085\n",
      "   Batch  6680 of 293047. Elapsed: 1:57:45 Loss: 0.4983  Average_Train_Loss: 0.0084\n",
      "   Batch  6720 of 293047. Elapsed: 1:58:27 Loss: 0.1451  Average_Train_Loss: 0.0084\n",
      "   Batch  6760 of 293047. Elapsed: 1:59:09 Loss: 0.4110  Average_Train_Loss: 0.0085\n",
      "   Batch  6800 of 293047. Elapsed: 1:59:52 Loss: 0.6722  Average_Train_Loss: 0.0085\n",
      "   Batch  6840 of 293047. Elapsed: 2:00:34 Loss: 0.2564  Average_Train_Loss: 0.0085\n",
      "   Batch  6880 of 293047. Elapsed: 2:01:16 Loss: 0.4471  Average_Train_Loss: 0.0085\n",
      "   Batch  6920 of 293047. Elapsed: 2:01:59 Loss: 0.0490  Average_Train_Loss: 0.0085\n",
      "   Batch  6960 of 293047. Elapsed: 2:02:41 Loss: 0.2809  Average_Train_Loss: 0.0084\n",
      "   Batch  7000 of 293047. Elapsed: 2:03:23 Loss: 0.0276  Average_Train_Loss: 0.0084\n",
      "   Batch  7040 of 293047. Elapsed: 2:04:05 Loss: 0.2820  Average_Train_Loss: 0.0084\n",
      "   Batch  7080 of 293047. Elapsed: 2:04:48 Loss: 0.4480  Average_Train_Loss: 0.0084\n",
      "   Batch  7120 of 293047. Elapsed: 2:05:30 Loss: 0.0572  Average_Train_Loss: 0.0084\n",
      "   Batch  7160 of 293047. Elapsed: 2:06:12 Loss: 0.5598  Average_Train_Loss: 0.0084\n",
      "   Batch  7200 of 293047. Elapsed: 2:06:55 Loss: 0.3076  Average_Train_Loss: 0.0084\n",
      "   Batch  7240 of 293047. Elapsed: 2:07:37 Loss: 0.1817  Average_Train_Loss: 0.0084\n",
      "   Batch  7280 of 293047. Elapsed: 2:08:19 Loss: 0.4365  Average_Train_Loss: 0.0084\n",
      "   Batch  7320 of 293047. Elapsed: 2:09:02 Loss: 0.2971  Average_Train_Loss: 0.0084\n",
      "   Batch  7360 of 293047. Elapsed: 2:09:44 Loss: 0.4523  Average_Train_Loss: 0.0084\n",
      "   Batch  7400 of 293047. Elapsed: 2:10:26 Loss: 0.5350  Average_Train_Loss: 0.0084\n",
      "   Batch  7440 of 293047. Elapsed: 2:11:09 Loss: 0.1703  Average_Train_Loss: 0.0084\n",
      "   Batch  7480 of 293047. Elapsed: 2:11:51 Loss: 0.2274  Average_Train_Loss: 0.0084\n",
      "   Batch  7520 of 293047. Elapsed: 2:12:33 Loss: 0.3806  Average_Train_Loss: 0.0084\n",
      "   Batch  7560 of 293047. Elapsed: 2:13:15 Loss: 0.2528  Average_Train_Loss: 0.0084\n",
      "   Batch  7600 of 293047. Elapsed: 2:13:58 Loss: 0.1987  Average_Train_Loss: 0.0084\n",
      "   Batch  7640 of 293047. Elapsed: 2:14:40 Loss: 0.1947  Average_Train_Loss: 0.0084\n",
      "   Batch  7680 of 293047. Elapsed: 2:15:22 Loss: 0.2028  Average_Train_Loss: 0.0084\n",
      "   Batch  7720 of 293047. Elapsed: 2:16:04 Loss: 0.2116  Average_Train_Loss: 0.0084\n",
      "   Batch  7760 of 293047. Elapsed: 2:16:47 Loss: 0.1012  Average_Train_Loss: 0.0084\n",
      "   Batch  7800 of 293047. Elapsed: 2:17:29 Loss: 0.1474  Average_Train_Loss: 0.0084\n",
      "   Batch  7840 of 293047. Elapsed: 2:18:12 Loss: 0.1301  Average_Train_Loss: 0.0084\n",
      "   Batch  7880 of 293047. Elapsed: 2:18:54 Loss: 0.1453  Average_Train_Loss: 0.0084\n",
      "   Batch  7920 of 293047. Elapsed: 2:19:36 Loss: 0.1523  Average_Train_Loss: 0.0084\n",
      "   Batch  7960 of 293047. Elapsed: 2:20:19 Loss: 0.0455  Average_Train_Loss: 0.0084\n",
      "   Batch  8000 of 293047. Elapsed: 2:21:01 Loss: 0.2688  Average_Train_Loss: 0.0084\n",
      "   Batch  8040 of 293047. Elapsed: 2:21:43 Loss: 0.4584  Average_Train_Loss: 0.0084\n",
      "   Batch  8080 of 293047. Elapsed: 2:22:25 Loss: 0.1886  Average_Train_Loss: 0.0084\n",
      "   Batch  8120 of 293047. Elapsed: 2:23:08 Loss: 0.2979  Average_Train_Loss: 0.0084\n",
      "   Batch  8160 of 293047. Elapsed: 2:23:50 Loss: 0.1535  Average_Train_Loss: 0.0084\n",
      "   Batch  8200 of 293047. Elapsed: 2:24:32 Loss: 0.5180  Average_Train_Loss: 0.0084\n",
      "   Batch  8240 of 293047. Elapsed: 2:25:15 Loss: 0.2778  Average_Train_Loss: 0.0084\n",
      "   Batch  8280 of 293047. Elapsed: 2:25:57 Loss: 0.2347  Average_Train_Loss: 0.0084\n",
      "   Batch  8320 of 293047. Elapsed: 2:26:39 Loss: 0.3350  Average_Train_Loss: 0.0084\n",
      "   Batch  8360 of 293047. Elapsed: 2:27:22 Loss: 0.1777  Average_Train_Loss: 0.0084\n",
      "   Batch  8400 of 293047. Elapsed: 2:28:04 Loss: 0.4816  Average_Train_Loss: 0.0084\n",
      "   Batch  8440 of 293047. Elapsed: 2:28:46 Loss: 0.6413  Average_Train_Loss: 0.0084\n",
      "   Batch  8480 of 293047. Elapsed: 2:29:28 Loss: 0.2227  Average_Train_Loss: 0.0084\n",
      "   Batch  8520 of 293047. Elapsed: 2:30:11 Loss: 0.3096  Average_Train_Loss: 0.0084\n",
      "   Batch  8560 of 293047. Elapsed: 2:30:53 Loss: 0.2550  Average_Train_Loss: 0.0084\n",
      "   Batch  8600 of 293047. Elapsed: 2:31:35 Loss: 0.1022  Average_Train_Loss: 0.0084\n",
      "   Batch  8640 of 293047. Elapsed: 2:32:18 Loss: 0.1078  Average_Train_Loss: 0.0084\n",
      "   Batch  8680 of 293047. Elapsed: 2:33:00 Loss: 0.2627  Average_Train_Loss: 0.0084\n",
      "   Batch  8720 of 293047. Elapsed: 2:33:42 Loss: 0.0619  Average_Train_Loss: 0.0084\n",
      "   Batch  8760 of 293047. Elapsed: 2:34:25 Loss: 0.2026  Average_Train_Loss: 0.0084\n",
      "   Batch  8800 of 293047. Elapsed: 2:35:07 Loss: 0.2301  Average_Train_Loss: 0.0084\n",
      "   Batch  8840 of 293047. Elapsed: 2:35:49 Loss: 0.1179  Average_Train_Loss: 0.0084\n",
      "   Batch  8880 of 293047. Elapsed: 2:36:32 Loss: 0.2930  Average_Train_Loss: 0.0084\n",
      "   Batch  8920 of 293047. Elapsed: 2:37:14 Loss: 0.2737  Average_Train_Loss: 0.0084\n",
      "   Batch  8960 of 293047. Elapsed: 2:37:56 Loss: 0.5296  Average_Train_Loss: 0.0084\n",
      "   Batch  9000 of 293047. Elapsed: 2:38:38 Loss: 0.1340  Average_Train_Loss: 0.0084\n",
      "   Batch  9040 of 293047. Elapsed: 2:39:21 Loss: 0.2130  Average_Train_Loss: 0.0084\n",
      "   Batch  9080 of 293047. Elapsed: 2:40:03 Loss: 0.1219  Average_Train_Loss: 0.0084\n",
      "   Batch  9120 of 293047. Elapsed: 2:40:45 Loss: 0.0411  Average_Train_Loss: 0.0084\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 2:41:25\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.33\n",
      "  Validation took: 0:02:49\n",
      "\n",
      "Training complete!\n",
      "Total training took 8:12:42 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "  print('')\n",
    "  print('======= EPOCH {:} / {:} ======'.format(epoch_i + 1, epochs))\n",
    "  print('Training...')\n",
    "\n",
    "  t0 = time.time()\n",
    "  total_train_loss = 0\n",
    "  model.train()\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    if step % 40 == 0 and not step == 0:\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "      #Report progress\n",
    "      print('   Batch {0:>5} of {1:>5}. Elapsed: {2:} Loss: {3:.4f}  Average_Train_Loss: {4:.4f}'.format(step, len(train_data), elapsed, res['loss'], total_train_loss/(batch_size * (step + 1))))\n",
    "\n",
    "    #batch  contains 3 pytorch tensors: [0] input ids, [1] aattention masks, [2] labels\n",
    "    #print(batch[0])\n",
    "    #print(batch[1])\n",
    "    #print(batch[2])\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    #Forward pass\n",
    "    res = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "    loss = res['loss']\n",
    "    logits = res['logits'] #вероятности классов для батча\n",
    "\n",
    "    total_train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    #нормы градиентов обрезаем до 1.0, чтобы предотвратить проблему взрывающихся градиентов\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step() #обновление весов\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader)\n",
    "    train_time = format_time(time.time() - t0)\n",
    "\n",
    "  print(' Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "  print(' Training epoch took: {:}'.format(train_time))\n",
    "\n",
    "\n",
    "  #Validation\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "  t0 = time.time()\n",
    "  model.eval()\n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "  nb_eval_steps = 0\n",
    "\n",
    "  for batch in validation_dataloader:\n",
    "      b_input_ids = batch[0].to(device)\n",
    "      b_input_mask = batch[1].to(device)\n",
    "      b_labels = batch[2].to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions.\n",
    "          result = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "      loss = result.loss\n",
    "      logits = result.logits\n",
    "      total_eval_loss += loss.item()\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "  validation_time = format_time(time.time() - t0)\n",
    "\n",
    "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "  #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "  print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "  training_stats.append(\n",
    "      {\n",
    "          'epoch': epoch_i + 1,\n",
    "          'Training Loss': avg_train_loss,\n",
    "          'Valid. Loss': avg_val_loss,\n",
    "          'Valid. Accur.': avg_val_accuracy,\n",
    "          'Training Time': train_time,\n",
    "          'Validation Time': validation_time\n",
    "      }\n",
    "  )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "J0cLOHlOTC0b",
    "outputId": "0cff557e-f004-4985-960d-777cceac1032",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>2:41:23</td>\n",
       "      <td>0:02:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>2:41:26</td>\n",
       "      <td>0:02:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>2:41:25</td>\n",
       "      <td>0:02:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1             0.4262       0.3591         0.8925       2:41:23         0:02:49\n",
       "2             0.3310       0.3280         0.8978       2:41:26         0:02:49\n",
       "3             0.2677       0.3262         0.8978       2:41:25         0:02:49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv0zGE-tW31S"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01080632209777832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 166100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd234b318754899b894b5f9ed039a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daozerova/.conda/envs/fl/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences= X_test['Описание заявки'].values\n",
    "labels = y_test['TARGET_NUM'].values\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "      sent,\n",
    "      add_special_tokens = True,\n",
    "      max_length = 256,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True,\n",
    "      return_tensors = 'pt',\n",
    "  )\n",
    "  input_ids.append(encoded_dict['input_ids'])\n",
    "  attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "test_data = TensorDataset(input_ids, attention_masks, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vw8CDp27W2jW",
    "outputId": "e65248c4-0099-48fb-e96e-5579d9cf06ea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 166,100 test sentences...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013647317886352539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03bb83afe3943b9afad97d5f306f89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "prediction_sampler = SequentialSampler(test_data)\n",
    "prediction_dataloader = DataLoader(test_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_data)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict\n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids,\n",
    "                     token_type_ids=None,\n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21765205, -1.5519723 , -1.4594034 , ..., -1.2515924 ,\n",
       "         9.4910145 ,  2.5111623 ],\n",
       "       [-0.9524071 , -2.7549949 ,  0.518867  , ...,  0.08588488,\n",
       "         9.839141  ,  1.436584  ],\n",
       "       [-2.3296    ,  3.281514  , -2.017745  , ..., -2.3463485 ,\n",
       "         4.526649  ,  4.094553  ],\n",
       "       ...,\n",
       "       [-0.97943366,  1.8491099 , -2.7189112 , ..., -0.48699787,\n",
       "         5.6260653 ,  2.4752731 ],\n",
       "       [-0.24861492, -3.3224757 , -1.2537609 , ...,  4.329068  ,\n",
       "         6.3773365 , -1.284208  ],\n",
       "       [-0.7388959 , -1.6368381 , -1.4841821 , ..., -1.0935892 ,\n",
       "         6.3063793 ,  5.776569  ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the results across all batches.\n",
    "concat_predictions = np.concatenate(predictions, axis=0)\n",
    "concat_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14, 14, ..., 14, 14, 14])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(concat_predictions, axis=1).flatten()\n",
    "flat_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155638</th>\n",
       "      <td>7.7137e-06</td>\n",
       "      <td>3.2290e-06</td>\n",
       "      <td>3.8565e-06</td>\n",
       "      <td>3.5297e-06</td>\n",
       "      <td>1.3805e-06</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>4.3092e-06</td>\n",
       "      <td>2.5522e-05</td>\n",
       "      <td>8.5211e-06</td>\n",
       "      <td>2.1071e-06</td>\n",
       "      <td>3.6326e-06</td>\n",
       "      <td>3.7286e-06</td>\n",
       "      <td>9.1308e-07</td>\n",
       "      <td>8.0967e-06</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>2.4286e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078</th>\n",
       "      <td>6.8472e-04</td>\n",
       "      <td>5.2171e-02</td>\n",
       "      <td>2.2865e-04</td>\n",
       "      <td>1.1558e-03</td>\n",
       "      <td>3.8404e-03</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>2.4260e-02</td>\n",
       "      <td>3.7655e-03</td>\n",
       "      <td>9.8176e-03</td>\n",
       "      <td>2.9695e-02</td>\n",
       "      <td>5.5119e-02</td>\n",
       "      <td>2.8927e-02</td>\n",
       "      <td>1.1294e-04</td>\n",
       "      <td>2.8730e-03</td>\n",
       "      <td>0.7278</td>\n",
       "      <td>2.8113e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10761</th>\n",
       "      <td>1.5488e-04</td>\n",
       "      <td>1.7047e-05</td>\n",
       "      <td>5.2372e-05</td>\n",
       "      <td>6.0910e-05</td>\n",
       "      <td>5.2294e-05</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>2.9302e-04</td>\n",
       "      <td>1.1622e-01</td>\n",
       "      <td>1.7452e-04</td>\n",
       "      <td>2.3764e-04</td>\n",
       "      <td>5.2564e-05</td>\n",
       "      <td>6.3095e-05</td>\n",
       "      <td>1.6256e-05</td>\n",
       "      <td>6.7787e-04</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>1.8853e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77513</th>\n",
       "      <td>5.7679e-05</td>\n",
       "      <td>1.5482e-05</td>\n",
       "      <td>1.5753e-05</td>\n",
       "      <td>3.1375e-05</td>\n",
       "      <td>5.4983e-06</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>4.2511e-05</td>\n",
       "      <td>1.2974e-04</td>\n",
       "      <td>4.5082e-05</td>\n",
       "      <td>2.4720e-05</td>\n",
       "      <td>1.7546e-05</td>\n",
       "      <td>2.3985e-05</td>\n",
       "      <td>5.6981e-06</td>\n",
       "      <td>1.3446e-04</td>\n",
       "      <td>0.9384</td>\n",
       "      <td>8.3589e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145313</th>\n",
       "      <td>3.9466e-03</td>\n",
       "      <td>5.4008e-04</td>\n",
       "      <td>3.7314e-04</td>\n",
       "      <td>1.4591e-04</td>\n",
       "      <td>7.1520e-05</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>1.2529e-02</td>\n",
       "      <td>8.9271e-03</td>\n",
       "      <td>1.3939e-04</td>\n",
       "      <td>1.0364e-03</td>\n",
       "      <td>9.8028e-05</td>\n",
       "      <td>3.3749e-04</td>\n",
       "      <td>5.9346e-05</td>\n",
       "      <td>4.2346e-04</td>\n",
       "      <td>0.8442</td>\n",
       "      <td>1.6110e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4       5   \\\n",
       "155638  7.7137e-06  3.2290e-06  3.8565e-06  3.5297e-06  1.3805e-06  0.0004   \n",
       "12078   6.8472e-04  5.2171e-02  2.2865e-04  1.1558e-03  3.8404e-03  0.0314   \n",
       "10761   1.5488e-04  1.7047e-05  5.2372e-05  6.0910e-05  5.2294e-05  0.0040   \n",
       "77513   5.7679e-05  1.5482e-05  1.5753e-05  3.1375e-05  5.4983e-06  0.0609   \n",
       "145313  3.9466e-03  5.4008e-04  3.7314e-04  1.4591e-04  7.1520e-05  0.1271   \n",
       "\n",
       "                6           7           8           9           10  \\\n",
       "155638  4.3092e-06  2.5522e-05  8.5211e-06  2.1071e-06  3.6326e-06   \n",
       "12078   2.4260e-02  3.7655e-03  9.8176e-03  2.9695e-02  5.5119e-02   \n",
       "10761   2.9302e-04  1.1622e-01  1.7452e-04  2.3764e-04  5.2564e-05   \n",
       "77513   4.2511e-05  1.2974e-04  4.5082e-05  2.4720e-05  1.7546e-05   \n",
       "145313  1.2529e-02  8.9271e-03  1.3939e-04  1.0364e-03  9.8028e-05   \n",
       "\n",
       "                11          12          13      14          15  \n",
       "155638  3.7286e-06  9.1308e-07  8.0967e-06  0.9995  2.4286e-05  \n",
       "12078   2.8927e-02  1.1294e-04  2.8730e-03  0.7278  2.8113e-02  \n",
       "10761   6.3095e-05  1.6256e-05  6.7787e-04  0.8777  1.8853e-04  \n",
       "77513   2.3985e-05  5.6981e-06  1.3446e-04  0.9384  8.3589e-05  \n",
       "145313  3.3749e-04  5.9346e-05  4.2346e-04  0.8442  1.6110e-04  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "concat_predictions = softmax(pd.DataFrame(concat_predictions), axis=1)\n",
    "concat_predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "oDeXL1Y7W7ma",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14, 14, ..., 14, 13, 14])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "flat_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBGzrEDvW-_7",
    "outputId": "c1f529be-204b-4292-8ec0-45a28628f037",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score micro: 0.896\n",
      "F1 Score macro: 0.578\n",
      "F1 Score weighted: 0.889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1\n",
    "f1 = f1_score(flat_true_labels, flat_predictions, average='micro')\n",
    "f2 = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "f3 = f1_score(flat_true_labels, flat_predictions, average='weighted')\n",
    "\n",
    "print('F1 Score micro: %.3f' % f1)\n",
    "print('F1 Score macro: %.3f' % f2)\n",
    "print('F1 Score weighted: %.3f' % f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14, 14, 14, ..., 14, 13, 14]),\n",
       " array([14, 14, 14, ..., 14, 14, 14]),\n",
       " 166100,\n",
       " 166100)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_true_labels, flat_predictions, len(flat_true_labels), len(flat_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Report ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimal_threshold(\n",
    "    y_test, y_test_pred, \n",
    "    metric_thr = 0.9, \n",
    "    metric_fun = precision_score, # average_precision_score,\n",
    "    n_q = 100, start = .01, stop = .99\n",
    "):\n",
    "    quantiles = np.quantile(y_test_pred, np.linspace(.1, .99, num=n_q))\n",
    "    optimal_threshold = quantiles[-1]\n",
    "    metric_history = pd.Series(index=quantiles, dtype=np.float64)\n",
    "    for threshold in quantiles:\n",
    "        y_test_labels = (y_test_pred > threshold).astype(int)\n",
    "        metric = metric_fun(y_test, y_test_labels)\n",
    "        metric_history[threshold] = metric\n",
    "        if metric >= metric_thr:\n",
    "            optimal_threshold = threshold\n",
    "            return optimal_threshold, quantiles, metric_history\n",
    "    \n",
    "    return optimal_threshold, quantiles, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimal_thresholds(\n",
    "    y_test_bin, y_test_pred_bin, \n",
    "    y_test_multi, y_test_pred_multi,\n",
    "    metric_fun = precision_score,\n",
    "    bin_thr   = .90,\n",
    "    multi_thr = .8,\n",
    "):\n",
    "    optimal_threshold_bin, quantiles_bin, metric_history_bin =\\\n",
    "    get_optimal_threshold(\n",
    "        1 - y_test_bin, \n",
    "        1 - y_test_pred_bin, \n",
    "        metric_fun = metric_fun,\n",
    "        metric_thr = bin_thr,\n",
    "    )\n",
    "\n",
    "    optimal_threshold_multi, quantiles_multi, metric_history_multi =\\\n",
    "    dict(), dict(), dict()\n",
    "\n",
    "    for label in y_test_pred_multi.columns:\n",
    "        _optimal_threshold, _quantiles, _metric_history =\\\n",
    "        get_optimal_threshold(\n",
    "            (y_test_multi == label).astype(int), \n",
    "            y_test_pred_multi.loc[y_test_multi.index, label], \n",
    "            metric_fun = metric_fun,\n",
    "            metric_thr = multi_thr,\n",
    "        )\n",
    "        optimal_threshold_multi[label] = _optimal_threshold\n",
    "        quantiles_multi[label]         = _quantiles\n",
    "        metric_history_multi[label]    = _metric_history\n",
    "\n",
    "    optimal_threshold_multi = pd.Series(optimal_threshold_multi)\n",
    "    \n",
    "    return optimal_threshold_bin, quantiles_bin, metric_history_bin,\\\n",
    "           optimal_threshold_multi, quantiles_multi, metric_history_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_multi_classification_report(\n",
    "    y_test, y_test_bin, y_test_multi, \n",
    "    y_test_pred_bin, y_test_pred_multi,\n",
    "    optimal_threshold_bin,\n",
    "    optimal_threshold_multi,\n",
    "    display_results = True,\n",
    "):\n",
    "    #### Применение порогов\n",
    "    y_test_labels_bin = 1 - (1 - y_test_pred_bin > optimal_threshold_bin)\n",
    "    '''\n",
    "    y_test_labels_multi = y_test_pred_multi.ge(optimal_threshold_multi, axis=1)\\\n",
    "                          .mul(optimal_threshold_multi.index + 1).add(-1).max(axis=1)\n",
    "    '''\n",
    "    y_test_labels_multi = pd.Series(np.argmax(y_test_pred_multi.ge(optimal_threshold_multi, axis=1)\\\n",
    "                      .mul(y_test_pred_multi).values, axis=1), index=y_test_pred_multi.index)\n",
    "\n",
    "    y_test_labels_multi.loc[~y_test_pred_multi.ge(optimal_threshold_multi, axis=1).any(axis=1)] = -1\n",
    "\n",
    "    \n",
    "    mask = (y_test_labels_multi != -1) & y_test_labels_multi.index.isin(y_test_multi.index)\n",
    "    #print( y_test_labels_multi,  y_test_labels_multi.index, y_test_labels_multi[mask].index)\n",
    "\n",
    "    #### Оценка бинарной классификации\n",
    "    precision, recall, f1_score, support = metrics.precision_recall_fscore_support(y_test_bin, y_test_labels_bin)\n",
    "    df_metrics_bin = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1_score,\n",
    "        'Target count': support,\n",
    "        'Predicted count': y_test_labels_bin.value_counts().sort_index().values,\n",
    "    }, index = [f'Class {i}' for i in range(precision.shape[0])])\n",
    "    df_metrics_bin = df_metrics_bin.rename_axis('Binary classification', axis=1)\n",
    "    \n",
    "    if display_results:\n",
    "        display(df_metrics_bin)\n",
    "\n",
    "    #### Мультитаргет классификации\n",
    "    mask = (y_test_labels_multi != -1) & y_test_labels_multi.index.isin(y_test_multi.index)\n",
    "\n",
    "    precision, recall, f1_score, support = metrics.precision_recall_fscore_support(\n",
    "        y_test_multi[mask], \n",
    "        y_test_labels_multi[mask],\n",
    "        zero_division = 0\n",
    "    )\n",
    "    \n",
    "    df_metrics_multi = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1_score,\n",
    "        'Target count': support,\n",
    "        'Predicted count': y_test_labels_multi[mask].value_counts().sort_index().reindex(np.arange(support.shape[0])).fillna(0).astype(int).values,\n",
    "    }, index = [f'Class {i}'for i in range(precision.shape[0])])\n",
    "    df_metrics_multi = df_metrics_multi.rename_axis('Multi classification', axis=1)\n",
    "    \n",
    "    if display_results:\n",
    "        display(df_metrics_multi)\n",
    "\n",
    "    #### Оценка бинарной + мультитаргет классификации\n",
    "    y_test_labels_multi = y_test_pred_multi.ge(optimal_threshold_multi, axis=1)\\\n",
    "                          .mul(optimal_threshold_multi.index + 1).add(-1).max(axis=1)\n",
    "\n",
    "    mask = (y_test_labels_multi != -1) & (y_test_labels_bin == 0)\n",
    "\n",
    "    precision, recall, f1_score, support = metrics.precision_recall_fscore_support(\n",
    "        y_test[mask], \n",
    "        y_test_labels_multi[mask].replace({-1: 14, 14: 15}),\n",
    "        zero_division = 0\n",
    "    )\n",
    "\n",
    "    df_metrics_bin_multi = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1_score,\n",
    "        'Target count': support,\n",
    "        'Predicted count': y_test_labels_multi[mask].replace({-1: 14, 14: 15}).value_counts()\\\n",
    "                           .sort_index().reindex(np.arange(support.shape[0])).fillna(0).astype(int).values,\n",
    "    }, index = [f'Class {i}'for i in range(precision.shape[0])])\n",
    "    df_metrics_bin_multi = df_metrics_bin_multi.rename_axis('Binary + Multi classification', axis=1)\n",
    "    \n",
    "    if display_results:\n",
    "        display(df_metrics_bin_multi)\n",
    "\n",
    "    #### Оценка бинарной + мультитаргет классификации в общем\n",
    "    y_test_labels_multi_all = y_test_labels_multi.replace({-1: 14, 14: 15})\n",
    "    y_test_labels_multi_all.loc[y_test_labels_bin == 1] = 14\n",
    "\n",
    "    precision, recall, f1_score, support = metrics.precision_recall_fscore_support(\n",
    "        y_test, \n",
    "        y_test_labels_multi_all,\n",
    "        zero_division = 0\n",
    "    )\n",
    "    df_metrics_multi_all = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1_score,\n",
    "        'Target count': support,\n",
    "        'Predicted count': y_test_labels_multi_all.value_counts()\\\n",
    "                           .sort_index().reindex(np.arange(16)).fillna(0).astype(int).values,\n",
    "    }, index = [f'Class {i}'for i in range(precision.shape[0])])\n",
    "    df_metrics_multi_all = df_metrics_multi_all.rename_axis('Total classification', axis=1)\n",
    "\n",
    "    if display_results:\n",
    "        display(df_metrics_multi_all)\n",
    "\n",
    "    #### Эффект от модели\n",
    "    df_model_effect = pd.Series(name = 'Показатель', dtype = np.float64)\n",
    "    df_model_effect.loc['Всего заявок, шт.'] = df_metrics_multi_all.loc[:, 'Target count'].sum()\n",
    "    df_model_effect.loc['Правильных заявок, шт.'] = df_metrics_multi_all.loc['Class 14', 'Target count']\n",
    "    df_model_effect.loc['Перенаправленных заявок, шт.'] = df_metrics_multi_all.loc[\n",
    "        df_metrics_multi_all.index != 'Class 14', \n",
    "        'Target count'\n",
    "    ].sum()\n",
    "    df_model_effect.loc['Перенаправленных заявок, %'] =\\\n",
    "    df_model_effect.loc['Перенаправленных заявок, шт.'] / df_model_effect.loc['Всего заявок, шт.'] * 100\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (всего), шт.'] = df_metrics_multi_all.loc[\n",
    "        df_metrics_multi_all.index != 'Class 14', \n",
    "        'Predicted count'\n",
    "    ].sum()\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (всего), %'] =\\\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (всего), шт.'] / df_model_effect.loc['Всего заявок, шт.'] * 100\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (правильно), шт.'] = df_metrics_multi_all.loc[\n",
    "        df_metrics_multi_all.index != 'Class 14', \n",
    "        'Predicted count'\n",
    "    ].mul(df_metrics_multi_all.loc[\n",
    "        df_metrics_multi_all.index != 'Class 14', \n",
    "        'Precision'\n",
    "    ]).sum()\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (правильно), %'] =\\\n",
    "    df_model_effect.loc['Перенаправленных заявок по модели (правильно), шт.'] / df_model_effect.loc['Всего заявок, шт.'] * 100\n",
    "    df_model_effect.loc['Снижение неправильных заявок на, %'] =\\\n",
    "    ( df_model_effect.loc['Перенаправленных заявок по модели (правильно), шт.'] ) / df_model_effect.loc['Перенаправленных заявок, шт.'] * 100 #- df_metrics_bin_multi.loc['Class 14', 'Target count'] \n",
    "    df_model_effect.loc['Снижение неправильных заявок (c учетом ошибок) на, %'] =\\\n",
    "    ( df_model_effect.loc['Перенаправленных заявок по модели (правильно), шт.'] - df_metrics_bin_multi.loc['Class 14', 'Target count'] ) / df_model_effect.loc['Перенаправленных заявок, шт.'] * 100 \n",
    "\n",
    "    df_model_effect = df_model_effect.round(2).to_frame().rename_axis('Эффект от модели', axis=1)\n",
    "    #print(df_model_effect.loc['Снижение неправильных заявок (c учетом ошибок) на, %'])\n",
    "    #print(type(df_model_effect.loc['Снижение неправильных заявок (c учетом ошибок) на, %']))\n",
    "    a = df_model_effect.values[-1][0]\n",
    "    #print(a, type(a))\n",
    "    if display_results:\n",
    "        display(df_model_effect)\n",
    "    \n",
    "    return df_metrics_bin, df_metrics_multi, df_metrics_bin_multi, df_metrics_multi_all, df_model_effect, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = pd.Series(flat_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0         1\n",
       " 1         1\n",
       " 2         1\n",
       " 3         1\n",
       " 4         1\n",
       "          ..\n",
       " 166095    1\n",
       " 166096    1\n",
       " 166097    1\n",
       " 166098    0\n",
       " 166099    1\n",
       " Length: 166100, dtype: int64,\n",
       " 8         11\n",
       " 10         5\n",
       " 17         5\n",
       " 24         2\n",
       " 31         5\n",
       "           ..\n",
       " 166060     5\n",
       " 166061    14\n",
       " 166065    13\n",
       " 166094    11\n",
       " 166098    13\n",
       " Length: 28224, dtype: int64,\n",
       " 0         14\n",
       " 1         14\n",
       " 2         14\n",
       " 3         14\n",
       " 4         14\n",
       "           ..\n",
       " 166095    14\n",
       " 166096    14\n",
       " 166097    14\n",
       " 166098    13\n",
       " 166099    14\n",
       " Length: 166100, dtype: int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target\n",
    "y_test_bin = pd.Series(flat_true_labels).eq(14).astype(int)\n",
    "y_test_multi = y_test[y_test != 14].replace({15:14})\n",
    "y_test = pd.Series(flat_true_labels)\n",
    "y_test_bin, y_test_multi, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Predictions\n",
    "y_test_pred_multi = concat_predictions\n",
    "#pd.DataFrame(\n",
    "#    best_lgb_multi.predict(X_test_features), \n",
    "#    index = X_test_features.index\n",
    "#)\n",
    "\n",
    "y_test_pred_bin = y_test_pred_multi.loc[:, 14]\n",
    "\n",
    "y_test_pred_multi = y_test_pred_multi.drop(columns=[14]).rename(columns={15: 14})\n",
    "y_test_pred_multi = y_test_pred_multi.div(1 - y_test_pred_bin, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.84 83 64\n"
     ]
    }
   ],
   "source": [
    "max_perc = 0\n",
    "save_i, save_j  = 0, 0\n",
    "for i in range(65, 85):\n",
    "    for j in range(60, 85):\n",
    "        optimal_threshold_bin, quantiles_bin, metric_history_bin,\\\n",
    "        optimal_threshold_multi, quantiles_multi, metric_history_multi =\\\n",
    "        get_optimal_thresholds(\n",
    "            y_test_bin, y_test_pred_bin, \n",
    "            y_test_multi, y_test_pred_multi,\n",
    "            metric_fun = precision_score,\n",
    "            bin_thr   = i / 100, #0.77, #.75,\n",
    "            multi_thr = j / 100, #0.8, #.7,\n",
    "        )\n",
    "        _, _, _, _, _, a =\\\n",
    "        bin_multi_classification_report(\n",
    "            y_test, y_test_bin, y_test_multi, \n",
    "            y_test_pred_bin, y_test_pred_multi,\n",
    "            optimal_threshold_bin,\n",
    "            optimal_threshold_multi,\n",
    "            display_results = False,\n",
    "        )\n",
    "        if a > max_perc:\n",
    "            max_perc = a\n",
    "            save_i, save_j = i, j\n",
    "            \n",
    "print(max_perc, save_i, save_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_threshold_bin, quantiles_bin, metric_history_bin,\\\n",
    "optimal_threshold_multi, quantiles_multi, metric_history_multi =\\\n",
    "get_optimal_thresholds(\n",
    "    y_test_bin, y_test_pred_bin, \n",
    "    y_test_multi, y_test_pred_multi,\n",
    "    metric_fun = precision_score,\n",
    "    bin_thr   = save_i / 100, #0.77, #.75,\n",
    "    multi_thr = save_j / 100, #0.8, #.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Binary classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.8462</td>\n",
       "      <td>0.6318</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>28224</td>\n",
       "      <td>21073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.9284</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>137876</td>\n",
       "      <td>145027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Binary classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                   0.8462  0.6318    0.7235         28224   \n",
       "Class 1                   0.9284  0.9765    0.9518        137876   \n",
       "\n",
       "Binary classification  Predicted count  \n",
       "Class 0                          21073  \n",
       "Class 1                         145027  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Multi classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1095</td>\n",
       "      <td>1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.7099</td>\n",
       "      <td>0.6881</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>1273</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.8736</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>3315</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.6591</td>\n",
       "      <td>0.6273</td>\n",
       "      <td>0.6428</td>\n",
       "      <td>829</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5472</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>318</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.7202</td>\n",
       "      <td>0.7206</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>3644</td>\n",
       "      <td>3646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.6996</td>\n",
       "      <td>0.4422</td>\n",
       "      <td>0.5419</td>\n",
       "      <td>848</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.7326</td>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>2134</td>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.6872</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>1730</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.6493</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.5069</td>\n",
       "      <td>837</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>1336</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.7584</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>3013</td>\n",
       "      <td>3213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>1073</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>2772</td>\n",
       "      <td>2790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.6452</td>\n",
       "      <td>0.6601</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>2727</td>\n",
       "      <td>2790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Multi classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                  0.7551  0.8055    0.7795          1095   \n",
       "Class 1                  0.7099  0.6881    0.6988          1273   \n",
       "Class 2                  0.8736  0.9487    0.9096          3315   \n",
       "Class 3                  0.6591  0.6273    0.6428           829   \n",
       "Class 4                  0.6170  0.5472    0.5800           318   \n",
       "Class 5                  0.7202  0.7206    0.7204          3644   \n",
       "Class 6                  0.6996  0.4422    0.5419           848   \n",
       "Class 7                  0.7326  0.8102    0.7695          2134   \n",
       "Class 8                  0.6872  0.5156    0.5892          1730   \n",
       "Class 9                  0.6493  0.4158    0.5069           837   \n",
       "Class 10                 0.6920  0.7283    0.7097          1336   \n",
       "Class 11                 0.7112  0.7584    0.7340          3013   \n",
       "Class 12                 0.7940  0.9590    0.8687          1073   \n",
       "Class 13                 0.8427  0.8481    0.8454          2772   \n",
       "Class 14                 0.6452  0.6601    0.6525          2727   \n",
       "\n",
       "Multi classification  Predicted count  \n",
       "Class 0                          1168  \n",
       "Class 1                          1234  \n",
       "Class 2                          3600  \n",
       "Class 3                           789  \n",
       "Class 4                           282  \n",
       "Class 5                          3646  \n",
       "Class 6                           536  \n",
       "Class 7                          2360  \n",
       "Class 8                          1298  \n",
       "Class 9                           536  \n",
       "Class 10                         1406  \n",
       "Class 11                         3213  \n",
       "Class 12                         1296  \n",
       "Class 13                         2790  \n",
       "Class 14                         2790  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Binary + Multi classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.7151</td>\n",
       "      <td>510</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>780</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.9076</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>3107</td>\n",
       "      <td>3550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.5286</td>\n",
       "      <td>0.6332</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>379</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6089</td>\n",
       "      <td>208</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.6780</td>\n",
       "      <td>0.7074</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>1938</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.5299</td>\n",
       "      <td>0.4562</td>\n",
       "      <td>0.4903</td>\n",
       "      <td>388</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>1284</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.5604</td>\n",
       "      <td>1022</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.4871</td>\n",
       "      <td>0.5084</td>\n",
       "      <td>464</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.7483</td>\n",
       "      <td>0.7071</td>\n",
       "      <td>1013</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.8213</td>\n",
       "      <td>0.7699</td>\n",
       "      <td>2389</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.7087</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>1009</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.6367</td>\n",
       "      <td>0.9414</td>\n",
       "      <td>0.7597</td>\n",
       "      <td>1605</td>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 15</th>\n",
       "      <td>0.5184</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.6001</td>\n",
       "      <td>1384</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Binary + Multi classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                           0.7069  0.7235    0.7151           510   \n",
       "Class 1                           0.6208  0.7282    0.6702           780   \n",
       "Class 2                           0.7944  0.9076    0.8472          3107   \n",
       "Class 3                           0.5286  0.6332    0.5762           379   \n",
       "Class 4                           0.5936  0.6250    0.6089           208   \n",
       "Class 5                           0.6780  0.7074    0.6924          1938   \n",
       "Class 6                           0.5299  0.4562    0.4903           388   \n",
       "Class 7                           0.6898  0.8762    0.7719          1284   \n",
       "Class 8                           0.5620  0.5587    0.5604          1022   \n",
       "Class 9                           0.5318  0.4871    0.5084           464   \n",
       "Class 10                          0.6702  0.7483    0.7071          1013   \n",
       "Class 11                          0.7245  0.8213    0.7699          2389   \n",
       "Class 12                          0.7087  0.9911    0.8264          1009   \n",
       "Class 13                          0.6367  0.9414    0.7597          1605   \n",
       "Class 14                          0.0000  0.0000    0.0000          3133   \n",
       "Class 15                          0.5184  0.7124    0.6001          1384   \n",
       "\n",
       "Binary + Multi classification  Predicted count  \n",
       "Class 0                                    522  \n",
       "Class 1                                    915  \n",
       "Class 2                                   3550  \n",
       "Class 3                                    454  \n",
       "Class 4                                    219  \n",
       "Class 5                                   2022  \n",
       "Class 6                                    334  \n",
       "Class 7                                   1631  \n",
       "Class 8                                   1016  \n",
       "Class 9                                    425  \n",
       "Class 10                                  1131  \n",
       "Class 11                                  2708  \n",
       "Class 12                                  1411  \n",
       "Class 13                                  2373  \n",
       "Class 14                                     0  \n",
       "Class 15                                  1902  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Total classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>0.4556</td>\n",
       "      <td>1098</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.4195</td>\n",
       "      <td>0.5007</td>\n",
       "      <td>1354</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8216</td>\n",
       "      <td>3315</td>\n",
       "      <td>3550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.5286</td>\n",
       "      <td>0.2634</td>\n",
       "      <td>0.3516</td>\n",
       "      <td>911</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.4569</td>\n",
       "      <td>350</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.6780</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>3739</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.5299</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.2774</td>\n",
       "      <td>942</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.5151</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>2184</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.3935</td>\n",
       "      <td>1886</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>1107</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>1402</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.6258</td>\n",
       "      <td>0.6716</td>\n",
       "      <td>3135</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.7087</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>0.8045</td>\n",
       "      <td>1075</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.6367</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>2781</td>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.9262</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>137876</td>\n",
       "      <td>145487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 15</th>\n",
       "      <td>0.5184</td>\n",
       "      <td>0.3348</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>2945</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Total classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                  0.7069  0.3361    0.4556          1098   \n",
       "Class 1                  0.6208  0.4195    0.5007          1354   \n",
       "Class 2                  0.7944  0.8507    0.8216          3315   \n",
       "Class 3                  0.5286  0.2634    0.3516           911   \n",
       "Class 4                  0.5936  0.3714    0.4569           350   \n",
       "Class 5                  0.6780  0.3667    0.4760          3739   \n",
       "Class 6                  0.5299  0.1879    0.2774           942   \n",
       "Class 7                  0.6898  0.5151    0.5898          2184   \n",
       "Class 8                  0.5620  0.3028    0.3935          1886   \n",
       "Class 9                  0.5318  0.2042    0.2950          1107   \n",
       "Class 10                 0.6702  0.5407    0.5985          1402   \n",
       "Class 11                 0.7245  0.6258    0.6716          3135   \n",
       "Class 12                 0.7087  0.9302    0.8045          1075   \n",
       "Class 13                 0.6367  0.5433    0.5863          2781   \n",
       "Class 14                 0.9262  0.9773    0.9510        137876   \n",
       "Class 15                 0.5184  0.3348    0.4068          2945   \n",
       "\n",
       "Total classification  Predicted count  \n",
       "Class 0                           522  \n",
       "Class 1                           915  \n",
       "Class 2                          3550  \n",
       "Class 3                           454  \n",
       "Class 4                           219  \n",
       "Class 5                          2022  \n",
       "Class 6                           334  \n",
       "Class 7                          1631  \n",
       "Class 8                          1016  \n",
       "Class 9                           425  \n",
       "Class 10                         1131  \n",
       "Class 11                         2708  \n",
       "Class 12                         1411  \n",
       "Class 13                         2373  \n",
       "Class 14                       145487  \n",
       "Class 15                         1902  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Эффект от модели</th>\n",
       "      <th>Показатель</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Всего заявок, шт.</th>\n",
       "      <td>166100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Правильных заявок, шт.</th>\n",
       "      <td>137876.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок, шт.</th>\n",
       "      <td>28224.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок, %</th>\n",
       "      <td>16.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (всего), шт.</th>\n",
       "      <td>20613.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (всего), %</th>\n",
       "      <td>12.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (правильно), шт.</th>\n",
       "      <td>13814.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (правильно), %</th>\n",
       "      <td>8.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Снижение неправильных заявок на, %</th>\n",
       "      <td>48.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Снижение неправильных заявок (c учетом ошибок) на, %</th>\n",
       "      <td>37.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Эффект от модели                                    Показатель\n",
       "Всего заявок, шт.                                    166100.00\n",
       "Правильных заявок, шт.                               137876.00\n",
       "Перенаправленных заявок, шт.                          28224.00\n",
       "Перенаправленных заявок, %                               16.99\n",
       "Перенаправленных заявок по модели (всего), шт.        20613.00\n",
       "Перенаправленных заявок по модели (всего), %             12.41\n",
       "Перенаправленных заявок по модели (правильно), шт.    13814.00\n",
       "Перенаправленных заявок по модели (правильно), %          8.32\n",
       "Снижение неправильных заявок на, %                       48.94\n",
       "Снижение неправильных заявок (c учетом ошибок) ...       37.84"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_metrics_bin, df_metrics_multi,\\\n",
    "df_metrics_bin_multi, df_metrics_multi_all,\\\n",
    "df_model_effect, _ =\\\n",
    "bin_multi_classification_report(\n",
    "    y_test, y_test_bin, y_test_multi, \n",
    "    y_test_pred_bin, y_test_pred_multi,\n",
    "    optimal_threshold_bin,\n",
    "    optimal_threshold_multi,\n",
    "    display_results = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold_bin, quantiles_bin, metric_history_bin,\\\n",
    "optimal_threshold_multi, quantiles_multi, metric_history_multi =\\\n",
    "get_optimal_thresholds(\n",
    "    y_test_bin, y_test_pred_bin, \n",
    "    y_test_multi, y_test_pred_multi,\n",
    "    metric_fun = precision_score,\n",
    "    bin_thr   = 0.8, #save_i, #0.77, #.75,\n",
    "    multi_thr = 0.75, #save_j, #0.8, #.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Binary classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.8097</td>\n",
       "      <td>0.6902</td>\n",
       "      <td>0.7452</td>\n",
       "      <td>28224</td>\n",
       "      <td>24060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.9384</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>137876</td>\n",
       "      <td>142040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Binary classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                   0.8097  0.6902    0.7452         28224   \n",
       "Class 1                   0.9384  0.9668    0.9524        137876   \n",
       "\n",
       "Binary classification  Predicted count  \n",
       "Class 0                          24060  \n",
       "Class 1                         142040  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Multi classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.8227</td>\n",
       "      <td>0.8119</td>\n",
       "      <td>1015</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.7644</td>\n",
       "      <td>0.8227</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>970</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.8405</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>3315</td>\n",
       "      <td>3812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.7668</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7242</td>\n",
       "      <td>599</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>250</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.7721</td>\n",
       "      <td>0.7706</td>\n",
       "      <td>3278</td>\n",
       "      <td>3291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.3847</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>590</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.7997</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>1928</td>\n",
       "      <td>2052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.4917</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>1267</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.7491</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>419</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>0.7851</td>\n",
       "      <td>1073</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7713</td>\n",
       "      <td>0.8349</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>2598</td>\n",
       "      <td>2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.8178</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>1066</td>\n",
       "      <td>1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.8103</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>0.8443</td>\n",
       "      <td>2720</td>\n",
       "      <td>2958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>0.6609</td>\n",
       "      <td>1707</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Multi classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                  0.8013  0.8227    0.8119          1015   \n",
       "Class 1                  0.7644  0.8227    0.7925           970   \n",
       "Class 2                  0.8405  0.9665    0.8991          3315   \n",
       "Class 3                  0.7668  0.6861    0.7242           599   \n",
       "Class 4                  0.6170  0.6960    0.6541           250   \n",
       "Class 5                  0.7691  0.7721    0.7706          3278   \n",
       "Class 6                  0.8021  0.3847    0.5200           590   \n",
       "Class 7                  0.7997  0.8511    0.8246          1928   \n",
       "Class 8                  0.7886  0.4917    0.6057          1267   \n",
       "Class 9                  0.7491  0.5060    0.6040           419   \n",
       "Class 10                 0.7960  0.7745    0.7851          1073   \n",
       "Class 11                 0.7713  0.8349    0.8018          2598   \n",
       "Class 12                 0.8178  0.9728    0.8886          1066   \n",
       "Class 13                 0.8103  0.8812    0.8443          2720   \n",
       "Class 14                 0.7650  0.5817    0.6609          1707   \n",
       "\n",
       "Multi classification  Predicted count  \n",
       "Class 0                          1042  \n",
       "Class 1                          1044  \n",
       "Class 2                          3812  \n",
       "Class 3                           536  \n",
       "Class 4                           282  \n",
       "Class 5                          3291  \n",
       "Class 6                           283  \n",
       "Class 7                          2052  \n",
       "Class 8                           790  \n",
       "Class 9                           283  \n",
       "Class 10                         1044  \n",
       "Class 11                         2812  \n",
       "Class 12                         1268  \n",
       "Class 13                         2958  \n",
       "Class 14                         1298  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Binary + Multi classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.6270</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>584</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.5973</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.6952</td>\n",
       "      <td>742</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.7607</td>\n",
       "      <td>0.9477</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>3193</td>\n",
       "      <td>3978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.5274</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.5977</td>\n",
       "      <td>377</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.6141</td>\n",
       "      <td>198</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.6357</td>\n",
       "      <td>0.7587</td>\n",
       "      <td>0.6917</td>\n",
       "      <td>1964</td>\n",
       "      <td>2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.5261</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>0.4788</td>\n",
       "      <td>321</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.7045</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>1289</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.5753</td>\n",
       "      <td>0.5533</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>967</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.5559</td>\n",
       "      <td>0.5498</td>\n",
       "      <td>313</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.6910</td>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.7435</td>\n",
       "      <td>978</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>2288</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.7539</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.8550</td>\n",
       "      <td>1021</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.9342</td>\n",
       "      <td>0.7602</td>\n",
       "      <td>1793</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 15</th>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>1047</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Binary + Multi classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                           0.6270  0.8031    0.7042           584   \n",
       "Class 1                           0.5973  0.8315    0.6952           742   \n",
       "Class 2                           0.7607  0.9477    0.8440          3193   \n",
       "Class 3                           0.5274  0.6897    0.5977           377   \n",
       "Class 4                           0.5314  0.7273    0.6141           198   \n",
       "Class 5                           0.6357  0.7587    0.6917          1964   \n",
       "Class 6                           0.5261  0.4393    0.4788           321   \n",
       "Class 7                           0.7045  0.8898    0.7864          1289   \n",
       "Class 8                           0.5753  0.5533    0.5640           967   \n",
       "Class 9                           0.5437  0.5559    0.5498           313   \n",
       "Class 10                          0.6910  0.8047    0.7435           978   \n",
       "Class 11                          0.7522  0.8571    0.8012          2288   \n",
       "Class 12                          0.7539  0.9873    0.8550          1021   \n",
       "Class 13                          0.6408  0.9342    0.7602          1793   \n",
       "Class 14                          0.0000  0.0000    0.0000          3771   \n",
       "Class 15                          0.5695  0.6180    0.5928          1047   \n",
       "\n",
       "Binary + Multi classification  Predicted count  \n",
       "Class 0                                    748  \n",
       "Class 1                                   1033  \n",
       "Class 2                                   3978  \n",
       "Class 3                                    493  \n",
       "Class 4                                    271  \n",
       "Class 5                                   2344  \n",
       "Class 6                                    268  \n",
       "Class 7                                   1628  \n",
       "Class 8                                    930  \n",
       "Class 9                                    320  \n",
       "Class 10                                  1139  \n",
       "Class 11                                  2607  \n",
       "Class 12                                  1337  \n",
       "Class 13                                  2614  \n",
       "Class 14                                     0  \n",
       "Class 15                                  1136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Total classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.6270</td>\n",
       "      <td>0.4271</td>\n",
       "      <td>0.5081</td>\n",
       "      <td>1098</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.5973</td>\n",
       "      <td>0.4557</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>1354</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0.7607</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>0.8298</td>\n",
       "      <td>3315</td>\n",
       "      <td>3978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0.5274</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>911</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 4</th>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>0.4638</td>\n",
       "      <td>350</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 5</th>\n",
       "      <td>0.6357</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>3739</td>\n",
       "      <td>2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 6</th>\n",
       "      <td>0.5261</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>942</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 7</th>\n",
       "      <td>0.7045</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>0.6018</td>\n",
       "      <td>2184</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 8</th>\n",
       "      <td>0.5753</td>\n",
       "      <td>0.2837</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>1886</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 9</th>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.2439</td>\n",
       "      <td>1107</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 10</th>\n",
       "      <td>0.6910</td>\n",
       "      <td>0.5613</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>1402</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 11</th>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>3135</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 12</th>\n",
       "      <td>0.7539</td>\n",
       "      <td>0.9377</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>1075</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 13</th>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.6023</td>\n",
       "      <td>0.6209</td>\n",
       "      <td>2781</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 14</th>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>137876</td>\n",
       "      <td>145254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 15</th>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.3171</td>\n",
       "      <td>2945</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Total classification  Precision  Recall  F1_Score  Target count  \\\n",
       "Class 0                  0.6270  0.4271    0.5081          1098   \n",
       "Class 1                  0.5973  0.4557    0.5170          1354   \n",
       "Class 2                  0.7607  0.9128    0.8298          3315   \n",
       "Class 3                  0.5274  0.2854    0.3704           911   \n",
       "Class 4                  0.5314  0.4114    0.4638           350   \n",
       "Class 5                  0.6357  0.3985    0.4899          3739   \n",
       "Class 6                  0.5261  0.1497    0.2331           942   \n",
       "Class 7                  0.7045  0.5252    0.6018          2184   \n",
       "Class 8                  0.5753  0.2837    0.3800          1886   \n",
       "Class 9                  0.5437  0.1572    0.2439          1107   \n",
       "Class 10                 0.6910  0.5613    0.6194          1402   \n",
       "Class 11                 0.7522  0.6255    0.6830          3135   \n",
       "Class 12                 0.7539  0.9377    0.8358          1075   \n",
       "Class 13                 0.6408  0.6023    0.6209          2781   \n",
       "Class 14                 0.9232  0.9726    0.9473        137876   \n",
       "Class 15                 0.5695  0.2197    0.3171          2945   \n",
       "\n",
       "Total classification  Predicted count  \n",
       "Class 0                           748  \n",
       "Class 1                          1033  \n",
       "Class 2                          3978  \n",
       "Class 3                           493  \n",
       "Class 4                           271  \n",
       "Class 5                          2344  \n",
       "Class 6                           268  \n",
       "Class 7                          1628  \n",
       "Class 8                           930  \n",
       "Class 9                           320  \n",
       "Class 10                         1139  \n",
       "Class 11                         2607  \n",
       "Class 12                         1337  \n",
       "Class 13                         2614  \n",
       "Class 14                       145254  \n",
       "Class 15                         1136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Эффект от модели</th>\n",
       "      <th>Показатель</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Всего заявок, шт.</th>\n",
       "      <td>166100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Правильных заявок, шт.</th>\n",
       "      <td>137876.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок, шт.</th>\n",
       "      <td>28224.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок, %</th>\n",
       "      <td>16.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (всего), шт.</th>\n",
       "      <td>20846.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (всего), %</th>\n",
       "      <td>12.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (правильно), шт.</th>\n",
       "      <td>14081.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Перенаправленных заявок по модели (правильно), %</th>\n",
       "      <td>8.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Снижение неправильных заявок на, %</th>\n",
       "      <td>49.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Снижение неправильных заявок (c учетом ошибок) на, %</th>\n",
       "      <td>36.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Эффект от модели                                    Показатель\n",
       "Всего заявок, шт.                                    166100.00\n",
       "Правильных заявок, шт.                               137876.00\n",
       "Перенаправленных заявок, шт.                          28224.00\n",
       "Перенаправленных заявок, %                               16.99\n",
       "Перенаправленных заявок по модели (всего), шт.        20846.00\n",
       "Перенаправленных заявок по модели (всего), %             12.55\n",
       "Перенаправленных заявок по модели (правильно), шт.    14081.00\n",
       "Перенаправленных заявок по модели (правильно), %          8.48\n",
       "Снижение неправильных заявок на, %                       49.89\n",
       "Снижение неправильных заявок (c учетом ошибок) ...       36.53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_metrics_bin, df_metrics_multi,\\\n",
    "df_metrics_bin_multi, df_metrics_multi_all,\\\n",
    "df_model_effect, _ =\\\n",
    "bin_multi_classification_report(\n",
    "    y_test, y_test_bin, y_test_multi, \n",
    "    y_test_pred_bin, y_test_pred_multi,\n",
    "    optimal_threshold_bin,\n",
    "    optimal_threshold_multi,\n",
    "    display_results = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test, columns=['0']).to_parquet('bert_16/y_test.parquet')\n",
    "pd.DataFrame(y_test_bin, columns=['0']).to_parquet('bert_16/y_test_bin.parquet')\n",
    "pd.DataFrame(y_test_multi, columns=['0']).to_parquet('bert_16/y_test_multi.parquet')\n",
    "\n",
    "pd.DataFrame(y_test_pred_bin, columns=['0']).to_parquet('bert_16/y_test_pred_bin.parquet')\n",
    "pd.DataFrame(y_test_pred_multi, columns=['0']).to_parquet('bert_16/y_test_pred_multi.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total classification</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Target count</th>\n",
       "      <th>Predicted count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DebitCardsSupport</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>0.4556</td>\n",
       "      <td>1098</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MB.oplata_dbo</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.4195</td>\n",
       "      <td>0.5007</td>\n",
       "      <td>1354</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Авторизация. Неактуальная</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8216</td>\n",
       "      <td>3315</td>\n",
       "      <td>3550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Антифрод</td>\n",
       "      <td>0.5286</td>\n",
       "      <td>0.2634</td>\n",
       "      <td>0.3516</td>\n",
       "      <td>911</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Возврат средств</td>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.4569</td>\n",
       "      <td>350</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Другое: неправильная заявка</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>3739</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>МТС Деньги ВЕБ</td>\n",
       "      <td>0.5299</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.2774</td>\n",
       "      <td>942</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Отдел активно-пассивных операций</td>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.5151</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>2184</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Отдел поддержки карточных технологий</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.3935</td>\n",
       "      <td>1886</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Переводы между своими счетами</td>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>1107</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Платёжный Хаб</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>1402</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Поддержка ПЦ ЭК</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.6258</td>\n",
       "      <td>0.6716</td>\n",
       "      <td>3135</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Поддержка СДП.Платежи</td>\n",
       "      <td>0.7087</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>0.8045</td>\n",
       "      <td>1075</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Поддержка бизнес процессов'</td>\n",
       "      <td>0.6367</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>2781</td>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Правильная заявка</td>\n",
       "      <td>0.9262</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>137876</td>\n",
       "      <td>145487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Приложение МТС Банк</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>0.3348</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>2945</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Total classification  Precision  Recall  F1_Score  \\\n",
       "0                      DebitCardsSupport     0.7069  0.3361    0.4556   \n",
       "1                          MB.oplata_dbo     0.6208  0.4195    0.5007   \n",
       "2              Авторизация. Неактуальная     0.7944  0.8507    0.8216   \n",
       "3                               Антифрод     0.5286  0.2634    0.3516   \n",
       "4                        Возврат средств     0.5936  0.3714    0.4569   \n",
       "5            Другое: неправильная заявка     0.6780  0.3667    0.4760   \n",
       "6                         МТС Деньги ВЕБ     0.5299  0.1879    0.2774   \n",
       "7       Отдел активно-пассивных операций     0.6898  0.5151    0.5898   \n",
       "8   Отдел поддержки карточных технологий     0.5620  0.3028    0.3935   \n",
       "9          Переводы между своими счетами     0.5318  0.2042    0.2950   \n",
       "10                         Платёжный Хаб     0.6702  0.5407    0.5985   \n",
       "11                       Поддержка ПЦ ЭК     0.7245  0.6258    0.6716   \n",
       "12                 Поддержка СДП.Платежи     0.7087  0.9302    0.8045   \n",
       "13           Поддержка бизнес процессов'     0.6367  0.5433    0.5863   \n",
       "14                     Правильная заявка     0.9262  0.9773    0.9510   \n",
       "15                   Приложение МТС Банк     0.5184  0.3348    0.4068   \n",
       "\n",
       "    Target count  Predicted count  \n",
       "0           1098              522  \n",
       "1           1354              915  \n",
       "2           3315             3550  \n",
       "3            911              454  \n",
       "4            350              219  \n",
       "5           3739             2022  \n",
       "6            942              334  \n",
       "7           2184             1631  \n",
       "8           1886             1016  \n",
       "9           1107              425  \n",
       "10          1402             1131  \n",
       "11          3135             2708  \n",
       "12          1075             1411  \n",
       "13          2781             2373  \n",
       "14        137876           145487  \n",
       "15          2945             1902  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_class = pd.read_excel('total_class.xlsx').drop(['Unnamed: 0'], axis=1)\n",
    "total_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3214.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "166100.00 - (142040 + 20846)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "fl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
